{"meta":{"title":"My Blog","subtitle":"Welcome to my personal blog","description":"A blog powered by Hexo and Stellar theme","author":"nash635","url":"https://nash635.github.io","root":"/"},"pages":[{"title":"å…³äºæˆ‘","date":"2025-07-23T17:19:49.000Z","updated":"2025-11-10T08:32:58.192Z","comments":true,"path":"about/index.html","permalink":"https://nash635.github.io/about/","excerpt":"","text":"å…³äºæˆ‘ä½ å¥½ï¼æˆ‘æ˜¯ nash635ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„ä¸ªäººåšå®¢ï¼ ğŸ™‹â€â™‚ï¸ ä¸ªäººä»‹ç»æˆ‘æ˜¯ä¸€åçƒ­çˆ±æŠ€æœ¯çš„å¼€å‘è€…ï¼Œå–œæ¬¢æ¢ç´¢æ–°æŠ€æœ¯ï¼Œè®°å½•å­¦ä¹ è¿‡ç¨‹ï¼Œåˆ†äº«æŠ€æœ¯ç»éªŒã€‚ ğŸ’» æŠ€æœ¯æ ˆ åˆ†å¸ƒå¼ç³»ç»Ÿ æ·±åº¦å­¦ä¹  ç®—æ³•å·¥ç¨‹ ğŸ“ åšå®¢å†…å®¹åœ¨è¿™ä¸ªåšå®¢ä¸­ï¼Œæˆ‘ä¼šåˆ†äº«ï¼š æŠ€æœ¯å­¦ä¹ ç¬”è®° é¡¹ç›®å¼€å‘ç»éªŒ å·¥å…·ä½¿ç”¨å¿ƒå¾— ç”Ÿæ´»æ„Ÿæ‚Ÿ ğŸ“« è”ç³»æ–¹å¼å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–æƒ³è¦äº¤æµï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ï¼š GitHub: nash635 Email: shaj24@mails.tsinghua.edu.cn æ„Ÿè°¢æ‚¨çš„è®¿é—®ï¼å¸Œæœ›æˆ‘çš„åˆ†äº«èƒ½å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚"},{"title":"å‹é“¾","date":"2025-11-10T08:32:58.192Z","updated":"2025-11-10T08:32:58.192Z","comments":true,"path":"friends/index.html","permalink":"https://nash635.github.io/friends/","excerpt":"","text":"å‹æƒ…é“¾æ¥æ¬¢è¿äº¤æ¢å‹é“¾ï¼"},{"title":"æ ‡ç­¾","date":"2025-07-23T17:19:36.000Z","updated":"2025-11-10T08:32:58.194Z","comments":false,"path":"tags/index.html","permalink":"https://nash635.github.io/tags/","excerpt":"","text":""},{"title":"è¯¾ç¨‹ç¬”è®°","date":"2025-11-10T10:25:12.513Z","updated":"2025-11-10T10:25:12.513Z","comments":true,"path":"categories/course-notes/index.html","permalink":"https://nash635.github.io/categories/course-notes/","excerpt":"","text":""},{"title":"å…¶ä»–","date":"2025-11-10T10:25:12.574Z","updated":"2025-11-10T10:25:12.574Z","comments":true,"path":"categories/other/index.html","permalink":"https://nash635.github.io/categories/other/","excerpt":"","text":""},{"title":"è¯»ä¹¦ç¬”è®°","date":"2025-11-10T10:25:12.533Z","updated":"2025-11-10T10:25:12.533Z","comments":true,"path":"categories/reading-notes/index.html","permalink":"https://nash635.github.io/categories/reading-notes/","excerpt":"","text":""},{"title":"éšç¬”","date":"2025-11-10T10:25:12.556Z","updated":"2025-11-10T10:25:12.556Z","comments":true,"path":"categories/essay/index.html","permalink":"https://nash635.github.io/categories/essay/","excerpt":"","text":""},{"title":"æŠ€æœ¯ç ”ç©¶","date":"2025-11-10T10:25:12.491Z","updated":"2025-11-10T10:25:12.491Z","comments":true,"path":"categories/tech-research/index.html","permalink":"https://nash635.github.io/categories/tech-research/","excerpt":"","text":""}],"posts":[{"title":"Transformer Engine æ¶æ„è®¾è®¡åˆ†æ","slug":"TransformerEngine_DeepDive","date":"2025-11-17T02:00:03.000Z","updated":"2025-11-17T14:42:49.955Z","comments":true,"path":"2025/11/17/TransformerEngine_DeepDive/","permalink":"https://nash635.github.io/2025/11/17/TransformerEngine_DeepDive/","excerpt":"","text":"Transformer Engine æ¶æ„è®¾è®¡åˆ†ææ–‡æ¡£1. é¡¹ç›®æ¦‚è¿°1.1 é¡¹ç›®ç®€ä»‹Transformer Engine (TE) æ˜¯ NVIDIA å¼€å‘çš„é«˜æ€§èƒ½ Transformer æ¨¡å‹åŠ é€Ÿåº“ï¼Œä¸“é—¨ç”¨äºåœ¨ NVIDIA GPUï¼ˆHopperã€Adaã€Blackwell æ¶æ„ï¼‰ä¸ŠåŠ é€Ÿ Transformer æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚æ ¸å¿ƒç‰¹æ€§æ˜¯æ”¯æŒ 8 ä½æµ®ç‚¹æ•°ï¼ˆFP8ï¼‰ç²¾åº¦ï¼Œåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½å†…å­˜å ç”¨å’Œæå‡æ€§èƒ½ã€‚ 1.2 æ ¸å¿ƒç‰¹æ€§ FP8&#x2F;FP4 æ··åˆç²¾åº¦è®­ç»ƒï¼šæ”¯æŒ E4M3ã€E5M2ã€NVFP4 ç­‰å¤šç§ä½ç²¾åº¦æ ¼å¼ æ¡†æ¶æ— å…³ï¼šæä¾› C++ API ä»¥åŠ PyTorchã€JAX çš„ Python ç»‘å®š é«˜åº¦ä¼˜åŒ–ï¼šèåˆç®—å­ã€ä¼˜åŒ–çš„ GEMM å†…æ ¸ã€cuDNN é›†æˆ CPU Offloadï¼šæ”¯æŒæ¿€æ´»å€¼å¸è½½åˆ° CPUï¼Œé™ä½ GPU å†…å­˜å ç”¨ æ˜“ç”¨æ€§ï¼šç±»ä¼¼ automatic mixed precision çš„ API è®¾è®¡ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼šå†…ç½® MPIã€NVSHMEM æ”¯æŒ 1.3 æŠ€æœ¯æ ˆ æ ¸å¿ƒè¯­è¨€ï¼šC++ã€CUDA Python ç»‘å®šï¼šPybind11 æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorch 2.1+ã€JAX æ„å»ºç³»ç»Ÿï¼šCMakeã€setuptools ä¾èµ–åº“ï¼š CUDA 12.1+ï¼šGPU ç¼–ç¨‹åŸºç¡€ cuBLASLtï¼šNVIDIA é«˜æ€§èƒ½çº¿æ€§ä»£æ•°åº“ï¼ˆLightweight ç‰ˆæœ¬ï¼‰ï¼Œæä¾› FP8 GEMM æ”¯æŒ cuDNNï¼šæ·±åº¦ç¥ç»ç½‘ç»œåŠ é€Ÿåº“ï¼Œæä¾›èåˆæ³¨æ„åŠ›å®ç° cutlassï¼šCUDA æ¨¡æ¿åŒ–çº¿æ€§ä»£æ•°åº“ï¼Œç”¨äºè‡ªå®šä¹‰é«˜æ€§èƒ½ kernel NCCLï¼ˆå¯é€‰ï¼‰ï¼šå¤š GPU é€šä¿¡ NVSHMEMï¼ˆå¯é€‰ï¼‰ï¼šå¯¹ç§°å†…å­˜è®¿é—®ï¼Œä½å»¶è¿Ÿé€šä¿¡ 2. ç³»ç»Ÿæ¶æ„è®¾è®¡2.1 æ•´ä½“æ¶æ„å±‚æ¬¡12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455graph TB subgraph &quot;ç”¨æˆ·å±‚ (User Layer)&quot; A[PyTorch/JAX åº”ç”¨ä»£ç ] end subgraph &quot;æ¡†æ¶é€‚é…å±‚ (Framework Adapter Layer)&quot; B1[transformer_engine.pytorch] B2[transformer_engine.jax] end subgraph &quot;Python API å±‚ (Python API Layer)&quot; C1[Module API&lt;br/&gt;Linear, LayerNorm, Attention] C2[Quantization API&lt;br/&gt;fp8_autocast, recipes] C3[Distributed API&lt;br/&gt;checkpoint, communication] C4[Tensor API&lt;br/&gt;Float8Tensor, Quantizer] end subgraph &quot;æ ¸å¿ƒè®¡ç®—å±‚ (Core Compute Layer)&quot; D[transformer_engine.common&lt;br/&gt;Framework-Agnostic C++ Library] end subgraph &quot;ç®—å­å±‚ (Operator Layer)&quot; E1[GEMM Operators&lt;br/&gt;cuBLAS, cutlass] E2[Normalization&lt;br/&gt;LayerNorm, RMSNorm] E3[Attention&lt;br/&gt;Fused Attention, FlashAttention] E4[Activation&lt;br/&gt;GELU, SwiGLU] E5[Communication&lt;br/&gt;NCCL, NVSHMEM] end subgraph &quot;ç¡¬ä»¶å±‚ (Hardware Layer)&quot; F[NVIDIA GPU&lt;br/&gt;Hopper/Ada/Blackwell + Tensor Cores] end A --&gt; B1 A --&gt; B2 B1 --&gt; C1 B1 --&gt; C2 B1 --&gt; C3 B1 --&gt; C4 B2 --&gt; C1 B2 --&gt; C2 C1 --&gt; D C2 --&gt; D C3 --&gt; D C4 --&gt; D D --&gt; E1 D --&gt; E2 D --&gt; E3 D --&gt; E4 D --&gt; E5 E1 --&gt; F E2 --&gt; F E3 --&gt; F E4 --&gt; F E5 --&gt; F 2.2 æ¨¡å—æ¶æ„1234567891011121314151617181920212223242526272829303132333435363738394041graph LR subgraph &quot;transformer_engine&quot; A[__init__.py] subgraph &quot;common (C++æ ¸å¿ƒ)&quot; B1[transformer_engine.cpp] B2[gemm/] B3[normalization/] B4[fused_attn/] B5[activation/] B6[recipe/] end subgraph &quot;pytorch (PyTorchç»‘å®š)&quot; C1[module/] C2[attention/] C3[quantization.py] C4[distributed.py] C5[tensor/] C6[ops/] end subgraph &quot;jax (JAXç»‘å®š)&quot; D1[flax/] D2[attention.py] D3[dense.py] D4[layernorm.py] end end A --&gt; B1 A --&gt; C1 A --&gt; D1 B1 --&gt; B2 B1 --&gt; B3 B1 --&gt; B4 B1 --&gt; B5 C1 --&gt; C2 C1 --&gt; C3 C1 --&gt; C4 C1 --&gt; C5 3. æ ¸å¿ƒç»„ä»¶è¯¦ç»†è®¾è®¡3.1 Common Layer (æ ¸å¿ƒ C++ åº“)3.1.1 èŒè´£ å®ç°æ¡†æ¶æ— å…³çš„æ ¸å¿ƒè®¡ç®—é€»è¾‘ ç®¡ç† FP8&#x2F;FP4 é‡åŒ–å’Œç¼©æ”¾å› å­ æä¾›é«˜æ€§èƒ½ CUDA å†…æ ¸ ç±»å‹è½¬æ¢å’Œå†…å­˜ç®¡ç† 3.1.2 å…³é”®æ¨¡å—transformer_engine.cpp å¼ é‡ç±»å‹å®šä¹‰å’ŒéªŒè¯ DType æšä¸¾ï¼ˆkFloat32, kFloat16, kBFloat16, kFloat8E4M3, kFloat8E5M2, kFloat4E2M1ï¼‰ ç¼©æ”¾æ¨¡å¼ç®¡ç†ï¼ˆNVTE_DELAYED_TENSOR_SCALING, NVTE_MXFP8_1D_SCALING, NVTE_BLOCK_SCALING_1D&#x2F;2D, NVTE_NVFP4_1D_SCALINGï¼‰ å¼ é‡åˆæ³•æ€§æ£€æŸ¥ gemm&#x2F; (çŸ©é˜µä¹˜æ³•) cublaslt_gemm.cuï¼šåŸºäº cuBLASLt çš„é«˜æ€§èƒ½ GEMM å®ç° æ”¯æŒ FP8&#x2F;FP16&#x2F;BF16 æ··åˆç²¾åº¦ Fast Accumulatorã€Epilogue Fusionã€è‡ªåŠ¨è°ƒä¼˜ è¯¦è§ 11.4 èŠ‚ cuBLASLt è¯¦ç»†å¯¹æ¯” cutlass_grouped_gemm.cuï¼šåŸºäº cutlass çš„åˆ†ç»„ GEMMï¼ˆç”¨äº MoEï¼‰ normalization&#x2F; (å½’ä¸€åŒ–) LayerNormã€RMSNorm çš„ FP8 å®ç° èåˆçš„ bias å’Œ dropout æ“ä½œ Zero-centered gamma æ”¯æŒ fused_attn&#x2F; (èåˆæ³¨æ„åŠ›) åŸºäº cuDNN çš„èåˆæ³¨æ„åŠ›å®ç° æ”¯æŒå¤šç§ mask ç±»å‹ï¼ˆcausal, padding, arbitraryï¼‰ æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSliding Window Attentionï¼‰ FlashAttention é›†æˆ activation&#x2F; (æ¿€æ´»å‡½æ•°) GELU, ReLU, SwiGLU, GEGLU ç­‰ æ”¯æŒ FP8 è¾“å…¥è¾“å‡º èåˆå®ç°ä»¥å‡å°‘å†…å­˜è®¿é—® 3.2 PyTorch Adapter Layer3.2.1 Module å­ç³»ç»Ÿ12345678910111213141516171819202122232425262728293031323334353637classDiagram class TransformerEngineBaseModule &#123; +fp8: bool +fp8_calibration: bool +fp8_parameters: bool +forward() +_get_fp8_params() &#125; class Linear &#123; +in_features: int +out_features: int +weight: Parameter +bias: Optional[Parameter] +forward(inp: Tensor) &#125; class LayerNormLinear &#123; +normalization: str +forward(inp: Tensor) &#125; class LayerNormMLP &#123; +activation: str +ffn_hidden_size: int +forward(inp: Tensor) &#125; class GroupedLinear &#123; +num_gemms: int +forward(inp: Tensor) &#125; TransformerEngineBaseModule &lt;|-- Linear TransformerEngineBaseModule &lt;|-- LayerNormLinear TransformerEngineBaseModule &lt;|-- LayerNormMLP TransformerEngineBaseModule &lt;|-- GroupedLinear æ ¸å¿ƒæ¨¡å—ï¼š Linearï¼šåŸºç¡€çº¿æ€§å±‚ï¼Œæ”¯æŒ FP8 æƒé‡å’Œæ¿€æ´» LayerNormLinearï¼šèåˆ LayerNorm + Linear LayerNormMLPï¼šèåˆ LayerNorm + MLPï¼ˆä¸¤å±‚çº¿æ€§å±‚ï¼‰ GroupedLinearï¼šåˆ†ç»„çº¿æ€§å±‚ï¼ˆç”¨äº MoE ç­‰åœºæ™¯ï¼‰ LayerNorm&#x2F;RMSNormï¼šå½’ä¸€åŒ–å±‚ 3.2.2 Attention å­ç³»ç»Ÿ1234567graph TD A[MultiheadAttention] --&gt; B[DotProductAttention] A --&gt; C[RotaryPositionEmbedding] B --&gt; D[Fused Attention Backend] B --&gt; E[Unfused Attention] D --&gt; F[cuDNN Fused Attn] D --&gt; G[FlashAttention] å…³é”®ç‰¹æ€§ï¼š Multi-Query Attention (MQA) Grouped Query Attention (GQA) å¤šç§ mask ç±»å‹æ”¯æŒ RoPEï¼ˆRotary Position Embeddingï¼‰ Sliding Window Attention InferenceParams for KV cache FP8 DPA (Dot Product Attention) æ”¯æŒ FP8 MHA (Multi-Head Attention) ç«¯åˆ°ç«¯ä¼˜åŒ– 3.2.3 Quantization å­ç³»ç»Ÿ123456789101112131415graph TD A[fp8_autocast Context Manager] --&gt; B&#123;Recipe Type&#125; B --&gt;|DelayedScaling| C[Delayed Scaling&lt;br/&gt;å»¶è¿Ÿç¼©æ”¾å› å­æ›´æ–°] B --&gt;|Float8CurrentScaling| D[Current Scaling&lt;br/&gt;å½“å‰æ‰¹æ¬¡ç¼©æ”¾] B --&gt;|MXFP8BlockScaling| E[MXFP8 Block Scaling&lt;br/&gt;1D å—çº§ç¼©æ”¾] B --&gt;|Float8BlockScaling| F[FP8 Block Scaling&lt;br/&gt;1D/2D å—çº§ç¼©æ”¾] B --&gt;|NVFP4BlockScaling| G[NVFP4 Block Scaling&lt;br/&gt;4-bit é‡åŒ–] C --&gt; H[FP8MetaManager] D --&gt; H E --&gt; H F --&gt; H G --&gt; H H --&gt; I[Forward/Backward Pass&lt;br/&gt;with FP8] Recipe ç³»ç»Ÿï¼š DelayedScalingï¼šä½¿ç”¨å†å² amax ç»Ÿè®¡æ›´æ–°ç¼©æ”¾å› å­ Float8CurrentScalingï¼šåŸºäºå½“å‰æ‰¹æ¬¡çš„ amax MXFP8BlockScalingï¼šMicroscaling FP8ï¼ˆé€‚ç”¨äº Blackwellï¼‰ Float8BlockScalingï¼šå—çº§é‡åŒ–ï¼ˆ1D&#x2F;2Dï¼‰ NVFP4BlockScalingï¼š4-bit é‡åŒ–ï¼ˆBlackwell ä¸“ç”¨ï¼‰ 3.2.4 Tensor å­ç³»ç»Ÿ12345678910111213141516171819202122232425262728classDiagram class QuantizedTensor &#123; +data: Tensor +scale_inv: Tensor +quantizer: Quantizer +dtype: DType &#125; class Float8Tensor &#123; +Float8Quantizer &#125; class MXFP8Tensor &#123; +MXFP8Quantizer &#125; class Float8BlockwiseQTensor &#123; +Float8BlockQuantizer &#125; class NVFP4Tensor &#123; +NVFP4Quantizer &#125; QuantizedTensor &lt;|-- Float8Tensor QuantizedTensor &lt;|-- MXFP8Tensor QuantizedTensor &lt;|-- Float8BlockwiseQTensor QuantizedTensor &lt;|-- NVFP4Tensor 3.3 JAX Adapter Layer3.3.1 Flax é›†æˆ æä¾› Flax å…¼å®¹çš„ Module JAX JIT ç¼–è¯‘æ”¯æŒ XLA FFIï¼ˆForeign Function Interfaceï¼‰é›†æˆ 3.3.2 æ ¸å¿ƒæ¨¡å— Denseï¼šçº¿æ€§å±‚ LayerNormï¼šå½’ä¸€åŒ–å±‚ DotProductAttentionï¼šæ³¨æ„åŠ›æœºåˆ¶ LayerNormMLPï¼šèåˆ MLP 4. æ•°æ®æµä¸è°ƒç”¨å…³ç³» æ—¶åºå›¾ç¬¦å·è¯´æ˜ï¼š A-&gt;&gt;Bï¼ˆå®çº¿ç®­å¤´ï¼‰ï¼šA è°ƒç”¨ B çš„æ–¹æ³•æˆ–å‘é€æ¶ˆæ¯ B--&gt;&gt;Aï¼ˆè™šçº¿ç®­å¤´ï¼‰ï¼šB è¿”å›ç»“æœç»™ Aï¼ˆå‡½æ•°è¿”å›å€¼æˆ–å“åº”ï¼‰ activate/deactivateï¼šè¡¨ç¤ºç»„ä»¶å¤„äºæ´»è·ƒçŠ¶æ€çš„ç”Ÿå‘½å‘¨æœŸ ç®­å¤´ä¸Šçš„æ–‡å­—ï¼šè¯´æ˜å…·ä½“çš„è°ƒç”¨æˆ–è¿”å›å†…å®¹ 4.1 å‰å‘ä¼ æ’­æ•°æ®æµ123456789101112131415161718192021222324252627282930313233343536sequenceDiagram participant User as ç”¨æˆ·ä»£ç  participant Autocast as fp8_autocast participant Module as TE Module&lt;br/&gt;(e.g., Linear) participant Quantizer as Quantizer participant CPP as C++ Backend participant CUDA as CUDA Kernel User-&gt;&gt;Autocast: with fp8_autocast(recipe) activate Autocast Autocast-&gt;&gt;Autocast: è®¾ç½® FP8 Recipe User-&gt;&gt;Module: forward(input) activate Module Module-&gt;&gt;Quantizer: é‡åŒ–è¾“å…¥ (FP32/BF16 â†’ FP8) Quantizer-&gt;&gt;CPP: nvte_cast_to_fp8() CPP-&gt;&gt;CUDA: Cast Kernel CUDA--&gt;&gt;CPP: FP8 Tensor CPP--&gt;&gt;Quantizer: FP8 Tensor Quantizer--&gt;&gt;Module: FP8 Input Module-&gt;&gt;CPP: nvte_fp8_gemm(input, weight) CPP-&gt;&gt;CUDA: cuBLASLt FP8 GEMM CUDA--&gt;&gt;CPP: FP8 Output CPP--&gt;&gt;Module: FP8 Output Module-&gt;&gt;Quantizer: åé‡åŒ–è¾“å‡º (FP8 â†’ FP32/BF16) Quantizer-&gt;&gt;CPP: nvte_cast_from_fp8() CPP-&gt;&gt;CUDA: Cast Kernel CUDA--&gt;&gt;CPP: High Precision Tensor CPP--&gt;&gt;Quantizer: Output Quantizer--&gt;&gt;Module: Output Module--&gt;&gt;User: Output deactivate Module deactivate Autocast 4.2 åå‘ä¼ æ’­ä¸ç¼©æ”¾å› å­æ›´æ–° ç‰¹åˆ«è¯´æ˜ï¼šåå‘ä¼ æ’­ä¸­çš„ç®­å¤´æ–¹å‘ä¸ forward ä¸åŒ å®çº¿ç®­å¤´ï¼šæ—¢å¯ä»¥è¡¨ç¤ºâ€è°ƒç”¨ backward()â€ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºâ€ä¼ é€’æ¢¯åº¦â€ åœ¨ PyTorch Autograd ä¸­ï¼Œbackward() ä¸æ˜¯ç®€å•çš„è¿”å›å€¼ï¼Œè€Œæ˜¯é“¾å¼è°ƒç”¨æœºåˆ¶ æ¯ä¸ªæ¨¡å—è®¡ç®—å®Œæ¢¯åº¦åï¼Œä¼šä¸»åŠ¨è°ƒç”¨å‰ä¸€ä¸ªèŠ‚ç‚¹çš„ backward()ï¼Œå› æ­¤ç”¨å®çº¿ 1234567891011121314151617181920212223242526272829303132sequenceDiagram participant Loss as Loss.backward() participant Module as TE Module participant Autograd as PyTorch Autograd participant CPP as C++ Backend participant Recipe as FP8 Recipe Loss-&gt;&gt;Autograd: è§¦å‘åå‘ä¼ æ’­ Autograd-&gt;&gt;Module: backward() activate Module Module-&gt;&gt;CPP: nvte_fp8_gemm_bwd() CPP-&gt;&gt;CPP: è®¡ç®—æ¢¯åº¦ (FP8) CPP--&gt;&gt;Module: æ¢¯åº¦ï¼ˆè¿”å›å€¼ï¼‰ Note over Module: è®¡ç®—å®Œæˆåè§¦å‘å‰ä¸€å±‚ Module-&gt;&gt;Autograd: è°ƒç”¨å‰ä¸€å±‚ backward() Note right of Module: è¿™æ˜¯ä¸»åŠ¨è°ƒç”¨ï¼Œ&lt;br/&gt;ä¸æ˜¯è¢«åŠ¨è¿”å› Module-&gt;&gt;Recipe: æ”¶é›† amax ç»Ÿè®¡ Recipe-&gt;&gt;Recipe: æ›´æ–°ç¼©æ”¾å› å­å†å² alt DelayedScaling Recipe-&gt;&gt;Recipe: amax_history.append(current_amax) Recipe-&gt;&gt;Recipe: scale = max(amax_history) * margin else Float8CurrentScaling Recipe-&gt;&gt;Recipe: scale = current_amax * margin end Recipe--&gt;&gt;Module: æ›´æ–°çš„ç¼©æ”¾å› å­ Module-&gt;&gt;Autograd: æ¢¯åº¦ä¼ æ’­ï¼ˆè°ƒç”¨å‰ä¸€å±‚ï¼‰ deactivate Module å…³é”®æ¦‚å¿µï¼š Forward Passï¼šA â†’ B â†’ Cï¼Œæ¯å±‚è¿”å›è¾“å‡ºï¼ˆè™šçº¿è¿”å›ï¼‰ Backward Passï¼šC.backward() â†’ B.backward() â†’ A.backward()ï¼Œé“¾å¼è°ƒç”¨ï¼ˆå®çº¿è°ƒç”¨ï¼‰ Backward ä¸æ˜¯ç®€å•çš„ returnï¼Œè€Œæ˜¯è§¦å‘å‰ä¸€å±‚çš„è®¡ç®—ï¼ˆAutograd çš„æ ¸å¿ƒæœºåˆ¶ï¼‰ 4.3 TransformerLayer å®Œæ•´è°ƒç”¨é“¾1234567891011121314151617181920212223242526272829303132333435graph TD A[TransformerLayer.forward] --&gt; B&#123;Parallel Attn-MLP?&#125; B --&gt;|No| C[Self Attention] C --&gt; D[Residual + Dropout] D --&gt; E[LayerNorm] E --&gt; F&#123;Layer Type?&#125; F --&gt;|Encoder| G[MLP] F --&gt;|Decoder| H[&quot;Cross Attention&lt;br/&gt;(éœ€è¦Encoderè¾“å‡º)&quot;] H --&gt; I[Residual + Dropout] I --&gt; G G --&gt; J[Residual + Dropout] J --&gt; K[Output] B --&gt;|Yes| L[&quot;è¾“å…¥: hidden_states&quot;] L --&gt; M[Self Attention Branch] L --&gt; N[MLP Branch] M --&gt; O[&quot;ç›´æ¥ç›¸åŠ &lt;br/&gt;(ä¸å«LayerNorm)&lt;br/&gt;+ Residual&quot;] N --&gt; O O --&gt; K subgraph &quot;Self Attention Detail&quot; C --&gt; C1[MultiheadAttention] C1 --&gt; C2[&quot;QKV Projection&lt;br/&gt;(LayerNormLinear)&quot;] C2 --&gt; C3[DotProductAttention] C3 --&gt; C4[&quot;Output Projection&lt;br/&gt;(Linear)&quot;] C4 --&gt; D end subgraph &quot;MLP Detail&quot; G --&gt; G1[LayerNormMLP] G1 --&gt; G2[FC1 + Activation] G2 --&gt; G3[FC2] G3 --&gt; J end å…³é”®æ•°æ®æµè¯´æ˜ï¼š 123456789101112131415161718192021222324252627282930313233343536373839# ========== ä»£ç å¯¹åº”ï¼ˆtransformer.py 756-840 è¡Œï¼‰ ==========# æ¨¡å¼1: æ ‡å‡†é¡ºåºæ¨¡å¼ (parallel_attention_mlp=False)# --------------------------------------------------------# 1. Self Attention å®Œæ•´æµç¨‹attention_output = self.self_attention(hidden_states) # MultiheadAttention å†…éƒ¨æµç¨‹ (multi_head_attention.py 773-1002 è¡Œ)ï¼š # hidden_states â†’ LayerNormLinear (å«LayerNorm) â†’ QKVåˆ†ç¦» # â†’ DotProductAttention (Q*K^T, softmax, *V) # â†’ Output Projection Linear â†’ attention_output# 2. ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥ + Dropouthidden_states = bias_dropout_add(attention_output, residual)# 3. Decoder æ¨¡å¼é¢å¤–çš„ Cross Attentionif layer_type == &quot;decoder&quot;: # Query æ¥è‡ª Self Attention è¾“å‡ºï¼ŒKey/Value æ¥è‡ª Encoder è¾“å‡º cross_attn_output = self.inter_attention( hidden_states, # Query encoder_output=encoder_output # Key/Value ) hidden_states = bias_dropout_add(cross_attn_output, hidden_states)# 4. MLP æµç¨‹mlp_output = self.layernorm_mlp(hidden_states) # å†…éƒ¨æµç¨‹ï¼šLayerNorm â†’ FC1 â†’ Activation â†’ FC2# 5. ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥ + Dropoutoutput = bias_dropout_add(mlp_output, hidden_states)# æ¨¡å¼2: å¹¶è¡Œæ³¨æ„åŠ›-MLPæ¨¡å¼ (parallel_attention_mlp=True)# --------------------------------------------------------# æ³¨æ„åŠ›å’ŒMLPå¹¶è¡Œè®¡ç®—ï¼Œç„¶åç›´æ¥ç›¸åŠ self_attention_outputs = self.self_attention(hidden_states)mlp_outputs = self.layernorm_mlp(hidden_states) # åŒæ—¶è®¡ç®—output = bias_dropout_add( self_attention_outputs + mlp_outputs, # ç›´æ¥ç›¸åŠ  hidden_states # æ®‹å·®) æ³¨æ„ï¼šå¾ˆå¤šæ¶æ„å›¾ä¼šå°† Self Attention ç”»æˆä¸€ä¸ªæ•´ä½“æ¨¡å—ï¼Œä»é¡¶éƒ¨å¼•å‡ºè¾“å‡ºçº¿ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•è™½ç„¶ç®€æ´ï¼Œä½†å®¹æ˜“è®©äººè¯¯è§£ Output Projection Linear æ²¡æœ‰å¯¹å¤–è¾“å‡ºã€‚å®é™…ä¸Šï¼ŒOutput Projection çš„è¾“å‡ºå°±æ˜¯æ•´ä¸ª Self Attention æ¨¡å—çš„è¾“å‡ºã€‚ 4.4 æ¶æ„å›¾å¸¸è§è¯¯è§£ä¸ä»£ç å¯¹ç…§é€šè¿‡ä»£ç  reviewï¼Œä»¥ä¸‹æ˜¯å®¹æ˜“è¯¯è§£çš„å‡ ä¸ªè¦ç‚¹ï¼š æ¶æ„å›¾è¡¨ç¤º å¸¸è§è¯¯è§£ å®é™…ä»£ç å®ç° ä»£ç ä½ç½® QKV Projection (LayerNormLinear) LayerNorm åœ¨æŠ•å½±ä¹‹å‰å•ç‹¬å­˜åœ¨ LayerNormLinear å†…éƒ¨å·²åŒ…å« LayerNormæ˜¯èåˆå®ç° multi_head_attention.py:773self.qkv(hidden_states) Output Projection æ— è¾“å‡ºè¿æ¥ åªæœ‰é¡¶éƒ¨çš„è¿æ¥ï¼Œåº•éƒ¨æ²¡è¾“å‡º Output Projection çš„è¾“å‡ºå°±æ˜¯ Self Attention æ¨¡å—çš„è¾“å‡ºç›´æ¥è¿åˆ° Residual + Dropout multi_head_attention.py:1002-1018return (attention_output, ...) å¹¶è¡Œæ¨¡å¼çš„â€åˆå¹¶â€ ä¸¤ä¸ªåˆ†æ”¯å„è‡ªåšresidualå†åˆå¹¶ ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºç›´æ¥ç›¸åŠ åªåšä¸€æ¬¡ residual transformer.py:836-840bias_dropout_add(attn + mlp, residual) Decoder Cross Attention å›¾ä¸­åªæ˜¾ç¤ºå•è¾“å…¥ éœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼šâ€¢ Query: Self Attn è¾“å‡ºâ€¢ Key&#x2F;Value: Encoder è¾“å‡º transformer.py:789-819inter_attention(hidden_states, encoder_output) LayerNorm ä½ç½® åœ¨æ¯ä¸ªæ¨¡å—å¤–éƒ¨ Pre-LNï¼šåœ¨æ¨¡å—å†…éƒ¨Post-LNï¼šåœ¨æ¨¡å—å¤–éƒ¨TE é»˜è®¤ä½¿ç”¨ Pre-LN multi_head_attention.py:input_layernorm=True å…³é”®å‘ç°ï¼š èåˆç®—å­ï¼šLayerNormLinear å’Œ LayerNormMLP éƒ½æ˜¯èåˆå®ç°ï¼Œå†…éƒ¨åŒ…å« LayerNorm Pre-Layer Normï¼šTE é»˜è®¤ä½¿ç”¨ Pre-LN æ¶æ„ï¼ˆLayerNorm åœ¨ Attention&#x2F;MLP ä¹‹å‰ï¼‰ æ®‹å·®è¿æ¥æ—¶æœºï¼šæ¯ä¸ªä¸»è¦æ¨¡å—ï¼ˆAttentionã€MLPï¼‰ä¹‹åéƒ½æœ‰æ®‹å·®è¿æ¥ å¹¶è¡Œæ¨¡å¼ï¼šæ˜¯çœŸæ­£çš„å¹¶è¡Œï¼ˆåŒæ—¶è®¡ç®—ï¼‰ï¼Œä¸æ˜¯ä¸²è¡Œåä¼ªè£…çš„å¹¶è¡Œ 5. å…³é”®æŠ€æœ¯å®ç°5.1 FP8 é‡åŒ–ç­–ç•¥5.1.1 å»¶è¿Ÿç¼©æ”¾ï¼ˆDelayed Scalingï¼‰1234567891011# ä¼ªä»£ç class DelayedScaling: def __init__(self, margin=0, interval=1, fp8_format=E4M3): self.amax_history = deque(maxlen=interval) self.margin = margin def get_scale(self, tensor): amax = tensor.abs().max() self.amax_history.append(amax) scale = max(self.amax_history) * (1 + self.margin) / fp8_max return scale 5.1.2 å—çº§ç¼©æ”¾ï¼ˆBlock Scalingï¼‰ å°†å¼ é‡åˆ†æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—ç‹¬ç«‹è®¡ç®—ç¼©æ”¾å› å­ å‡å°‘é‡åŒ–è¯¯å·®ï¼Œæé«˜ç²¾åº¦ é€‚ç”¨äºæƒé‡çŸ©é˜µå’Œæ¿€æ´» 5.2 èåˆç®—å­ä¼˜åŒ–5.2.1 LayerNorm + Linear èåˆ1234567891011è¾“å…¥: [seq_len, batch, hidden_dim]â†“LayerNorm (èåˆ bias + dropout)â†“FP8 Castâ†“FP8 GEMM (weight é¢„å…ˆé‡åŒ–)â†“FP8 Cast Backâ†“è¾“å‡º: [seq_len, batch, out_dim] ä¼˜åŠ¿ï¼š å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•° é™ä½é‡åŒ–&#x2F;åé‡åŒ–å¼€é”€ æé«˜ GPU åˆ©ç”¨ç‡ 5.2.2 Fused Attention ä½¿ç”¨ cuDNN èåˆçš„æ³¨æ„åŠ›å®ç° æ”¯æŒ FlashAttention-2 åç«¯ è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç° 5.3 åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ5.3.1 User Buffer (UB) é‡å é€šä¿¡å’Œè®¡ç®— æ”¯æŒ FP8 é€šä¿¡ åŸºäº NVSHMEM çš„ä½å»¶è¿Ÿé€šä¿¡ 5.3.2 Checkpoint12345678# åˆ†å¸ƒå¼ checkpointfrom transformer_engine.pytorch import checkpoint# ä¿å­˜checkpoint(model, optimizer, save_dir, dist_group)# åŠ è½½checkpoint.load(model, optimizer, load_dir, dist_group) 6. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥6.1 å†…å­˜ä¼˜åŒ– FP8 é‡åŒ–ï¼šå‡å°‘ 50% å†…å­˜å ç”¨ Gradient Checkpointingï¼šé‡è®¡ç®—ä¸­é—´æ¿€æ´» CPU Offloadingï¼šå°†æ¿€æ´»å€¼å¸è½½åˆ° CPU å†…å­˜ ä½¿ç”¨ get_cpu_offload_context() ç®¡ç†å¸è½½ æ”¯æŒå¼‚æ­¥æ•°æ®ä¼ è¾“ï¼Œå‡å°‘æ€§èƒ½å½±å“ é€‚ç”¨äºè¶…å¤§æ¨¡å‹è®­ç»ƒ 6.2 è®¡ç®—ä¼˜åŒ– Tensor Core åˆ©ç”¨ï¼šFP8 Tensor Core ååé‡æ˜¯ FP16 çš„ 2 å€ èåˆç®—å­ï¼šå‡å°‘ kernel launch å¼€é”€ cuDNN é›†æˆï¼šä½¿ç”¨ cuDNN ä¼˜åŒ–çš„æ³¨æ„åŠ›å®ç° 6.3 é€šä¿¡ä¼˜åŒ– FP8 All-Reduceï¼šå‡å°‘é€šä¿¡æ•°æ®é‡ é€šä¿¡è®¡ç®—é‡å ï¼šUser Buffer æœºåˆ¶ NVSHMEMï¼šä½å»¶è¿Ÿç‚¹å¯¹ç‚¹é€šä¿¡ 7. æ‰©å±•æ€§ä¸ä¾èµ–7.1 è‡ªå®šä¹‰ Recipe123456from transformer_engine.common.recipe import CustomRecipecustom_recipe = CustomRecipe( margin=0.1, fp8_format=Format.HYBRID, amax_history_len=10, amax_compute_algo=&#x27;max&#x27;) 7.2 è‡ªå®šä¹‰ç®—å­é€šè¿‡ C++ API æ‰©å±•æ–°ç®—å­ï¼Œä½¿ç”¨ Pybind11 æš´éœ²åˆ° Python 7.3 æ¡†æ¶æ”¯æŒPyTorchï¼ˆå®Œæ•´æ”¯æŒï¼‰ã€JAX&#x2F;Flaxã€å…¶ä»–æ¡†æ¶ï¼ˆé€šè¿‡ C++ APIï¼‰ 7.4 ä¾èµ–å…³ç³»1234567graph TD A[TransformerEngine] --&gt; B[CUDA 12.1+] A --&gt; C[cuDNN] A --&gt; D[cuBLASLt] A --&gt; E[PyTorch 2.1+ / JAX] A --&gt; F[cutlass] A -.-&gt; G[NCCL/MPI/NVSHMEM&lt;br/&gt;å¯é€‰] 8. ä½¿ç”¨ç¤ºä¾‹PyTorch åŸºç¡€ç¤ºä¾‹12345678910111213141516import torchimport transformer_engine.pytorch as tefrom transformer_engine.common import recipe# åˆ›å»ºæ¨¡å‹model = te.Linear(768, 3072, bias=True)# åˆ›å»º FP8 Recipefp8_recipe = recipe.DelayedScaling( margin=0, fp8_format=recipe.Format.HYBRID)# å‰å‘ä¼ æ’­ï¼ˆå¯ç”¨ FP8ï¼‰with te.fp8_autocast(enabled=True, recipe=fp8_recipe): output = model(input) TransformerLayer ç¤ºä¾‹123456789101112131415layer = te.TransformerLayer( hidden_size=1024, ffn_hidden_size=4096, num_attention_heads=16, num_gqa_groups=8, # Grouped Query Attention layernorm_epsilon=1e-5, hidden_dropout=0.1, attention_dropout=0.1, self_attn_mask_type=&#x27;causal&#x27;, normalization=&#x27;RMSNorm&#x27;, activation=&#x27;swiglu&#x27;)with te.fp8_autocast(enabled=True, recipe=fp8_recipe): output = layer(hidden_states, attention_mask) CPU Offload ç¤ºä¾‹12345678910111213141516from transformer_engine.pytorch import get_cpu_offload_context# åˆ›å»º CPU Offload ä¸Šä¸‹æ–‡cpu_offload_ctx, sync_fn = get_cpu_offload_context( enabled=True, num_layers=24)# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨with cpu_offload_ctx: output = model(input) loss = criterion(output, target) # åŒæ­¥ç­‰å¾…å¼‚æ­¥ä¼ è¾“å®Œæˆsync_fn()loss.backward() 9. æ¶æ„è®¾è®¡ä¼˜åŠ¿ æ¨¡å—åŒ–ï¼šæ¸…æ™°åˆ†å±‚ï¼ˆUser API â†’ Adapter â†’ Core â†’ Hardwareï¼‰ï¼Œæ¾è€¦åˆæ˜“æ‰©å±• é«˜æ€§èƒ½ï¼šé›¶æ‹·è´è®¾è®¡ã€èåˆç®—å­ã€ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ– æ˜“ç”¨æ€§ï¼šPythonic APIã€è‡ªåŠ¨ç®¡ç†ç¼©æ”¾å› å­ã€çµæ´»çš„ Recipe ç³»ç»Ÿ å·¥ä¸šçº§ï¼šå®Œå¤‡æµ‹è¯•ã€é½å…¨æ–‡æ¡£ã€æŒç»­é›†æˆ 10. æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ10.1 æ•°å€¼ç¨³å®šæ€§æŒ‘æˆ˜ï¼šFP8 åŠ¨æ€èŒƒå›´å°ï¼Œå®¹æ˜“æº¢å‡º&#x2F;ä¸‹æº¢è§£å†³æ–¹æ¡ˆï¼š Delayed Scaling ç­–ç•¥ Amax å†å²è¿½è¸ª Margin å‚æ•°è°ƒæ•´ 10.2 æ€§èƒ½ç“¶é¢ˆæŒ‘æˆ˜ï¼šé‡åŒ–&#x2F;åé‡åŒ–å¼€é”€è§£å†³æ–¹æ¡ˆï¼š èåˆç®—å­ é¢„é‡åŒ–æƒé‡ ç«¯åˆ°ç«¯ FP8 æµç¨‹ 10.3 æ¡†æ¶å…¼å®¹æ€§æŒ‘æˆ˜ï¼šä¸åŒæ¡†æ¶çš„ autograd æœºåˆ¶ä¸åŒè§£å†³æ–¹æ¡ˆï¼š æ¡†æ¶æ— å…³çš„ C++ æ ¸å¿ƒ é€‚é…å±‚æŠ½è±¡å·®å¼‚ Custom Autograd Functions 10.4 cuBLASLt vs cuBLAS é€‰æ‹©æŒ‘æˆ˜ï¼šä¸ºä»€ä¹ˆä½¿ç”¨ cuBLASLt è€Œä¸æ˜¯ä¼ ç»Ÿ cuBLASï¼ŸåŸå› åˆ†æï¼š FP8 æ”¯æŒï¼šcuBLAS ä¸æ”¯æŒ FP8 æ•°æ®ç±»å‹ï¼ŒcuBLASLt ä» CUDA 11.8+ å¼€å§‹åŸç”Ÿæ”¯æŒ çµæ´»æ€§ï¼šcuBLASLt çš„æè¿°ç¬¦ï¼ˆDescriptorï¼‰API å…è®¸ç²¾ç»†æ§åˆ¶æ¯ä¸ªæ“ä½œç»†èŠ‚ æ€§èƒ½ä¼˜åŒ–ï¼š Fast Accumulatorï¼ˆå¿«é€Ÿç´¯åŠ å™¨ï¼‰ï¼šé’ˆå¯¹ Hopper æ¶æ„çš„åˆ†å—ç´¯åŠ ä¼˜åŒ– Epilogue Fusionï¼šå°† biasã€æ¿€æ´»å‡½æ•°ç­‰åå¤„ç†èåˆåˆ° GEMM kernel ä¸­ è‡ªåŠ¨è°ƒä¼˜ï¼šæ ¹æ®çŸ©é˜µå¤§å°å’Œç¡¬ä»¶ç‰¹æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³• ç¼©æ”¾å› å­é›†æˆï¼šç›´æ¥æ”¯æŒ FP8 çš„ scale å’Œ scale_inv å‚æ•°ï¼Œæ— éœ€é¢å¤–çš„ kernel launch ä»£ç å¯¹æ¯”ï¼š 12345678910// cuBLAS (ä¼ ç»Ÿ APIï¼Œä¸æ”¯æŒ FP8)cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &amp;alpha, A, lda, B, ldb, &amp;beta, C, ldc);// cuBLASLt (æè¿°ç¬¦ APIï¼Œæ”¯æŒ FP8)cublasLtMatmul(handle, matmulDesc, &amp;alpha, A, Adesc, // å¯æŒ‡å®š FP8 ç±»å‹å’Œç¼©æ”¾å› å­ B, Bdesc, &amp;beta, C, Cdesc, C, Cdesc, &amp;algo, workspace, workspaceSize, stream); 11. æœªæ¥å‘å±•æ–¹å‘12.1 æ–°ç¡¬ä»¶æ”¯æŒ Blackwell æ¶æ„ä¼˜åŒ–ï¼šMXFP8ã€NVFP4 å¤š GPU æ¶æ„ï¼šæ›´å¥½çš„å¤šå¡æ”¯æŒ 12.2 æ–°åŠŸèƒ½ æ›´å¤šèåˆç®—å­ï¼šSoftmax, Dropout ç­‰ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ æ··åˆç²¾åº¦ç­–ç•¥ä¼˜åŒ– 12.3 ç”Ÿæ€ç³»ç»Ÿé›†æˆ Megatron-LM é›†æˆ HuggingFace Transformers æ”¯æŒ ONNX å¯¼å‡ºä¼˜åŒ– 12. æ€»ç»“Transformer Engine æ˜¯ NVIDIA å¼€å‘çš„é«˜åº¦æ¨¡å—åŒ–ã€æ€§èƒ½ä¼˜å…ˆã€æ˜“äºä½¿ç”¨çš„ Transformer åŠ é€Ÿåº“ã€‚å…¶æ ¸å¿ƒæ¶æ„è®¾è®¡ä¸ºå¤§è§„æ¨¡ Transformer æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†æä¾›äº†åšå®åŸºç¡€ï¼Œç‰¹ç‚¹åŒ…æ‹¬ï¼šåˆ†å±‚æ¸…æ™°ã€æ¡†æ¶æ— å…³ã€é«˜æ€§èƒ½ FP8 é‡åŒ–ã€æ˜“æ‰©å±•çš„ Recipe ç³»ç»Ÿï¼Œä»¥åŠå·¥ä¸šçº§çš„æµ‹è¯•å’Œæ–‡æ¡£ä½“ç³»ã€‚ é™„å½•å…³é”®æ–‡ä»¶ï¼š C++ æ ¸å¿ƒï¼štransformer_engine/common/ (transformer_engine.cpp, gemm&#x2F;, fused_attn&#x2F;, normalization&#x2F;) PyTorchï¼štransformer_engine/pytorch/ (module&#x2F;, attention&#x2F;, quantization.py) JAXï¼štransformer_engine/jax/ (flax&#x2F;, attention.py) èµ„æºé“¾æ¥ï¼š å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.nvidia.com/deeplearning/transformer-engine/ GitHubï¼šhttps://github.com/NVIDIA/TransformerEngine","categories":[{"name":"ç³»ç»Ÿæ¶æ„åˆ†æ","slug":"ç³»ç»Ÿæ¶æ„åˆ†æ","permalink":"https://nash635.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/"}],"tags":[{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://nash635.github.io/tags/NVIDIA/"},{"name":"Transformer Engine","slug":"Transformer-Engine","permalink":"https://nash635.github.io/tags/Transformer-Engine/"},{"name":"FP8","slug":"FP8","permalink":"https://nash635.github.io/tags/FP8/"},{"name":"æ··åˆç²¾åº¦","slug":"æ··åˆç²¾åº¦","permalink":"https://nash635.github.io/tags/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6/"},{"name":"GPUåŠ é€Ÿ","slug":"GPUåŠ é€Ÿ","permalink":"https://nash635.github.io/tags/GPU%E5%8A%A0%E9%80%9F/"}]},{"title":"Megatron-LM æ¶æ„æ·±åº¦åˆ†æ","slug":"Megatron_DeepDive","date":"2025-11-17T02:00:02.000Z","updated":"2025-11-17T14:42:49.955Z","comments":true,"path":"2025/11/17/Megatron_DeepDive/","permalink":"https://nash635.github.io/2025/11/17/Megatron_DeepDive/","excerpt":"","text":"Megatron-LM ä»£ç åº“æ¶æ„åˆ†ææŠ¥å‘Šç›®å½• é¡¹ç›®æ¦‚è¿° æ•´ä½“æ¶æ„ æ ¸å¿ƒæ¨¡å—åˆ†æ å¹¶è¡Œç­–ç•¥è¯¦è§£ æ¨¡å‹å®ç° è®­ç»ƒæµç¨‹ å…³é”®æŠ€æœ¯ç‰¹æ€§ ä»£ç ç»„ç»‡ç»“æ„ 1. é¡¹ç›®æ¦‚è¿°1.1 é¡¹ç›®å®šä½Megatron-LM æ˜¯ NVIDIA å¼€å‘çš„ç”¨äºå¤§è§„æ¨¡ Transformer æ¨¡å‹è®­ç»ƒçš„ GPU ä¼˜åŒ–åº“ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼š Megatron-LM: å‚è€ƒå®ç°ï¼ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒè„šæœ¬å’Œå·¥å…· Megatron Core: å¯ç»„åˆçš„ç”Ÿäº§çº§åº“ï¼Œæä¾›æ¨¡å—åŒ–çš„æ„å»ºå— 1.2 ä¸»è¦ç‰¹ç‚¹ GPU ä¼˜åŒ–çš„ Transformer å®ç° å¤šç§å¹¶è¡Œç­–ç•¥ï¼ˆTPã€PPã€DPã€EPã€CPï¼‰ æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„ï¼ˆGPTã€LLaMAã€Mixtralã€Mambaã€DeepSeek-V3 ç­‰ï¼‰ FP8ã€FP16ã€BF16ã€FP4 æ··åˆç²¾åº¦è®­ç»ƒ åˆ†å¸ƒå¼ä¼˜åŒ–å™¨å’Œæ£€æŸ¥ç‚¹ MoEï¼ˆMixture of Expertsï¼‰æ”¯æŒï¼ŒåŒ…æ‹¬ Shared Experts MLAï¼ˆMulti-Latent Attentionï¼‰é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ åŠ¨æ€æ¨ç†å¼•æ“ï¼ˆDynamic Inference Engineï¼‰ å®¹é”™è®­ç»ƒï¼ˆNVRx é›†æˆï¼‰ HyperCommGrid Nç»´é€šä¿¡ç½‘æ ¼ç®¡ç† æ¨ç†å¼•æ“å’Œæ¨¡å‹å¯¼å‡ºï¼ˆTensorRT-LLMï¼‰ 1.3 ç”Ÿæ€ç³»ç»Ÿ12345678910111213141516171819202122232425262728293031graph TB subgraph &quot;Dependencies ä¾èµ–åº“&quot; TE[Transformer Engine&lt;br/&gt;FP8ä¼˜åŒ–å†…æ ¸] Energon[Megatron Energon&lt;br/&gt;å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨] NVRx[NVRx&lt;br/&gt;å®¹é”™è®­ç»ƒ] end subgraph &quot;Core æ ¸å¿ƒåº“&quot; MCore[Megatron Core&lt;br/&gt;æ ¸å¿ƒæ„å»ºå—] end subgraph &quot;Applications åº”ç”¨å±‚&quot; MLM[Megatron-LM&lt;br/&gt;å‚è€ƒå®ç°] Bridge[Megatron Bridge&lt;br/&gt;HFäº’æ“ä½œ] NeMo[NeMo Framework&lt;br/&gt;ä¼ä¸šæ¡†æ¶] NeMoRL[NeMo RL&lt;br/&gt;RLHFè®­ç»ƒ] ModelOpt[TensorRT ModelOpt&lt;br/&gt;æ¨¡å‹ä¼˜åŒ–] end TE --&gt; MCore Energon --&gt; MCore NVRx --&gt; MCore MCore --&gt; MLM MCore --&gt; Bridge MCore --&gt; NeMo MCore --&gt; NeMoRL MCore --&gt; ModelOpt style MCore fill:#4CAF50 style MLM fill:#2196F3 2. æ•´ä½“æ¶æ„2.1 é¡¹ç›®ç»“æ„12345678910111213141516171819202122232425262728293031323334353637383940414243Megatron-LM/â”œâ”€â”€ megatron/ # æ ¸å¿ƒä»£ç ç›®å½•â”‚ â”œâ”€â”€ core/ # Megatron Core ç”Ÿäº§åº“â”‚ â”‚ â”œâ”€â”€ models/ # æ¨¡å‹å®ç°â”‚ â”‚ â”‚ â”œâ”€â”€ gpt/ # GPT æ¨¡å‹â”‚ â”‚ â”‚ â”œâ”€â”€ bert/ # BERT æ¨¡å‹â”‚ â”‚ â”‚ â”œâ”€â”€ T5/ # T5 æ¨¡å‹â”‚ â”‚ â”‚ â”œâ”€â”€ mamba/ # Mambaï¼ˆSSMï¼‰æ¨¡å‹â”‚ â”‚ â”‚ â”œâ”€â”€ multimodal/ # å¤šæ¨¡æ€æ¨¡å‹â”‚ â”‚ â”‚ â”œâ”€â”€ retro/ # RETRO æ¨¡å‹â”‚ â”‚ â”‚ â””â”€â”€ vision/ # è§†è§‰æ¨¡å‹â”‚ â”‚ â”œâ”€â”€ transformer/ # Transformer æ„å»ºå—â”‚ â”‚ â”‚ â”œâ”€â”€ transformer_layer.py # Transformer å±‚â”‚ â”‚ â”‚ â”œâ”€â”€ transformer_block.py # Transformer å—â”‚ â”‚ â”‚ â”œâ”€â”€ transformer_config.py # é…ç½®ç±»â”‚ â”‚ â”‚ â”œâ”€â”€ attention.py # æ³¨æ„åŠ›æœºåˆ¶â”‚ â”‚ â”‚ â””â”€â”€ mlp.py # MLP å±‚â”‚ â”‚ â”œâ”€â”€ tensor_parallel/ # å¼ é‡å¹¶è¡Œâ”‚ â”‚ â”œâ”€â”€ pipeline_parallel/ # æµæ°´çº¿å¹¶è¡Œâ”‚ â”‚ â”œâ”€â”€ distributed/ # åˆ†å¸ƒå¼è®­ç»ƒï¼ˆFSDPã€DDPï¼‰â”‚ â”‚ â”œâ”€â”€ optimizer/ # ä¼˜åŒ–å™¨â”‚ â”‚ â”œâ”€â”€ datasets/ # æ•°æ®é›†åŠ è½½å™¨â”‚ â”‚ â”œâ”€â”€ inference/ # æ¨ç†å¼•æ“â”‚ â”‚ â”œâ”€â”€ export/ # æ¨¡å‹å¯¼å‡ºâ”‚ â”‚ â”œâ”€â”€ quantization/ # é‡åŒ–â”‚ â”‚ â”œâ”€â”€ fusions/ # èåˆå†…æ ¸â”‚ â”‚ â””â”€â”€ dist_checkpointing/ # åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹â”‚ â”œâ”€â”€ training/ # è®­ç»ƒå¾ªç¯å’Œå·¥å…·â”‚ â”œâ”€â”€ legacy/ # é—ç•™ç»„ä»¶â”‚ â””â”€â”€ post_training/ # åè®­ç»ƒï¼ˆRLHF ç­‰ï¼‰â”œâ”€â”€ examples/ # ç¤ºä¾‹è„šæœ¬â”‚ â”œâ”€â”€ gpt3/ # GPT-3 ç¤ºä¾‹â”‚ â”œâ”€â”€ llama/ # LLaMA ç¤ºä¾‹â”‚ â”œâ”€â”€ mixtral/ # Mixtral ç¤ºä¾‹â”‚ â”œâ”€â”€ multimodal/ # å¤šæ¨¡æ€ç¤ºä¾‹â”‚ â””â”€â”€ post_training/ # åè®­ç»ƒç¤ºä¾‹â”œâ”€â”€ tools/ # å·¥å…·è„šæœ¬â”‚ â”œâ”€â”€ preprocess_data.py # æ•°æ®é¢„å¤„ç†â”‚ â”œâ”€â”€ checkpoint/ # æ£€æŸ¥ç‚¹è½¬æ¢å·¥å…·â”‚ â””â”€â”€ run_text_generation_server.py # æ¨ç†æœåŠ¡å™¨â””â”€â”€ tests/ # æµ‹è¯•å¥—ä»¶ â”œâ”€â”€ unit_tests/ # å•å…ƒæµ‹è¯• â””â”€â”€ functional_tests/ # åŠŸèƒ½æµ‹è¯• 2.2 ç³»ç»Ÿæ¶æ„å›¾123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115graph TB subgraph &quot;User Interface ç”¨æˆ·ç•Œé¢&quot; PretrainScripts[é¢„è®­ç»ƒè„šæœ¬&lt;br/&gt;pretrain_gpt.py&lt;br/&gt;pretrain_llama.py] Examples[ç¤ºä¾‹ä»£ç &lt;br/&gt;examples/] Tools[å·¥å…·é›†&lt;br/&gt;tools/] end subgraph &quot;Training Layer è®­ç»ƒå±‚&quot; TrainingLoop[è®­ç»ƒå¾ªç¯&lt;br/&gt;training/training.py] DataLoader[æ•°æ®åŠ è½½&lt;br/&gt;datasets/] Checkpoint[æ£€æŸ¥ç‚¹ç®¡ç†&lt;br/&gt;checkpointing.py] end subgraph &quot;Megatron Core æ ¸å¿ƒå±‚&quot; subgraph &quot;Models æ¨¡å‹å±‚&quot; GPT[GPTModel] BERT[BERTModel] T5[T5Model] Multimodal[MultimodalModel] end subgraph &quot;Transformer Components ç»„ä»¶å±‚&quot; TransformerBlock[TransformerBlock] TransformerLayer[TransformerLayer] Attention[Attention] MLP[MLP/MoE] Embedding[Embedding] end subgraph &quot;Parallelism å¹¶è¡Œå±‚&quot; TensorParallel[Tensor Parallel&lt;br/&gt;å¼ é‡å¹¶è¡Œ] PipelineParallel[Pipeline Parallel&lt;br/&gt;æµæ°´çº¿å¹¶è¡Œ] DataParallel[Data Parallel&lt;br/&gt;æ•°æ®å¹¶è¡Œ] ExpertParallel[Expert Parallel&lt;br/&gt;ä¸“å®¶å¹¶è¡Œ] ContextParallel[Context Parallel&lt;br/&gt;ä¸Šä¸‹æ–‡å¹¶è¡Œ] end subgraph &quot;Optimization ä¼˜åŒ–å±‚&quot; Optimizer[åˆ†å¸ƒå¼ä¼˜åŒ–å™¨&lt;br/&gt;DistributedOptimizer] MixedPrecision[æ··åˆç²¾åº¦&lt;br/&gt;FP8/FP16/BF16] GradAccum[æ¢¯åº¦ç´¯ç§¯] ParamScheduler[å‚æ•°è°ƒåº¦å™¨] end subgraph &quot;Inference &amp; Export æ¨ç†å¯¼å‡ºå±‚&quot; InferenceEngine[æ¨ç†å¼•æ“&lt;br/&gt;Dynamic Inference] ExportModule[æ¨¡å‹å¯¼å‡º&lt;br/&gt;TensorRT-LLM/ONNX] end end subgraph &quot;Infrastructure åŸºç¡€è®¾æ–½å±‚&quot; ParallelState[å¹¶è¡ŒçŠ¶æ€ç®¡ç†&lt;br/&gt;parallel_state.py] ProcessGroups[è¿›ç¨‹ç»„&lt;br/&gt;process_groups_config.py] Memory[å†…å­˜ç®¡ç†&lt;br/&gt;GlobalMemoryBuffer] Timers[æ€§èƒ½è®¡æ—¶å™¨&lt;br/&gt;timers.py] end subgraph &quot;External Dependencies å¤–éƒ¨ä¾èµ–&quot; PyTorch[PyTorch] TE[Transformer Engine] NCCL[NCCL] Apex[Apex&lt;br/&gt;å¯é€‰] end PretrainScripts --&gt; TrainingLoop Examples --&gt; TrainingLoop Tools --&gt; Checkpoint TrainingLoop --&gt; GPT TrainingLoop --&gt; BERT TrainingLoop --&gt; T5 TrainingLoop --&gt; Multimodal TrainingLoop --&gt; DataLoader TrainingLoop --&gt; Checkpoint GPT --&gt; TransformerBlock BERT --&gt; TransformerBlock T5 --&gt; TransformerBlock Multimodal --&gt; TransformerBlock TransformerBlock --&gt; TransformerLayer TransformerLayer --&gt; Attention TransformerLayer --&gt; MLP TransformerBlock --&gt; Embedding TransformerLayer --&gt; TensorParallel TransformerBlock --&gt; PipelineParallel TrainingLoop --&gt; DataParallel MLP --&gt; ExpertParallel Attention --&gt; ContextParallel TrainingLoop --&gt; Optimizer Optimizer --&gt; MixedPrecision Optimizer --&gt; GradAccum TrainingLoop --&gt; ParamScheduler TensorParallel --&gt; ParallelState PipelineParallel --&gt; ParallelState DataParallel --&gt; ParallelState ExpertParallel --&gt; ParallelState ParallelState --&gt; ProcessGroups ParallelState --&gt; Memory TrainingLoop --&gt; Timers TransformerLayer --&gt; TE ParallelState --&gt; NCCL Optimizer --&gt; PyTorch MixedPrecision --&gt; Apex style GPT fill:#FF6B6B style TransformerBlock fill:#4ECDC4 style TensorParallel fill:#95E1D3 style PipelineParallel fill:#95E1D3 style Optimizer fill:#FFD93D 3. æ ¸å¿ƒæ¨¡å—åˆ†æ3.1 Transformer ç»„ä»¶3.1.1 TransformerConfigé…ç½®ç±»ï¼Œç®¡ç†æ‰€æœ‰ Transformer ç›¸å…³çš„é…ç½®å‚æ•°ï¼š 12345678910111213141516171819202122@dataclassclass TransformerConfig(ModelParallelConfig): # æ¨¡å‹æ¶æ„ num_layers: int # Transformer å±‚æ•° hidden_size: int # éšè—å±‚å¤§å° num_attention_heads: int # æ³¨æ„åŠ›å¤´æ•° ffn_hidden_size: Optional[int] # FFN éšè—å±‚å¤§å° # å¹¶è¡Œé…ç½® tensor_model_parallel_size: int = 1 # å¼ é‡å¹¶è¡Œå¤§å° pipeline_model_parallel_size: int = 1 # æµæ°´çº¿å¹¶è¡Œå¤§å° expert_model_parallel_size: int = 1 # ä¸“å®¶å¹¶è¡Œå¤§å° context_parallel_size: int = 1 # ä¸Šä¸‹æ–‡å¹¶è¡Œå¤§å° # ç²¾åº¦é…ç½® fp16: bool = False # FP16 è®­ç»ƒ bf16: bool = False # BF16 è®­ç»ƒ fp8: Optional[str] = None # FP8 è®­ç»ƒ # MoE é…ç½® num_moe_experts: Optional[int] = None # MoE ä¸“å®¶æ•° moe_router_topk: int = 2 # TopK è·¯ç”± 3.1.2 TransformerLayeråŸºç¡€ Transformer å±‚å®ç°ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041graph TB Input[è¾“å…¥ Hidden States] subgraph &quot;Self-Attention Block&quot; SelfAttn[Self-Attention] Dropout1[Dropout] Residual1[Residual Connection] PostAttnLN[Post-Attention LayerNorm] end subgraph &quot;MLP/MoE Block&quot; MLPorMoE&#123;MLP or MoE?&#125; MLP[MLP] MoE[MoE Router + Experts] Dropout2[Dropout] Residual2[Residual Connection] PostMLPLN[Post-MLP LayerNorm] end Output[è¾“å‡º Hidden States] Input --&gt; SelfAttn SelfAttn --&gt; Dropout1 Dropout1 --&gt; Residual1 Input --&gt; Residual1 Residual1 --&gt; PostAttnLN PostAttnLN --&gt; MLPorMoE MLPorMoE --&gt;|æ ‡å‡†| MLP MLPorMoE --&gt;|MoE| MoE MLP --&gt; Dropout2 MoE --&gt; Dropout2 Dropout2 --&gt; Residual2 PostAttnLN --&gt; Residual2 Residual2 --&gt; PostMLPLN PostMLPLN --&gt; Output style SelfAttn fill:#FFB6C1 style MLP fill:#98D8C8 style MoE fill:#F7DC6F 3.1.3 Attention æœºåˆ¶æ”¯æŒå¤šç§æ³¨æ„åŠ›å®ç°ï¼š æ ‡å‡† Multi-Head Attention (MHA) Grouped Query Attention (GQA) Multi-Query Attention (MQA) Flash Attentionï¼ˆé€šè¿‡ Transformer Engineï¼‰ Context Parallel Attention 12345678# å…³é”®å‚æ•°class Attention: num_attention_heads: int # æ³¨æ„åŠ›å¤´æ•° num_query_groups: int # æŸ¥è¯¢ç»„æ•°ï¼ˆGQAï¼‰ kv_channels: int # K/V é€šé“æ•° attention_dropout: float # Dropout æ¦‚ç‡ attn_mask_type: AttnMaskType # æ©ç ç±»å‹ qkv_format: str # QKV æ ¼å¼ï¼ˆsbhd/bshdç­‰ï¼‰ 3.2 æ¨¡å‹å®ç°3.2.1 GPTModel æ¶æ„123456789101112131415161718192021222324252627282930313233343536graph TB subgraph &quot;GPTModel&quot; subgraph &quot;Pre-Process é¢„å¤„ç†é˜¶æ®µ&quot; TokenEmbed[Token Embedding&lt;br/&gt;è¯åµŒå…¥å±‚] PosEmbed[Position Embedding&lt;br/&gt;ä½ç½®ç¼–ç ] EmbedDropout[Embedding Dropout] end subgraph &quot;Encoder ç¼–ç å™¨&quot; TransBlock[TransformerBlock&lt;br/&gt;Nå±‚Transformer] end subgraph &quot;Post-Process åå¤„ç†é˜¶æ®µ&quot; FinalLN[Final LayerNorm] OutputLayer[Output Layer&lt;br/&gt;è¾“å‡ºæŠ•å½±å±‚] end subgraph &quot;Optional å¯é€‰ç»„ä»¶&quot; MTP[Multi-Token Prediction&lt;br/&gt;å¤šæ ‡è®°é¢„æµ‹] end end Input[Input Token IDs] --&gt; TokenEmbed Input --&gt; PosEmbed TokenEmbed --&gt; EmbedDropout PosEmbed --&gt; EmbedDropout EmbedDropout --&gt; TransBlock TransBlock --&gt; FinalLN FinalLN --&gt; OutputLayer OutputLayer --&gt; Logits[Logits] FinalLN -.å¯é€‰.-&gt; MTP MTP -.-&gt; MTPLogits[MTP Logits] style TransBlock fill:#4CAF50 style MTP fill:#FF9800 3.2.2 æ”¯æŒçš„æ¨¡å‹ç±»å‹ æ¨¡å‹ç±»å‹ å®ç°ä½ç½® ç‰¹æ€§ GPT megatron/core/models/gpt/ è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œå› æœæ³¨æ„åŠ› BERT megatron/core/models/bert/ åŒå‘ç¼–ç å™¨ï¼ŒMLMè®­ç»ƒ T5 megatron/core/models/T5/ ç¼–ç å™¨-è§£ç å™¨æ¶æ„ Mamba megatron/core/models/mamba/ çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ Multimodal megatron/core/models/multimodal/ å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLLaVAã€MiMoã€NVLMï¼‰ RETRO megatron/core/models/retro/ æ£€ç´¢å¢å¼ºæ¨¡å‹ Vision megatron/core/models/vision/ è§†è§‰æ¨¡å‹ï¼ˆCLIPã€RADIOã€ViTï¼‰ MiMo megatron/core/models/mimo/ å¤šå›¾åƒå¤šè¾“å‡ºè§†é¢‘VLM 4. å¹¶è¡Œç­–ç•¥è¯¦è§£4.1 å¹¶è¡Œç­–ç•¥æ¦‚è§ˆ1234567891011121314151617181920212223242526graph LR subgraph &quot;å¹¶è¡Œç»´åº¦&quot; DP[Data Parallel&lt;br/&gt;æ•°æ®å¹¶è¡Œ&lt;br/&gt;å¤åˆ¶æ¨¡å‹] TP[Tensor Parallel&lt;br/&gt;å¼ é‡å¹¶è¡Œ&lt;br/&gt;åˆ‡åˆ†å±‚å†…å¼ é‡] PP[Pipeline Parallel&lt;br/&gt;æµæ°´çº¿å¹¶è¡Œ&lt;br/&gt;åˆ‡åˆ†å±‚é—´] EP[Expert Parallel&lt;br/&gt;ä¸“å®¶å¹¶è¡Œ&lt;br/&gt;åˆ‡åˆ†MoEä¸“å®¶] CP[Context Parallel&lt;br/&gt;ä¸Šä¸‹æ–‡å¹¶è¡Œ&lt;br/&gt;åˆ‡åˆ†åºåˆ—] end Model[å®Œæ•´æ¨¡å‹] --&gt; DP Model --&gt; TP Model --&gt; PP Model --&gt; EP Model --&gt; CP DP --&gt;|ç»„åˆ| Hybrid[æ··åˆå¹¶è¡Œç­–ç•¥] TP --&gt;|ç»„åˆ| Hybrid PP --&gt;|ç»„åˆ| Hybrid EP --&gt;|ç»„åˆ| Hybrid CP --&gt;|ç»„åˆ| Hybrid style DP fill:#FFE66D style TP fill:#4ECDC4 style PP fill:#FF6B6B style EP fill:#95E1D3 style CP fill:#C7CEEA 4.2 Tensor Parallelï¼ˆå¼ é‡å¹¶è¡Œï¼‰åŸç†: åœ¨å±‚å†…åˆ‡åˆ†å¼ é‡ï¼ˆæƒé‡çŸ©é˜µå’Œæ¿€æ´»ï¼‰ï¼Œä¸åŒ GPU è®¡ç®—ä¸åŒéƒ¨åˆ† 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061graph TB subgraph &quot;Column Parallel åˆ—å¹¶è¡Œ&quot; Input1[Input&lt;br/&gt;Shape: [s, b, h]] subgraph &quot;GPU 0&quot; Weight1_0[W1[:, 0:h/2]] Output1_0[Output[:, :, 0:h/2]] end subgraph &quot;GPU 1&quot; Weight1_1[W1[:, h/2:h]] Output1_1[Output[:, :, h/2:h]] end AllGather1[All-Gather&lt;br/&gt;åˆå¹¶è¾“å‡º] ConcatOutput1[Concatenated Output] Input1 --&gt; Weight1_0 Input1 --&gt; Weight1_1 Weight1_0 --&gt; Output1_0 Weight1_1 --&gt; Output1_1 Output1_0 --&gt; AllGather1 Output1_1 --&gt; AllGather1 AllGather1 --&gt; ConcatOutput1 end subgraph &quot;Row Parallel è¡Œå¹¶è¡Œ&quot; Input2[Input&lt;br/&gt;Shape: [s, b, h]] Split[Split è¾“å…¥] subgraph &quot;GPU 0 &quot; Input2_0[Input[:, :, 0:h/2]] Weight2_0[W2[0:h/2, :]] Output2_0[Partial Output] end subgraph &quot;GPU 1 &quot; Input2_1[Input[:, :, h/2:h]] Weight2_1[W2[h/2:h, :]] Output2_1[Partial Output] end AllReduce2[All-Reduce&lt;br/&gt;æ±‚å’Œ] FinalOutput2[Final Output] Input2 --&gt; Split Split --&gt; Input2_0 Split --&gt; Input2_1 Input2_0 --&gt; Weight2_0 Input2_1 --&gt; Weight2_1 Weight2_0 --&gt; Output2_0 Weight2_1 --&gt; Output2_1 Output2_0 --&gt; AllReduce2 Output2_1 --&gt; AllReduce2 AllReduce2 --&gt; FinalOutput2 end style Weight1_0 fill:#FFB6C1 style Weight1_1 fill:#FFB6C1 style Weight2_0 fill:#98D8C8 style Weight2_1 fill:#98D8C8 æ ¸å¿ƒç»„ä»¶: ColumnParallelLinear: åˆ—å¹¶è¡Œçº¿æ€§å±‚ RowParallelLinear: è¡Œå¹¶è¡Œçº¿æ€§å±‚ VocabParallelEmbedding: è¯è¡¨å¹¶è¡ŒåµŒå…¥å±‚ é€šä¿¡æ¨¡å¼: All-Gather: æ”¶é›†æ‰€æœ‰ GPU çš„è¾“å‡º All-Reduce: å¯¹æ‰€æœ‰ GPU çš„è¾“å‡ºæ±‚å’Œ 4.3 Pipeline Parallelï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰åŸç†: å°†æ¨¡å‹æŒ‰å±‚åˆ‡åˆ†åˆ°ä¸åŒ GPUï¼Œé‡‡ç”¨æµæ°´çº¿è°ƒåº¦ç­–ç•¥ 1234567891011121314151617181920212223242526272829303132333435363738394041gantt title 1F1B Pipeline Schedule dateFormat X axisFormat %s section GPU-0 F1 : 0, 1 F2 : 1, 2 F3 : 2, 3 F4 : 3, 4 B1 : 4, 5 B2 : 5, 6 B3 : 6, 7 B4 : 7, 8 section GPU-1 Idle : 0, 1 F1 : 1, 2 F2 : 2, 3 F3 : 3, 4 B1 : 4, 5 B2 : 5, 6 B3 : 6, 7 B4 : 7, 8 section GPU-2 Idle : 0, 2 F1 : 2, 3 F2 : 3, 4 F3 : 4, 5 B1 : 5, 6 B2 : 6, 7 B3 : 7, 8 section GPU-3 Idle : 0, 3 F1 : 3, 4 F2 : 4, 5 F3 : 5, 6 F4 : 6, 7 B1 : 7, 8 è¯´æ˜: F1-F4: å‰å‘ä¼ æ’­ï¼ˆForward passï¼‰æ‰¹æ¬¡1-4 B1-B4: åå‘ä¼ æ’­ï¼ˆBackward passï¼‰æ‰¹æ¬¡1-4 Idle: ç©ºé—²ç­‰å¾… GPU-0: å¤„ç† Layer 0-7 GPU-1: å¤„ç† Layer 8-15 GPU-2: å¤„ç† Layer 16-23 GPU-3: å¤„ç† Layer 24-31 1234567891011121314151617181920212223242526**è°ƒåº¦ç­–ç•¥**:- **1F1B (1-Forward-1-Backward)**: äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­- **Interleaved 1F1B**: è™šæ‹Ÿæµæ°´çº¿å¹¶è¡Œï¼Œå‡å°‘æ°”æ³¡- **Combined 1F1B**: ç»“åˆæ•°æ®å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œ**æ ¸å¿ƒæ–‡ä»¶**:- `pipeline_parallel/schedules.py`: è°ƒåº¦ç®—æ³•å®ç°- `pipeline_parallel/p2p_communication.py`: ç‚¹å¯¹ç‚¹é€šä¿¡### 4.4 Data Parallelï¼ˆæ•°æ®å¹¶è¡Œï¼‰**åŸç†**: å¤åˆ¶æ¨¡å‹åˆ°å¤šä¸ª GPUï¼Œæ¯ä¸ª GPU å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡**å®ç°æ–¹å¼**:1. **DDP (DistributedDataParallel)**: PyTorch åŸç”Ÿ DDP2. **FSDP (Fully Sharded Data Parallel)**: å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ3. **ZeRO**: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€```python# DDP é…ç½®ddp_config = DistributedDataParallelConfig( grad_reduce_in_fp32=True, # FP32 æ¢¯åº¦è§„çº¦ overlap_grad_reduce=True, # é‡å æ¢¯åº¦é€šä¿¡ use_distributed_optimizer=True, # åˆ†å¸ƒå¼ä¼˜åŒ–å™¨) 4.5 Expert Parallelï¼ˆä¸“å®¶å¹¶è¡Œï¼‰åŸç†: åœ¨ MoE æ¨¡å‹ä¸­ï¼Œå°†ä¸“å®¶åˆ†å¸ƒåˆ°ä¸åŒ GPU 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748graph TB Input[è¾“å…¥ Tokens] Router[Router è·¯ç”±å™¨] subgraph &quot;GPU 0&quot; Expert0[Expert 0] Expert1[Expert 1] end subgraph &quot;GPU 1&quot; Expert2[Expert 2] Expert3[Expert 3] end subgraph &quot;GPU 2&quot; Expert4[Expert 4] Expert5[Expert 5] end AllToAll1[All-to-All&lt;br/&gt;Token åˆ†å‘] AllToAll2[All-to-All&lt;br/&gt;ç»“æœæ”¶é›†] Output[è¾“å‡º] Input --&gt; Router Router --&gt; AllToAll1 AllToAll1 --&gt; Expert0 AllToAll1 --&gt; Expert1 AllToAll1 --&gt; Expert2 AllToAll1 --&gt; Expert3 AllToAll1 --&gt; Expert4 AllToAll1 --&gt; Expert5 Expert0 --&gt; AllToAll2 Expert1 --&gt; AllToAll2 Expert2 --&gt; AllToAll2 Expert3 --&gt; AllToAll2 Expert4 --&gt; AllToAll2 Expert5 --&gt; AllToAll2 AllToAll2 --&gt; Output style Router fill:#F39C12 style Expert0 fill:#3498DB style Expert1 fill:#3498DB style Expert2 fill:#E74C3C style Expert3 fill:#E74C3C style Expert4 fill:#2ECC71 style Expert5 fill:#2ECC71 å…³é”®ç‰¹æ€§: TopK è·¯ç”±é€‰æ‹© è´Ÿè½½å‡è¡¡æŸå¤± Token Dropping ä¸“å®¶å®¹é‡å› å­ 4.6 Context Parallelï¼ˆä¸Šä¸‹æ–‡å¹¶è¡Œï¼‰åŸç†: åœ¨åºåˆ—ç»´åº¦ä¸Šåˆ‡åˆ†é•¿åºåˆ—ï¼Œé€‚ç”¨äºè¶…é•¿ä¸Šä¸‹æ–‡ 123456789# åºåˆ—åˆ‡åˆ†ç¤ºä¾‹# åŸå§‹åºåˆ—é•¿åº¦: 128K tokens# Context Parallel Size: 4# æ¯ä¸ª GPU å¤„ç†: 32K tokens# GPU 0: tokens[0:32K]# GPU 1: tokens[32K:64K]# GPU 2: tokens[64K:96K]# GPU 3: tokens[96K:128K] é€šä¿¡éœ€æ±‚: All-Gather: åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶æ”¶é›† K&#x2F;V Ring Attention: ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶ 4.7 å¹¶è¡Œç­–ç•¥é€‰æ‹©æŒ‡å—1234567891011121314151617181920212223242526272829303132333435363738394041graph TD Start[å¼€å§‹é€‰æ‹©å¹¶è¡Œç­–ç•¥] Q1&#123;æ¨¡å‹èƒ½å¦æ”¾å…¥&lt;br/&gt;å•ä¸ªGPU?&#125; Q2&#123;ä¸»è¦ç“¶é¢ˆæ˜¯?&#125; Q3&#123;æ˜¯å¦ä½¿ç”¨MoE?&#125; Q4&#123;åºåˆ—é•¿åº¦æ˜¯å¦&lt;br/&gt;è¶…è¿‡32K?&#125; Q5&#123;GPUæ•°é‡?&#125; DP[æ•°æ®å¹¶è¡Œ&lt;br/&gt;Data Parallel] TP_DP[å¼ é‡å¹¶è¡Œ + æ•°æ®å¹¶è¡Œ&lt;br/&gt;TP + DP] PP_TP_DP[æµæ°´çº¿ + å¼ é‡ + æ•°æ®&lt;br/&gt;PP + TP + DP] EP_ADDED[æ·»åŠ ä¸“å®¶å¹¶è¡Œ&lt;br/&gt;+ EP] CP_ADDED[æ·»åŠ ä¸Šä¸‹æ–‡å¹¶è¡Œ&lt;br/&gt;+ CP] Start --&gt; Q1 Q1 --&gt;|æ˜¯| DP Q1 --&gt;|å¦| Q2 Q2 --&gt;|å±‚å†…å‚æ•°é‡| Q5 Q2 --&gt;|å±‚æ•°è¿‡å¤š| PP_TP_DP Q5 --&gt;|2-8| TP_DP Q5 --&gt;|&gt;8| PP_TP_DP TP_DP --&gt; Q3 PP_TP_DP --&gt; Q3 Q3 --&gt;|æ˜¯| EP_ADDED Q3 --&gt;|å¦| Q4 EP_ADDED --&gt; Q4 Q4 --&gt;|æ˜¯| CP_ADDED Q4 --&gt;|å¦| End[å®Œæˆé…ç½®] CP_ADDED --&gt; End style DP fill:#90EE90 style TP_DP fill:#87CEEB style PP_TP_DP fill:#FFB6C1 style EP_ADDED fill:#F0E68C style CP_ADDED fill:#DDA0DD ç»éªŒæ³•åˆ™: æ¨¡å‹å¤§å° GPU æ•°é‡ æ¨èç­–ç•¥ &lt; 1B 1-8 DP only 1B - 13B 8-64 TP&#x3D;2-4, DP&#x3D;rest 13B - 70B 64-256 TP&#x3D;4-8, PP&#x3D;2-4, DP&#x3D;rest 70B - 175B 256-1024 TP&#x3D;8, PP&#x3D;4-8, DP&#x3D;rest &gt; 175B &gt; 1024 TP&#x3D;8, PP&#x3D;16+, DP&#x3D;rest 5. æ¨¡å‹å®ç°5.1 æ¨¡å‹å±‚æ¬¡ç»“æ„1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859classDiagram class MegatronModule &#123; +config: TransformerConfig +shared_embedding_or_output_weight() +initialize_embedding_or_output_weight() &#125; class LanguageModule &#123; +state_dict_for_save_checkpoint() +load_state_dict() &#125; class GPTModel &#123; +embedding: LanguageModelEmbedding +decoder: TransformerBlock +output_layer: Linear +forward() &#125; class TransformerBlock &#123; +num_layers: int +layers: ModuleList +final_layernorm: LayerNorm +forward() &#125; class TransformerLayer &#123; +self_attention: Attention +mlp: MLP +input_layernorm: LayerNorm +pre_mlp_layernorm: LayerNorm +forward() &#125; class Attention &#123; +linear_qkv: ColumnParallelLinear +core_attention: DotProductAttention +linear_proj: RowParallelLinear +forward() &#125; class MLP &#123; +linear_fc1: ColumnParallelLinear +activation_func: Activation +linear_fc2: RowParallelLinear +forward() &#125; MegatronModule &lt;|-- LanguageModule LanguageModule &lt;|-- GPTModel MegatronModule &lt;|-- TransformerBlock MegatronModule &lt;|-- TransformerLayer MegatronModule &lt;|-- Attention MegatronModule &lt;|-- MLP GPTModel *-- TransformerBlock TransformerBlock *-- TransformerLayer TransformerLayer *-- Attention TransformerLayer *-- MLP 5.2 MoEï¼ˆMixture of Expertsï¼‰å®ç°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566graph TB Input[è¾“å…¥ Hidden States] subgraph &quot;MoE Layer&quot; SharedCheck&#123;æ˜¯å¦æœ‰ Shared Experts?&#125; SharedExperts[Shared Experts å…±äº«ä¸“å®¶&lt;br/&gt;æ‰€æœ‰tokenéƒ½è®¡ç®—] Router[TopK Router è·¯ç”±å™¨&lt;br/&gt;è®¡ç®—ä¸“å®¶åˆ†æ•° + Aux Loss] subgraph &quot;Token Dispatching&quot; Dispatcher&#123;Token Dispatcher&lt;br/&gt;åˆ†å‘ç­–ç•¥&#125; AllGather[AllGather&lt;br/&gt;æ”¶é›†æ‰€æœ‰token] AllToAll[AllToAll&lt;br/&gt;tokené‡æ’åˆ—] Flex[Flex Dispatcher&lt;br/&gt;ç»Ÿä¸€TP+EPé€šä¿¡] end subgraph &quot;Routed Experts è·¯ç”±ä¸“å®¶&quot; E1[Expert 1] E2[Expert 2] EN[Expert N] end Combine[Token Combine åˆå¹¶&lt;br/&gt;åŠ æƒæ±‚å’Œä¸“å®¶è¾“å‡º] MixOutput[æ··åˆè¾“å‡º&lt;br/&gt;Shared + Routed] end Output[è¾“å‡º Hidden States] Input --&gt; SharedCheck SharedCheck --&gt;|æœ‰| SharedExperts SharedCheck --&gt;|æ— | Router SharedExperts --&gt; Router Router --&gt;|routing_map + probs| Dispatcher Dispatcher --&gt;|All-Gather| AllGather Dispatcher --&gt;|All-to-All| AllToAll Dispatcher --&gt;|ç»Ÿä¸€é€šä¿¡| Flex AllGather --&gt; E1 AllGather --&gt; E2 AllGather --&gt; EN AllToAll --&gt; E1 AllToAll --&gt; E2 AllToAll --&gt; EN Flex --&gt; E1 Flex --&gt; E2 Flex --&gt; EN E1 --&gt; Combine E2 --&gt; Combine EN --&gt; Combine Combine --&gt; MixOutput SharedExperts -.-&gt; MixOutput MixOutput --&gt; Output style Router fill:#FFD93D style SharedExperts fill:#FF6B6B style E1 fill:#98D8C8 style E2 fill:#98D8C8 style EN fill:#98D8C8 style Combine fill:#F7DC6F MoE å…³é”®å‚æ•°: 123456789101112131415161718# MoE é…ç½®moe_config = TransformerConfig( num_moe_experts=64, # ä¸“å®¶æ€»æ•° moe_router_topk=2, # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•° moe_aux_loss_coeff=0.01, # è¾…åŠ©æŸå¤±ç³»æ•° moe_token_dispatcher_type=&#x27;alltoall&#x27;, # Tokenåˆ†å‘ç±»å‹ expert_model_parallel_size=8, # ä¸“å®¶å¹¶è¡Œåº¦ moe_router_load_balancing_type=&#x27;aux_loss&#x27;, # è´Ÿè½½å‡è¡¡ç±»å‹ moe_router_dtype=&#x27;fp32&#x27;, # è·¯ç”±å™¨ç²¾åº¦ï¼ˆæ¨èfp32ï¼‰ moe_grouped_gemm=True, # åˆ†ç»„GEMMä¼˜åŒ– # Shared Experts é…ç½®ï¼ˆå¦‚ DeepSeek-V3ï¼‰ moe_shared_expert_intermediate_size=None, # å…±äº«ä¸“å®¶FFNå¤§å° moe_shared_expert_overlap=True, # å…±äº«ä¸“å®¶è®¡ç®—é‡å  # è´Ÿè½½å‡è¡¡ç­–ç•¥ moe_aux_loss_free=False, # æ— è¾…åŠ©æŸå¤±ç­–ç•¥ moe_expert_capacity_factor=1.0, # ä¸“å®¶å®¹é‡å› å­ moe_pad_expert_input_to_capacity=False, # å¡«å……åˆ°å®¹é‡) DeepSeek-V3 ç‰¹æ€§: Node-limited routing: èŠ‚ç‚¹é™åˆ¶è·¯ç”± Device-limited routing: è®¾å¤‡é™åˆ¶è·¯ç”± Aux-loss-free: æ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ Shared Experts: æ‰€æœ‰tokenéƒ½ç»è¿‡çš„å…±äº«ä¸“å®¶å±‚ Fine-grained parallelism: ç»†ç²’åº¦å¹¶è¡Œä¼˜åŒ– 5.3 Multi-Token Prediction (MTP)æ¦‚å¿µ: åœ¨è®­ç»ƒæ—¶åŒæ—¶é¢„æµ‹å¤šä¸ªæœªæ¥ tokenï¼Œæå‡è®­ç»ƒæ•ˆç‡ 123456789101112131415161718192021222324252627282930313233graph LR Input[è¾“å…¥åºåˆ—&lt;br/&gt;t1, t2, ..., tn] Encoder[ä¸»ç¼–ç å™¨&lt;br/&gt;TransformerBlock] subgraph &quot;MTP Block&quot; MTP_Layer1[MTP Layer 1] MTP_Layer2[MTP Layer 2] MTP_LayerK[MTP Layer K] end Pred_t1[é¢„æµ‹ t+1] Pred_t2[é¢„æµ‹ t+2] Pred_tk[é¢„æµ‹ t+k] Loss[ç»¼åˆæŸå¤±] Input --&gt; Encoder Encoder --&gt; MTP_Layer1 MTP_Layer1 --&gt; MTP_Layer2 MTP_Layer2 --&gt; MTP_LayerK Encoder --&gt; Pred_t1 MTP_Layer1 --&gt; Pred_t2 MTP_LayerK --&gt; Pred_tk Pred_t1 --&gt; Loss Pred_t2 --&gt; Loss Pred_tk --&gt; Loss style Encoder fill:#4CAF50 style MTP_Layer1 fill:#FF9800 style MTP_Layer2 fill:#FF9800 style MTP_LayerK fill:#FF9800 6. è®­ç»ƒæµç¨‹6.1 è®­ç»ƒä¸»å¾ªç¯123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778flowchart TD Start([å¼€å§‹è®­ç»ƒ]) Init[åˆå§‹åŒ–&lt;br/&gt;initialize_megatron] subgraph &quot;åˆå§‹åŒ–é˜¶æ®µ&quot; ParseArgs[è§£æå‘½ä»¤è¡Œå‚æ•°] InitDist[åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ&lt;br/&gt;parallel_state] BuildModel[æ„å»ºæ¨¡å‹&lt;br/&gt;model_provider] BuildOptim[æ„å»ºä¼˜åŒ–å™¨&lt;br/&gt;get_megatron_optimizer] BuildData[æ„å»ºæ•°æ®åŠ è½½å™¨&lt;br/&gt;build_train_valid_test_datasets] LoadCkpt[åŠ è½½æ£€æŸ¥ç‚¹&lt;br/&gt;load_checkpoint] end subgraph &quot;è®­ç»ƒå¾ªç¯&quot; EpochLoop&#123;éå† Epoch&#125; BatchLoop&#123;éå† Batch&#125; GetBatch[è·å–æ•°æ®æ‰¹æ¬¡] ForwardBackward[å‰å‘+åå‘ä¼ æ’­&lt;br/&gt;forward_backward_func] subgraph &quot;å‰å‘åå‘ä¼ æ’­&quot; Forward[å‰å‘ä¼ æ’­] ComputeLoss[è®¡ç®—æŸå¤±] Backward[åå‘ä¼ æ’­] end ReduceGrad[æ¢¯åº¦è§„çº¦&lt;br/&gt;All-Reduce] ClipGrad[æ¢¯åº¦è£å‰ª] UpdateParams[æ›´æ–°å‚æ•°&lt;br/&gt;optimizer.step] UpdateLR[æ›´æ–°å­¦ä¹ ç‡] LogMetrics[è®°å½•æŒ‡æ ‡] CheckSave&#123;æ˜¯å¦ä¿å­˜?&#125; SaveCkpt[ä¿å­˜æ£€æŸ¥ç‚¹] CheckEval&#123;æ˜¯å¦è¯„ä¼°?&#125; Evaluate[è¯„ä¼°æ¨¡å‹] end End([è®­ç»ƒç»“æŸ]) Start --&gt; Init Init --&gt; ParseArgs ParseArgs --&gt; InitDist InitDist --&gt; BuildModel BuildModel --&gt; BuildOptim BuildOptim --&gt; BuildData BuildData --&gt; LoadCkpt LoadCkpt --&gt; EpochLoop EpochLoop --&gt;|ç»§ç»­| BatchLoop EpochLoop --&gt;|å®Œæˆ| End BatchLoop --&gt;|ç»§ç»­| GetBatch BatchLoop --&gt;|å®Œæˆ| EpochLoop GetBatch --&gt; ForwardBackward ForwardBackward --&gt; Forward Forward --&gt; ComputeLoss ComputeLoss --&gt; Backward Backward --&gt; ReduceGrad ReduceGrad --&gt; ClipGrad ClipGrad --&gt; UpdateParams UpdateParams --&gt; UpdateLR UpdateLR --&gt; LogMetrics LogMetrics --&gt; CheckSave CheckSave --&gt;|æ˜¯| SaveCkpt CheckSave --&gt;|å¦| CheckEval SaveCkpt --&gt; CheckEval CheckEval --&gt;|æ˜¯| Evaluate CheckEval --&gt;|å¦| BatchLoop Evaluate --&gt; BatchLoop style Init fill:#90EE90 style ForwardBackward fill:#FFB6C1 style UpdateParams fill:#87CEEB 6.2 æ··åˆç²¾åº¦è®­ç»ƒæµç¨‹123456789101112131415161718192021222324252627282930313233343536373839404142434445graph TB Input[FP32/BF16 è¾“å…¥] subgraph &quot;å‰å‘ä¼ æ’­&quot; Cast1[è½¬æ¢ä¸º FP16/BF16/FP8] Compute1[æ¨¡å‹è®¡ç®—&lt;br/&gt;ä½ç²¾åº¦] Loss[è®¡ç®—æŸå¤±&lt;br/&gt;FP32] end subgraph &quot;åå‘ä¼ æ’­&quot; ScaleLoss[æŸå¤±ç¼©æ”¾&lt;br/&gt;Loss Scaling] Backward[åå‘ä¼ æ’­&lt;br/&gt;ä½ç²¾åº¦æ¢¯åº¦] Unscale[åç¼©æ”¾æ¢¯åº¦] CheckOverflow&#123;æ£€æŸ¥æº¢å‡º?&#125; Cast2[è½¬æ¢ä¸º FP32] end subgraph &quot;ä¼˜åŒ–å™¨&quot; ClipGrad[æ¢¯åº¦è£å‰ª&lt;br/&gt;FP32] Update[å‚æ•°æ›´æ–°&lt;br/&gt;FP32] UpdateScale[æ›´æ–°Loss Scale] end Output[æ›´æ–°åçš„å‚æ•°] Input --&gt; Cast1 Cast1 --&gt; Compute1 Compute1 --&gt; Loss Loss --&gt; ScaleLoss ScaleLoss --&gt; Backward Backward --&gt; Unscale Unscale --&gt; CheckOverflow CheckOverflow --&gt;|æº¢å‡º| UpdateScale CheckOverflow --&gt;|æ­£å¸¸| Cast2 UpdateScale --&gt; SkipStep[è·³è¿‡æ›´æ–°] Cast2 --&gt; ClipGrad ClipGrad --&gt; Update Update --&gt; Output style Compute1 fill:#FFE66D style Backward fill:#FF6B6B style Update fill:#4ECDC4 7. å…³é”®æŠ€æœ¯ç‰¹æ€§7.1 FP8 è®­ç»ƒTransformer Engine é›†æˆ: 12345678# FP8 é…ç½®config = TransformerConfig( fp8=&#x27;hybrid&#x27;, # FP8 æ¨¡å¼: hybrid/e4m3/e5m2 fp8_margin=0, # FP8 è¾¹ç•Œ fp8_interval=1, # ç¼©æ”¾å› å­æ›´æ–°é—´éš” fp8_amax_history_len=1024, # å†å²æœ€å¤§å€¼é•¿åº¦ fp8_amax_compute_algo=&#x27;max&#x27;, # è®¡ç®—ç®—æ³•) ç²¾åº¦å¯¹æ¯”: ç²¾åº¦ç±»å‹ æŒ‡æ•°ä½ å°¾æ•°ä½ åŠ¨æ€èŒƒå›´ é€‚ç”¨åœºæ™¯ FP32 8 23 10^38 åŸºå‡† FP16 5 10 10^4 é€šç”¨è®­ç»ƒ BF16 8 7 10^38 ç¨³å®šè®­ç»ƒ FP8 E4M3 4 3 10^2 å‰å‘ä¼ æ’­ FP8 E5M2 5 2 10^4 åå‘ä¼ æ’­ 7.2 åºåˆ—å¹¶è¡Œï¼ˆSequence Parallelï¼‰ä¸å¼ é‡å¹¶è¡Œç»“åˆ: 123456789101112131415161718192021graph LR subgraph &quot;æ ‡å‡† Tensor Parallel&quot; Input1[Input&lt;br/&gt;å¤åˆ¶åˆ°æ‰€æœ‰GPU] TP1[TP Computation] Output1[Output&lt;br/&gt;All-Reduce] end subgraph &quot;Sequence Parallel&quot; Input2[Input&lt;br/&gt;åˆ‡åˆ†åºåˆ—ç»´åº¦] TP2[TP Computation&lt;br/&gt;æ¯ä¸ªGPUå¤„ç†éƒ¨åˆ†åºåˆ—] Output2[Output&lt;br/&gt;All-Gather] end Input1 --&gt; TP1 TP1 --&gt; Output1 Input2 --&gt; TP2 TP2 --&gt; Output2 style Input2 fill:#90EE90 style TP2 fill:#FFB6C1 ä¼˜åŠ¿: å‡å°‘æ¿€æ´»å†…å­˜å ç”¨ é™ä½é€šä¿¡é‡ æ›´å¥½çš„æ‰©å±•æ€§ 7.3 é‡è®¡ç®—ï¼ˆActivation Recomputationï¼‰ç­–ç•¥: Full Recomputation: é‡è®¡ç®—æ‰€æœ‰æ¿€æ´» Selective Recomputation: ä»…é‡è®¡ç®—éƒ¨åˆ†å±‚ Partial Recomputation: é‡è®¡ç®—æ³¨æ„åŠ›å±‚ 123456# é…ç½®é‡è®¡ç®—config = TransformerConfig( recompute_granularity=&#x27;selective&#x27;, # full/selective/partial recompute_method=&#x27;uniform&#x27;, # uniform/block recompute_num_layers=1, # é‡è®¡ç®—å±‚æ•°) 7.4 åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ç‰¹æ€§: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆç±»ä¼¼ ZeRO-1ï¼‰ é‡å é€šä¿¡å’Œè®¡ç®— æ”¯æŒæ¢¯åº¦ç´¯ç§¯ 1234567891011# åˆ†å¸ƒå¼ä¼˜åŒ–å™¨é…ç½®optimizer_config = OptimizerConfig( optimizer=&#x27;adam&#x27;, lr=1e-4, weight_decay=0.1, adam_beta1=0.9, adam_beta2=0.999, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True,) 7.5 Flash Attentioné€šè¿‡ Transformer Engine é›†æˆï¼š ä¼˜åŠ¿: é™ä½ HBM è®¿é—® O(N) å†…å­˜å¤æ‚åº¦ 2-4å€åŠ é€Ÿ 12345# å¯ç”¨ Flash Attentionconfig = TransformerConfig( attention_backend=&#x27;flash&#x27;, # transformer_engine/torch attention_dropout=0.0,) 7.6 Multi-Latent Attention (MLA)æ¦‚å¿µ: DeepSeek-V3 å¼•å…¥çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ½œåœ¨å‹ç¼©é™ä½KV Cacheå†…å­˜ æ¶æ„ç‰¹ç‚¹: 12345678910111213141516171819202122232425262728293031graph LR Input[è¾“å…¥ Hidden States] subgraph &quot;MLA Layer&quot; QProj[Q Projection] QDown[Q Down Projection&lt;br/&gt;é™ç»´åˆ°æ½œåœ¨ç©ºé—´] QUp[Q Up Projection&lt;br/&gt;å‡ç»´] KVDown[KV Down Projection&lt;br/&gt;å…±äº«å‹ç¼©] KVUp[KV Up Projection&lt;br/&gt;åˆ†ç¦»Kå’ŒV] RoPE[Rotary Position&lt;br/&gt;Embedding] Attn[Attention&lt;br/&gt;Computation] end Output[è¾“å‡º] Input --&gt; QProj QProj --&gt; QDown QDown --&gt; QUp QUp --&gt; RoPE Input --&gt; KVDown KVDown --&gt; KVUp KVUp --&gt; RoPE RoPE --&gt; Attn Attn --&gt; Output style KVDown fill:#FFB6C1 style QDown fill:#98D8C8 å…³é”®ä¼˜åŠ¿: å†…å­˜æ•ˆç‡: KV Cache å†…å­˜é™ä½ 75%+ è®¡ç®—æ•ˆç‡: å‡å°‘æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ é•¿åºåˆ—æ”¯æŒ: æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ é…ç½®ç¤ºä¾‹: 1234567891011121314151617# MLA é…ç½®mla_config = MLATransformerConfig( # åŸºç¡€é…ç½® hidden_size=5120, num_attention_heads=128, # MLA ç‰¹å®šé…ç½® q_lora_rank=1536, # Qçš„LoRAç§© kv_lora_rank=512, # KVçš„LoRAç§© qk_rope_head_dim=64, # RoPEç»´åº¦ v_head_dim=128, # Vå¤´ç»´åº¦ qk_nope_head_dim=128, # éRoPEéƒ¨åˆ†ç»´åº¦ # ä¼˜åŒ–é€‰é¡¹ use_fused_rope=True, # èåˆRoPEç®—å­ cache_kv_in_compressed_form=True, # ç¼“å­˜å‹ç¼©å½¢å¼çš„KV) 7.7 HyperCommGridæ¦‚å¿µ: Nç»´é€šä¿¡ç½‘æ ¼ï¼Œçµæ´»ç®¡ç†å¤šç§å¹¶è¡Œç­–ç•¥çš„è¿›ç¨‹ç»„ ç‰¹ç‚¹: æ”¯æŒä»»æ„ç»´åº¦çš„å¹¶è¡Œç»„åˆ åŠ¨æ€åˆ›å»ºè¿›ç¨‹ç»„ é¿å…é‡å¤åˆ›å»ºç›¸åŒç»´åº¦ç»„åˆ ä½¿ç”¨ç¤ºä¾‹: 123456789101112131415from megatron.core.hyper_comm_grid import HyperCommGrid# åˆ›å»º4ç»´å¹¶è¡Œç½‘æ ¼: DP x TP x PP x EPgrid = HyperCommGrid( dim_names=[&#x27;dp&#x27;, &#x27;tp&#x27;, &#x27;pp&#x27;, &#x27;ep&#x27;], dim_sizes=[8, 4, 2, 4], world_size=256,)# åˆ›å»ºç‰¹å®šç»´åº¦çš„è¿›ç¨‹ç»„grid.create_pg([&#x27;tp&#x27;, &#x27;pp&#x27;]) # TP+PP ç»„åˆgrid.create_pg([&#x27;dp&#x27;, &#x27;ep&#x27;]) # DP+EP ç»„åˆ# è·å–è¿›ç¨‹ç»„tp_pp_group = grid.get_pg([&#x27;tp&#x27;, &#x27;pp&#x27;]) 7.8 åŠ¨æ€æ¨ç†å¼•æ“ç‰¹æ€§: In-flight Batching: åŠ¨æ€æ‰¹å¤„ç†ï¼Œæå‡ååé‡ Chunked KV Cache: åˆ†å—KVç¼“å­˜ç®¡ç† Multi-batch CUDA Graphs: å¤šæ‰¹æ¬¡CUDAå›¾ä¼˜åŒ– Async Support: å¼‚æ­¥æ¨ç†æ”¯æŒ æ¨ç†å¼•æ“æ¶æ„: 12345678910111213141516171819202122232425262728293031graph TB Client[æ¨ç†å®¢æˆ·ç«¯] subgraph &quot;æ¨ç†å¼•æ“&quot; Scheduler[è°ƒåº¦å™¨&lt;br/&gt;Scheduler] Coordinator[åè°ƒå™¨&lt;br/&gt;DP Coordinator] subgraph &quot;æ‰§è¡Œå±‚&quot; Engine1[æ¨ç†å¼•æ“ GPU-0] Engine2[æ¨ç†å¼•æ“ GPU-1] EngineN[æ¨ç†å¼•æ“ GPU-N] end KVCache[KV Cache ç®¡ç†å™¨] BatchManager[æ‰¹å¤„ç†ç®¡ç†å™¨] end Client --&gt; Scheduler Scheduler --&gt; Coordinator Coordinator --&gt; Engine1 Coordinator --&gt; Engine2 Coordinator --&gt; EngineN Engine1 --&gt; KVCache Engine2 --&gt; KVCache EngineN --&gt; KVCache Scheduler --&gt; BatchManager style Scheduler fill:#4CAF50 style KVCache fill:#FF9800 ä½¿ç”¨ç¤ºä¾‹: 12345678910111213141516171819from megatron.core.inference import DynamicInferenceEngine# åˆ›å»ºæ¨ç†å¼•æ“engine = DynamicInferenceEngine( model=model, tokenizer=tokenizer, max_batch_size=64, max_sequence_length=8192, enable_cuda_graph=True,)# å¼‚æ­¥æ¨ç†async def generate(): response = await engine.generate_async( prompts=[&quot;Hello, how are you?&quot;], max_new_tokens=100, temperature=0.7, ) return response 7.9 å®¹é”™è®­ç»ƒï¼ˆNVRxï¼‰NVIDIA Resiliency Extension é›†æˆ: åŠŸèƒ½: Straggler Detection: æ‰é˜Ÿæ£€æµ‹ Fault Detection: æ•…éšœæ£€æµ‹ Hang Detection: æŒ‚èµ·æ£€æµ‹ Automatic Recovery: è‡ªåŠ¨æ¢å¤ é…ç½®: 12345# å¯ç”¨å®¹é”™è®­ç»ƒ--enable-ft-pipeline--ft-timeout 300--straggler-detector-enabled--straggler-detector-window-size 10 7.10 CUDA Graphsä¼˜åŒ–å†…æ ¸å¯åŠ¨å¼€é”€: 1234567# CUDA Graphs é…ç½®if args.use_cuda_graph: cuda_graph = FullCudaGraphWrapper( model=model, optimizer=optimizer, data_loader=train_data_iterator, ) MoE CUDA Graphs ä¼˜åŒ–: æ”¯æŒåŠ¨æ€ä¸“å®¶é€‰æ‹© ä¼˜åŒ–Tokenåˆ†å‘è·¯å¾„ å‡å°‘å†…æ ¸å¯åŠ¨å¼€é”€ 8. ä»£ç ç»„ç»‡ç»“æ„8.1 å…³é”®æ–‡ä»¶æ¸…å•è®­ç»ƒå…¥å£ æ–‡ä»¶ åŠŸèƒ½ pretrain_gpt.py GPT æ¨¡å‹é¢„è®­ç»ƒå…¥å£ pretrain_bert.py BERT æ¨¡å‹é¢„è®­ç»ƒå…¥å£ pretrain_t5.py T5 æ¨¡å‹é¢„è®­ç»ƒå…¥å£ pretrain_vlm.py å¤šæ¨¡æ€æ¨¡å‹é¢„è®­ç»ƒå…¥å£ æ ¸å¿ƒç»„ä»¶ ç›®å½•&#x2F;æ–‡ä»¶ åŠŸèƒ½ megatron/core/transformer/ Transformer æ ¸å¿ƒç»„ä»¶ megatron/core/models/ æ¨¡å‹å®ç° megatron/core/parallel_state.py å¹¶è¡ŒçŠ¶æ€ç®¡ç† megatron/core/hyper_comm_grid.py Nç»´é€šä¿¡ç½‘æ ¼ç®¡ç† megatron/core/tensor_parallel/ å¼ é‡å¹¶è¡Œå®ç° megatron/core/pipeline_parallel/ æµæ°´çº¿å¹¶è¡Œå®ç° megatron/core/optimizer/ ä¼˜åŒ–å™¨å®ç° megatron/core/datasets/ æ•°æ®é›†åŠ è½½å™¨ megatron/core/inference/ æ¨ç†å¼•æ“ megatron/core/transformer/moe/ MoE å®ç° megatron/core/transformer/multi_latent_attention.py MLA å®ç° megatron/training/training.py è®­ç»ƒä¸»å¾ªç¯ megatron/training/checkpointing.py æ£€æŸ¥ç‚¹ç®¡ç† å·¥å…·è„šæœ¬ æ–‡ä»¶ åŠŸèƒ½ tools/preprocess_data.py æ•°æ®é¢„å¤„ç† tools/checkpoint/ æ£€æŸ¥ç‚¹è½¬æ¢å·¥å…· tools/run_text_generation_server.py æ¨ç†æœåŠ¡å™¨ 8.2 é…ç½®ç®¡ç†å±‚æ¬¡åŒ–é…ç½®: 123456789101112131415161718graph TD GlobalConfig[å…¨å±€é…ç½®&lt;br/&gt;megatron/training/arguments.py] ModelConfig[æ¨¡å‹é…ç½®&lt;br/&gt;TransformerConfig] ParallelConfig[å¹¶è¡Œé…ç½®&lt;br/&gt;ModelParallelConfig] OptimizerConfig[ä¼˜åŒ–å™¨é…ç½®&lt;br/&gt;OptimizerConfig] DataConfig[æ•°æ®é…ç½®&lt;br/&gt;DataConfig] GlobalConfig --&gt; ModelConfig GlobalConfig --&gt; ParallelConfig GlobalConfig --&gt; OptimizerConfig GlobalConfig --&gt; DataConfig LayerSpec[å±‚è§„èŒƒ&lt;br/&gt;ModuleSpec] ModelConfig --&gt; LayerSpec ProcessGroups[è¿›ç¨‹ç»„&lt;br/&gt;ProcessGroupCollection] ParallelConfig --&gt; ProcessGroups 8.3 æµ‹è¯•ä½“ç³»12345678910111213tests/â”œâ”€â”€ unit_tests/ # å•å…ƒæµ‹è¯•â”‚ â”œâ”€â”€ data/ # æ•°æ®åŠ è½½æµ‹è¯•â”‚ â”œâ”€â”€ dist_checkpointing/ # æ£€æŸ¥ç‚¹æµ‹è¯•â”‚ â”œâ”€â”€ distributed/ # åˆ†å¸ƒå¼æµ‹è¯•â”‚ â”œâ”€â”€ inference/ # æ¨ç†æµ‹è¯•â”‚ â”œâ”€â”€ models/ # æ¨¡å‹æµ‹è¯•â”‚ â”œâ”€â”€ pipeline_parallel/ # æµæ°´çº¿å¹¶è¡Œæµ‹è¯•â”‚ â”œâ”€â”€ tensor_parallel/ # å¼ é‡å¹¶è¡Œæµ‹è¯•â”‚ â””â”€â”€ transformer/ # Transformeræµ‹è¯•â””â”€â”€ functional_tests/ # åŠŸèƒ½æµ‹è¯• â”œâ”€â”€ test_scripts/ # æµ‹è¯•è„šæœ¬ â””â”€â”€ test_results/ # æµ‹è¯•ç»“æœ 9. æ€§èƒ½ä¼˜åŒ–æŠ€å·§9.1 å†…å­˜ä¼˜åŒ–1234567891011121314151617181920graph LR subgraph &quot;å†…å­˜ä¼˜åŒ–ç­–ç•¥&quot; A[æ¿€æ´»é‡è®¡ç®—&lt;br/&gt;Activation Recomputation] B[æ¢¯åº¦ç´¯ç§¯&lt;br/&gt;Gradient Accumulation] C[CPU Offloading&lt;br/&gt;å‚æ•°/ä¼˜åŒ–å™¨çŠ¶æ€] D[åºåˆ—å¹¶è¡Œ&lt;br/&gt;Sequence Parallel] E[æ··åˆç²¾åº¦&lt;br/&gt;Mixed Precision] F[Flash Attention&lt;br/&gt;é«˜æ•ˆæ³¨æ„åŠ›] end Memory[å†…å­˜ä½¿ç”¨] A --&gt; Memory B --&gt; Memory C --&gt; Memory D --&gt; Memory E --&gt; Memory F --&gt; Memory style Memory fill:#FF6B6B 9.2 é€šä¿¡ä¼˜åŒ– é‡å é€šä¿¡å’Œè®¡ç®— æ¢¯åº¦è§„çº¦ä¸åå‘ä¼ æ’­é‡å  å‚æ•°æ”¶é›†ä¸å‰å‘ä¼ æ’­é‡å  é€šä¿¡èåˆ å¤šä¸ªå°é€šä¿¡æ“ä½œèåˆä¸ºä¸€ä¸ªå¤§æ“ä½œ å‡å°‘é€šä¿¡æ¬¡æ•° é€šä¿¡å‹ç¼© FP16&#x2F;BF16 æ¢¯åº¦é€šä¿¡ æ¢¯åº¦å‹ç¼©ç®—æ³• 9.3 è®¡ç®—ä¼˜åŒ– å†…æ ¸èåˆ LayerNorm + Dropout Bias + GELU Softmax + Mask é«˜æ•ˆç®—å­ Flash Attention Fused Adam Fused LayerNorm CUDA Graphs å‡å°‘å†…æ ¸å¯åŠ¨å¼€é”€ 10. æœ€ä½³å®è·µ10.1 å¯åŠ¨é…ç½®ç¤ºä¾‹123456789101112131415161718192021222324#!/bin/bash# å…³é”®å‚æ•°é…ç½®ç¤ºä¾‹torchrun \\ --nproc_per_node 8 \\ --nnodes 8 \\ pretrain_gpt.py \\ --tensor-model-parallel-size 4 \\ --pipeline-model-parallel-size 2 \\ --num-layers 40 \\ --hidden-size 5120 \\ --num-attention-heads 40 \\ --seq-length 4096 \\ --micro-batch-size 1 \\ --global-batch-size 256 \\ --lr 1.5e-4 \\ --min-lr 1.5e-5 \\ --lr-decay-style cosine \\ --weight-decay 0.1 \\ --clip-grad 1.0 \\ --fp16 \\ --use-distributed-optimizer \\ --overlap-grad-reduce \\ --overlap-param-gather å…³é”®å‚æ•°è¯´æ˜: tensor-model-parallel-size: å¼ é‡å¹¶è¡Œåº¦ pipeline-model-parallel-size: æµæ°´çº¿å¹¶è¡Œåº¦ global-batch-size: å…¨å±€æ‰¹æ¬¡å¤§å° &#x3D; micro-batch-size Ã— DP Ã— æ¢¯åº¦ç´¯ç§¯æ­¥æ•° overlap-grad-reduce: é‡å æ¢¯åº¦é€šä¿¡ä¸è®¡ç®— use-distributed-optimizer: å¯ç”¨åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ 10.2 è°ƒè¯•æŠ€å·§ å¯ç”¨è¯¦ç»†æ—¥å¿— 12export NCCL_DEBUG=INFOexport TORCH_DISTRIBUTED_DEBUG=DETAIL æ£€æŸ¥å¼ é‡å½¢çŠ¶ 123from megatron.core import mpuprint(f&quot;TP rank: &#123;mpu.get_tensor_model_parallel_rank()&#125;&quot;)print(f&quot;Tensor shape: &#123;tensor.shape&#125;&quot;) éªŒè¯æ¢¯åº¦ 12345# æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸º NaNfor name, param in model.named_parameters(): if param.grad is not None: if torch.isnan(param.grad).any(): print(f&quot;NaN gradient in &#123;name&#125;&quot;) 11. æ€»ç»“11.1 æ ¸å¿ƒä¼˜åŠ¿ é«˜æ€§èƒ½: GPU ä¼˜åŒ–çš„å†…æ ¸å’Œé€šä¿¡ç­–ç•¥ å¯æ‰©å±•: æ”¯æŒåƒå¡çº§åˆ«çš„å¤§è§„æ¨¡è®­ç»ƒ çµæ´»æ€§: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºå®šåˆ¶ ç”Ÿæ€ä¸°å¯Œ: ä¸å¤šä¸ªæ¡†æ¶å’Œå·¥å…·é›†æˆ 11.2 é€‚ç”¨åœºæ™¯ å¤§è§„æ¨¡é¢„è®­ç»ƒï¼ˆ1B - 1000B+ å‚æ•°ï¼‰ å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒï¼ˆæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ï¼‰ ç»†ç²’åº¦ MoE æ¨¡å‹è®­ç»ƒï¼ˆDeepSeek-V3ã€Qwen3ã€Mixtralï¼‰ è¶…é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆ32K - 256K+ tokensï¼‰ é«˜æ€§èƒ½åˆ†å¸ƒå¼æ¨ç† Blackwell å¹³å°ä¼˜åŒ–è®­ç»ƒ è·¨æ•°æ®ä¸­å¿ƒè®­ç»ƒï¼ˆN&#x2F;Sè¿æ¥ï¼‰ 11.3 å­¦ä¹ è·¯å¾„123456789graph LR A[ç†è§£åŸºç¡€æ¦‚å¿µ] --&gt; B[å­¦ä¹ å¹¶è¡Œç­–ç•¥] B --&gt; C[è¿è¡Œç¤ºä¾‹ä»£ç ] C --&gt; D[è‡ªå®šä¹‰æ¨¡å‹] D --&gt; E[æ€§èƒ½ä¼˜åŒ–] E --&gt; F[ç”Ÿäº§éƒ¨ç½²] style A fill:#90EE90 style F fill:#FF6B6B 11.4 å‚è€ƒèµ„æº å®˜æ–¹æ–‡æ¡£: https://docs.nvidia.com/Megatron-Core/ GitHub ä»“åº“: https://github.com/NVIDIA/Megatron-LM è®ºæ–‡: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM Reducing Activation Recomputation in Large Transformer Models æŠ¥å‘Šç»“æŸ æ­¤æŠ¥å‘ŠåŸºäº Megatron-LM v0.14.0 ä»£ç åº“åˆ†æç”Ÿæˆ","categories":[{"name":"ç³»ç»Ÿæ¶æ„åˆ†æ","slug":"ç³»ç»Ÿæ¶æ„åˆ†æ","permalink":"https://nash635.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Megatron","slug":"Megatron","permalink":"https://nash635.github.io/tags/Megatron/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://nash635.github.io/tags/NVIDIA/"},{"name":"å¤§è¯­è¨€æ¨¡å‹","slug":"å¤§è¯­è¨€æ¨¡å‹","permalink":"https://nash635.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"åˆ†å¸ƒå¼è®­ç»ƒ","slug":"åˆ†å¸ƒå¼è®­ç»ƒ","permalink":"https://nash635.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"name":"Transformer","slug":"Transformer","permalink":"https://nash635.github.io/tags/Transformer/"}]},{"title":"HCCL é›†åˆé€šä¿¡åº“è®¾è®¡åˆ†æ","slug":"HCCL_DeepDive","date":"2025-11-17T02:00:01.000Z","updated":"2025-11-17T14:42:49.978Z","comments":true,"path":"2025/11/17/HCCL_DeepDive/","permalink":"https://nash635.github.io/2025/11/17/HCCL_DeepDive/","excerpt":"","text":"HCCL (Huawei Collective Communication Library) è®¾è®¡æ–‡æ¡£1. é¡¹ç›®æ¦‚è¿°1.1 é¡¹ç›®ç®€ä»‹HCCLï¼ˆHuawei Collective Communication Libraryï¼Œåä¸ºé›†åˆé€šä¿¡åº“ï¼‰æ˜¯åŸºäºæ˜‡è…¾AIå¤„ç†å™¨çš„é«˜æ€§èƒ½é›†åˆé€šä¿¡åº“ï¼Œä¸ºå•æœºå¤šå¡åŠå¤šæœºå¤šå¡ç¯å¢ƒæä¾›é«˜æ•ˆçš„æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œé›†åˆé€šä¿¡æ–¹æ¡ˆã€‚ å¼€æºä»£ç åº“ï¼šhttps://gitee.com/ascend/cann-hccl ç‰ˆæœ¬ä¿¡æ¯ï¼š é…å¥—CANNè½¯ä»¶ç‰ˆæœ¬å‘è¡Œ è®¸å¯è¯ï¼š CANN Open Software License Agreement Version 1.0 1.2 æ ¸å¿ƒç‰¹æ€§ âœ… é«˜æ€§èƒ½é€šä¿¡ç®—æ³•ï¼šæ”¯æŒ9ç§æ‹“æ‰‘ç®—æ³•ï¼ˆMeshã€Ringã€RHDã€PairWiseã€Starã€NHRã€NBã€AHCã€Pipelineï¼‰ âœ… çµæ´»çš„é€šä¿¡æ¨¡å¼ï¼šæ”¯æŒå•æœºå¤šå¡å’Œå¤šæœºå¤šå¡åœºæ™¯ âœ… æ™ºèƒ½ç®—æ³•é€‰æ‹©ï¼šæ ¹æ®é€šä¿¡åŸŸä¿¡æ¯å’Œæ•°æ®é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³• âœ… åˆ†å±‚ç½‘ç»œä¼˜åŒ–ï¼šæ”¯æŒServerå†…å’ŒServeré—´åˆ†çº§é€šä¿¡ âœ… å¤šç§é›†åˆæ“ä½œï¼šAllReduceã€AllGatherã€ReduceScatterã€Broadcastç­‰ 1.3 ç³»ç»Ÿæ¶æ„1234567891011121314151617181920212223242526272829303132333435363738394041424344graph TB subgraph &quot;é€‚é…å±‚&quot; A[å›¾å¼•æ“é€‚é…] --&gt; B[å•ç®—å­é€‚é…] B --&gt; C[é€šä¿¡åˆ‡åˆ†ä¼˜åŒ–] end subgraph &quot;é›†åˆé€šä¿¡ä¸šåŠ¡å±‚&quot; D[é€šä¿¡æ¡†æ¶æ¨¡å—] E[é€šä¿¡ç®—æ³•æ¨¡å—] D --&gt; D1[é€šä¿¡åŸŸç®¡ç†] D --&gt; D2[ç®—å­ä¸šåŠ¡ä¸²è”] D --&gt; D3[ç®—æ³•é€‰æ‹©] D --&gt; D4[èµ„æºç”³è¯·] D --&gt; D5[ä»»åŠ¡ä¸‹å‘] E --&gt; E1[Meshç®—æ³•] E --&gt; E2[Ringç®—æ³•] E --&gt; E3[RHDç®—æ³•] E --&gt; E4[PairWiseç®—æ³•] E --&gt; E5[Starç®—æ³•] E --&gt; E6[NHRç®—æ³•] E --&gt; E7[NBç®—æ³•] E --&gt; E8[AHCç®—æ³•] E --&gt; E9[Pipelineç®—æ³•] end subgraph &quot;é›†åˆé€šä¿¡å¹³å°å±‚&quot; F[èµ„æºæŠ½è±¡] G[ç»´æµ‹èƒ½åŠ›] end C --&gt; D D &lt;--&gt; E D --&gt; F F --&gt; G style D fill:#e1d5e7 style E fill:#e1d5e7 style D1 fill:#fff2cc style D2 fill:#fff2cc style D3 fill:#fff2cc style D4 fill:#fff2cc style D5 fill:#fff2cc 2. æ ¸å¿ƒæ¶æ„è®¾è®¡2.1 ä¸‰å±‚æ¶æ„æ¨¡å‹HCCLé‡‡ç”¨åˆ†å±‚è®¾è®¡ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒå±‚æ¬¡ï¼š 1234567891011121314graph LR A[é€‚é…å±‚] --&gt; B[ä¸šåŠ¡å±‚] B --&gt; C[å¹³å°å±‚] subgraph &quot;æœ¬æºç ä»“å®ç°èŒƒå›´&quot; B1[é€šä¿¡æ¡†æ¶] B2[é€šä¿¡ç®—æ³•] end B -.åŒ…å«.-&gt; B1 B -.åŒ…å«.-&gt; B2 style B1 fill:#e1d5e7 style B2 fill:#e1d5e7 2.1.1 é€‚é…å±‚èŒè´£ï¼š å›¾å¼•æ“ä¸å•ç®—å­çš„å¯¹æ¥é€‚é… é€šä¿¡æ“ä½œçš„åˆ‡åˆ†ä¸ä¼˜åŒ– ä»»åŠ¡åˆ†å‘ç­–ç•¥åˆ¶å®š 2.1.2 é›†åˆé€šä¿¡ä¸šåŠ¡å±‚ï¼ˆæœ¬ä»“æ ¸å¿ƒï¼‰é€šä¿¡æ¡†æ¶æ¨¡å—ï¼š é€šä¿¡åŸŸï¼ˆCommunicatorï¼‰ç”Ÿå‘½å‘¨æœŸç®¡ç† é›†åˆé€šä¿¡ç®—å­çš„ä¸šåŠ¡æµç¨‹ç¼–æ’ ç®—æ³•é€‰æ‹©ç­–ç•¥ä¸è°ƒåº¦ ä¸å¹³å°å±‚åä½œå®Œæˆèµ„æºç”³è¯· ä»»åŠ¡ä¸‹å‘ä¸æ‰§è¡Œç®¡ç† é€šä¿¡ç®—æ³•æ¨¡å—ï¼š å®ç°9ç§æ ¸å¿ƒé›†åˆé€šä¿¡ç®—æ³• èµ„æºæ¶ˆè€—è®¡ç®—ä¸è¯„ä¼° åŸºäºé€šä¿¡åŸŸä¿¡æ¯çš„ä»»åŠ¡ç¼–æ’ ç®—æ³•æ€§èƒ½æ¨¡å‹ï¼ˆÎ±-Î²æ¨¡å‹ï¼‰å®ç° 2.1.3 é›†åˆé€šä¿¡å¹³å°å±‚èŒè´£ï¼š NPUç¡¬ä»¶èµ„æºæŠ½è±¡ä¸ç®¡ç† HCCSé“¾è·¯èµ„æºç®¡ç† é€šä¿¡æ—¥å¿—ä¸æ€§èƒ½ç›‘æ§ é”™è¯¯è¯Šæ–­ä¸æ¢å¤æœºåˆ¶ 2.2 ç›®å½•ç»“æ„1234567891011cann-hccl/â”œâ”€â”€ src/domain/collective_communication/â”‚ â”œâ”€â”€ algorithm/ # é€šä¿¡ç®—æ³•å®ç°â”‚ â””â”€â”€ framework/ # é€šä¿¡æ¡†æ¶å®ç°â”œâ”€â”€ inc/hccl/ # å¯¹å¤–å¤´æ–‡ä»¶â”‚ â”œâ”€â”€ hccl.hâ”‚ â””â”€â”€ hccl_types.hâ”œâ”€â”€ docs/ # ç®—æ³•åŸç†æ–‡æ¡£â”œâ”€â”€ test/ # æµ‹è¯•ä»£ç â”œâ”€â”€ cmake/ # ç¼–è¯‘é…ç½®â””â”€â”€ build.sh # ç¼–è¯‘è„šæœ¬ 3. é›†åˆé€šä¿¡ç®—æ³•è¯¦è§£HCCLçš„æ ¸å¿ƒç«äº‰åŠ›åœ¨äºå…¶ä¸°å¯Œçš„é›†åˆé€šä¿¡ç®—æ³•åº“ï¼Œé’ˆå¯¹ä¸åŒçš„ç½‘ç»œæ‹“æ‰‘ã€èŠ‚ç‚¹è§„æ¨¡å’Œæ•°æ®é‡æä¾›æœ€ä¼˜è§£å†³æ–¹æ¡ˆã€‚ 3.1 æ€§èƒ½è¯„ä¼°æ¨¡å‹HCCLé‡‡ç”¨ Î±-Î²æ¨¡å‹ï¼ˆHockneyæ¨¡å‹ï¼‰ è¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼š $$T &#x3D; \\alpha + n\\beta + n\\gamma$$ å‚æ•°è¯´æ˜ï¼š Î±ï¼šèŠ‚ç‚¹é—´çš„å›ºå®šæ—¶å»¶ï¼ˆå¯åŠ¨å¼€é”€ï¼‰ Î²ï¼šæ¯byteæ•°æ®ä¼ è¾“è€—æ—¶ï¼ˆå¸¦å®½å€’æ•°ï¼‰ nï¼šé€šä¿¡æ•°æ®å¤§å°ï¼ˆbytesï¼‰ Î³ï¼šæ¯byteæ•°æ®è§„çº¦è®¡ç®—è€—æ—¶ pï¼šé€šä¿¡åŸŸèŠ‚ç‚¹ä¸ªæ•° 3.2 Mesh ç®—æ³•3.2.1 ç®—æ³•åŸç†123456789101112graph LR N0((Rank0)) ---|å…¨è¿æ¥| N1((Rank1)) N0 ---|å…¨è¿æ¥| N2((Rank2)) N0 ---|å…¨è¿æ¥| N3((Rank3)) N1 ---|å…¨è¿æ¥| N2 N1 ---|å…¨è¿æ¥| N3 N2 ---|å…¨è¿æ¥| N3 style N0 fill:#ffcccc style N1 fill:#ccffcc style N2 fill:#ccccff style N3 fill:#ffffcc ç‰¹ç‚¹ï¼š æ‹“æ‰‘ï¼š FullMeshäº’è”ï¼ŒNPUé—´å…¨è¿æ¥ æ—¶é—´å¤æ‚åº¦ï¼š O(1) é€‚ç”¨åœºæ™¯ï¼š Serverå†…é€šä¿¡ï¼Œå°è§„æ¨¡é›†ç¾¤ ä¼˜åŠ¿ï¼š ä¸€æ­¥å®Œæˆé€šä¿¡ï¼Œå»¶è¿Ÿæœ€ä½ åŠ£åŠ¿ï¼š èµ„æºå¼€é”€å¤§ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡ 3.2.2 æ‰§è¡Œæµç¨‹ç¤ºä¾‹ï¼ˆä»¥AllReduceä¸ºä¾‹ï¼‰è¯´æ˜ï¼š Meshç®—æ³•æ”¯æŒæ‰€æœ‰é›†åˆé€šä¿¡åŸè¯­ï¼ˆAllReduceã€AllGatherã€ReduceScatterã€Broadcastã€Reduceã€Scatterã€Gatherç­‰ï¼‰ï¼Œæ­¤å¤„ä»¥AllReduceä¸ºå…¸å‹ç¤ºä¾‹å±•ç¤ºæ‰§è¡Œæµç¨‹ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243sequenceDiagram participant R0 as Rank0 participant R1 as Rank1 participant R2 as Rank2 participant R3 as Rank3 Note over R0,R3: Phase 1: ReduceScatter (å¹¶å‘) Note over R0,R3: æ¯ä¸ªèŠ‚ç‚¹å°†æ•°æ®åˆ‡åˆ†ä¸ºpä»½ï¼Œå¹¶å‘å‘é€ç»™æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ R0-&gt;&gt;R1: å‘é€chunk_1 R0-&gt;&gt;R2: å‘é€chunk_2 R0-&gt;&gt;R3: å‘é€chunk_3 R1-&gt;&gt;R0: å‘é€chunk_0 R1-&gt;&gt;R2: å‘é€chunk_2 R1-&gt;&gt;R3: å‘é€chunk_3 R2-&gt;&gt;R0: å‘é€chunk_0 R2-&gt;&gt;R1: å‘é€chunk_1 R2-&gt;&gt;R3: å‘é€chunk_3 R3-&gt;&gt;R0: å‘é€chunk_0 R3-&gt;&gt;R1: å‘é€chunk_1 R3-&gt;&gt;R2: å‘é€chunk_2 Note over R0,R3: Phase 2: æœ¬åœ°Reduce Note over R0: è§„çº¦chunk_0 Note over R1: è§„çº¦chunk_1 Note over R2: è§„çº¦chunk_2 Note over R3: è§„çº¦chunk_3 Note over R0,R3: Phase 3: AllGather (å¹¶å‘) Note over R0,R3: æ¯ä¸ªèŠ‚ç‚¹å¹¶å‘å‘æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å‘é€è‡ªå·±çš„è§„çº¦ç»“æœ R0-&gt;&gt;R1: å¹¿æ’­chunk_0* R0-&gt;&gt;R2: å¹¿æ’­chunk_0* R0-&gt;&gt;R3: å¹¿æ’­chunk_0* R1-&gt;&gt;R0: å¹¿æ’­chunk_1* R1-&gt;&gt;R2: å¹¿æ’­chunk_1* R1-&gt;&gt;R3: å¹¿æ’­chunk_1* R2-&gt;&gt;R0: å¹¿æ’­chunk_2* R2-&gt;&gt;R1: å¹¿æ’­chunk_2* R2-&gt;&gt;R3: å¹¿æ’­chunk_2* R3-&gt;&gt;R0: å¹¿æ’­chunk_3* R3-&gt;&gt;R1: å¹¿æ’­chunk_3* R3-&gt;&gt;R2: å¹¿æ’­chunk_3* Note over R0,R3: æ‰€æœ‰èŠ‚ç‚¹æŒæœ‰å®Œæ•´è§„çº¦ç»“æœ æ‰§è¡Œæµç¨‹è¯¦ç»†æè¿°ï¼š Phase 1: ReduceScatterï¼ˆå¹¶å‘æ‰§è¡Œï¼‰ æ•°æ®å‡†å¤‡: æ¯ä¸ªèŠ‚ç‚¹å°†è‡ªå·±çš„nå­—èŠ‚æ•°æ®åˆ‡åˆ†ä¸ºpä¸ªchunkï¼Œæ¯ä¸ªchunkå¤§å°ä¸ºn&#x2F;på­—èŠ‚ å¹¶å‘å‘é€: Rank0ä¿ç•™chunk_0ï¼Œå°†chunk_1å‘é€ç»™Rank1ï¼Œchunk_2å‘é€ç»™Rank2ï¼Œchunk_3å‘é€ç»™Rank3 Rank1ä¿ç•™chunk_1ï¼Œå°†chunk_0å‘é€ç»™Rank0ï¼Œchunk_2å‘é€ç»™Rank2ï¼Œchunk_3å‘é€ç»™Rank3 Rank2ä¿ç•™chunk_2ï¼Œå°†chunk_0å‘é€ç»™Rank0ï¼Œchunk_1å‘é€ç»™Rank1ï¼Œchunk_3å‘é€ç»™Rank3 Rank3ä¿ç•™chunk_3ï¼Œå°†chunk_0å‘é€ç»™Rank0ï¼Œchunk_1å‘é€ç»™Rank1ï¼Œchunk_2å‘é€ç»™Rank2 é€šä¿¡ç‰¹ç‚¹: å…¨è¿æ¥å¹¶å‘ï¼Œæ‰€æœ‰é€šä¿¡åŒæ—¶è¿›è¡Œï¼Œåˆ©ç”¨FullMeshæ‹“æ‰‘çš„åŒå‘å¸¦å®½ Phase 2: æœ¬åœ°Reduceï¼ˆæœ¬åœ°è®¡ç®—ï¼‰ Rank0å¯¹æ¥æ”¶åˆ°çš„æ‰€æœ‰chunk_0è¿›è¡Œè§„çº¦ï¼šchunk_0* &#x3D; chunk_0(R0) + chunk_0(R1) + chunk_0(R2) + chunk_0(R3) Rank1å¯¹æ¥æ”¶åˆ°çš„æ‰€æœ‰chunk_1è¿›è¡Œè§„çº¦ï¼šchunk_1* &#x3D; chunk_1(R0) + chunk_1(R1) + chunk_1(R2) + chunk_1(R3) Rank2å¯¹æ¥æ”¶åˆ°çš„æ‰€æœ‰chunk_2è¿›è¡Œè§„çº¦ï¼šchunk_2* &#x3D; chunk_2(R0) + chunk_2(R1) + chunk_2(R2) + chunk_2(R3) Rank3å¯¹æ¥æ”¶åˆ°çš„æ‰€æœ‰chunk_3è¿›è¡Œè§„çº¦ï¼šchunk_3* &#x3D; chunk_3(R0) + chunk_3(R1) + chunk_3(R2) + chunk_3(R3) æ­¤æ—¶æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1&#x2F;pçš„å®Œæ•´è§„çº¦ç»“æœ Phase 3: AllGatherï¼ˆå¹¶å‘æ‰§è¡Œï¼‰ å¹¶å‘å¹¿æ’­: Rank0å°†chunk_0*å¹¶å‘å‘é€ç»™Rank1, Rank2, Rank3 Rank1å°†chunk_1*å¹¶å‘å‘é€ç»™Rank0, Rank2, Rank3 Rank2å°†chunk_2*å¹¶å‘å‘é€ç»™Rank0, Rank1, Rank3 Rank3å°†chunk_3*å¹¶å‘å‘é€ç»™Rank0, Rank1, Rank2 æœ€ç»ˆçŠ¶æ€: æ‰€æœ‰èŠ‚ç‚¹æŒæœ‰å®Œæ•´çš„è§„çº¦ç»“æœ[chunk_0*, chunk_1*, chunk_2*, chunk_3*] é€šä¿¡ç‰¹ç‚¹: å…¨è¿æ¥å¹¶å‘ï¼Œå……åˆ†åˆ©ç”¨FullMeshæ‹“æ‰‘çš„æ‰€æœ‰é“¾è·¯ å…¶ä»–åŸè¯­ï¼š AllGather: ç›´æ¥æ‰§è¡ŒPhase 3ï¼ˆå¹¶å‘æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹æ•°æ®ï¼‰ ReduceScatter: æ‰§è¡ŒPhase 1 + Phase 2ï¼ˆå¹¶å‘è§„çº¦ååˆ†æ•£ï¼‰ Broadcast: æ ¹èŠ‚ç‚¹å‘æ‰€æœ‰èŠ‚ç‚¹å¹¶å‘å‘é€å®Œæ•´æ•°æ® Gather: æ‰€æœ‰èŠ‚ç‚¹å‘æ ¹èŠ‚ç‚¹å¹¶å‘å‘é€æ•°æ® 3.2.3 æ€§èƒ½æ¨¡å‹ æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ Scatter $\\alpha + \\frac{1}{p}n\\beta$ ä¸€æ­¥å®Œæˆï¼Œæ ¹èŠ‚ç‚¹å‘pä¸ªèŠ‚ç‚¹å¹¶å‘å‘é€ï¼Œæ¯èŠ‚ç‚¹æ¥æ”¶n&#x2F;pæ•°æ® Gather $\\alpha + \\frac{1}{p}n\\beta$ ä¸€æ­¥å®Œæˆï¼Œpä¸ªèŠ‚ç‚¹å‘æ ¹èŠ‚ç‚¹å¹¶å‘å‘é€ï¼Œæ ¹èŠ‚ç‚¹æ¥æ”¶å…¨éƒ¨æ•°æ® Broadcast $2\\alpha + \\frac{2}{p}n\\beta$ Scatter + AllGatherå®ç°ï¼ˆä¸¤æ­¥ï¼‰ï¼Œæ¯æ­¥ä¼ è¾“éƒ¨åˆ†æ•°æ® Reduce $2\\alpha + \\frac{2}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter + Gatherå®ç°ï¼Œéœ€è§„çº¦æ‰€æœ‰è¾“å…¥æ•°æ® ReduceScatter $\\alpha + \\frac{1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ä¸€æ­¥å®Œæˆï¼Œå¹¶å‘è§„çº¦å¹¶åˆ†å‘ï¼Œæ¯èŠ‚ç‚¹æ¥æ”¶n&#x2F;pç»“æœ AllGather $\\alpha + \\frac{1}{p}n\\beta$ ä¸€æ­¥å®Œæˆï¼Œå…¨è¿æ¥å¹¶å‘ä¼ è¾“ï¼Œæ¯èŠ‚ç‚¹å‘é€n&#x2F;pæ•°æ® AllReduce $2\\alpha + \\frac{2}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter + AllGatherä¸¤é˜¶æ®µï¼Œæ€»å…±ä¸¤æ­¥é€šä¿¡ 3.3 Ring ç®—æ³•3.3.1 ç®—æ³•åŸç†123456789101112131415graph LR R0((Rank0)) --&gt;|å³æ‰‹å¡| R1((Rank1)) R1 --&gt;|å³æ‰‹å¡| R2((Rank2)) R2 --&gt;|å³æ‰‹å¡| R3((Rank3)) R3 --&gt;|å³æ‰‹å¡| R0 R0 -.-&gt;|å·¦æ‰‹å¡| R3 R1 -.-&gt;|å·¦æ‰‹å¡| R0 R2 -.-&gt;|å·¦æ‰‹å¡| R1 R3 -.-&gt;|å·¦æ‰‹å¡| R2 style R0 fill:#ffcccc style R1 fill:#ccffcc style R2 fill:#ccccff style R3 fill:#ffffcc ç‰¹ç‚¹ï¼š æ‹“æ‰‘ï¼š ç¯å½¢ç»“æ„ï¼Œæ¯ä¸ªèŠ‚ç‚¹åªä¸å·¦å³é‚»å±…é€šä¿¡ æ—¶é—´å¤æ‚åº¦ï¼š O(p-1) - çº¿æ€§å¤æ‚åº¦ é€‚ç”¨åœºæ™¯ï¼š Serverå†…å’ŒServeré—´é€šä¿¡ å°è§„æ¨¡é›†ç¾¤æˆ–å°æ•°æ®é‡ ç½‘ç»œæ‹¥å¡åœºæ™¯ Pipelineä¸é€‚ç”¨çš„åœºæ™¯ 3.3.2 æ‰§è¡Œæµç¨‹ç¤ºä¾‹ï¼ˆä»¥AllReduceä¸ºä¾‹ï¼‰è¯´æ˜ï¼š Ringç®—æ³•æ”¯æŒå¤šç§é›†åˆé€šä¿¡åŸè¯­ï¼ˆAllReduceã€AllGatherã€ReduceScatterã€Broadcastã€Reduceã€Scatterã€Gatherç­‰ï¼‰ï¼Œæ­¤å¤„ä»¥AllReduceä¸ºå…¸å‹ç¤ºä¾‹å±•ç¤ºæ‰§è¡Œæµç¨‹ã€‚ ä¸¤é˜¶æ®µæ¦‚è§ˆ123456789101112graph TB subgraph &quot;é˜¶æ®µ1: ReduceScatter (p-1æ­¥)&quot; A1[æ•°æ®åˆ‡åˆ†ä¸ºpå—] --&gt; A2[æ²¿ç¯ä¼ è¾“å¹¶è§„çº¦] A2 --&gt; A3[æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1/pè§„çº¦ç»“æœ] end subgraph &quot;é˜¶æ®µ2: AllGather (p-1æ­¥)&quot; B1[æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1/pæ•°æ®] --&gt; B2[æ²¿ç¯ä¼ è¾“å®Œæ•´æ•°æ®] B2 --&gt; B3[æ‰€æœ‰èŠ‚ç‚¹è·å¾—å®Œæ•´ç»“æœ] end A3 --&gt; B1 è¯¦ç»†æ‰§è¡Œæ­¥éª¤ï¼ˆ4èŠ‚ç‚¹ç¤ºä¾‹ï¼‰é˜¶æ®µ1: ReduceScatterï¼ˆp-1&#x3D;3æ­¥å®Œæˆï¼‰ Ringç®—æ³•çš„ReduceScatteré˜¶æ®µï¼šæ¯ä¸ªèŠ‚ç‚¹åœ¨æ¯ä¸€æ­¥éƒ½å‘å³é‚»å±…å‘é€ä¸€ä¸ªæ•°æ®å—ï¼Œå¹¶æ¥æ”¶å·¦é‚»å±…çš„æ•°æ®å—è¿›è¡Œè§„çº¦ã€‚ç»è¿‡p-1æ­¥åï¼Œæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰ä¸€ä¸ªå®Œæ•´è§„çº¦çš„æ•°æ®å—ã€‚ 1234567891011121314151617181920212223242526272829303132333435sequenceDiagram participant R0 as Rank0&lt;br/&gt;[A0,B0,C0,D0] participant R1 as Rank1&lt;br/&gt;[A1,B1,C1,D1] participant R2 as Rank2&lt;br/&gt;[A2,B2,C2,D2] participant R3 as Rank3&lt;br/&gt;[A3,B3,C3,D3] Note over R0,R3: Step 1: æ¯ä¸ªèŠ‚ç‚¹å‘å³å‘é€ä¸€ä¸ªå— R0-&gt;&gt;R1: D0 R1-&gt;&gt;R2: D1 R2-&gt;&gt;R3: D2 R3-&gt;&gt;R0: D3 Note over R0: æ¥æ”¶D3ï¼ŒæŒæœ‰[A0,B0,C0,D0+D3] Note over R1: æ¥æ”¶D0ï¼ŒæŒæœ‰[A1,B1,C1,D0+D1] Note over R2: æ¥æ”¶D1ï¼ŒæŒæœ‰[A2,B2,C2+D1,D2] Note over R3: æ¥æ”¶D2ï¼ŒæŒæœ‰[A3+D2,B3,C3,D3] Note over R0,R3: Step 2: ç»§ç»­å‘å³å‘é€åˆšè§„çº¦çš„å— R0-&gt;&gt;R1: C0 R1-&gt;&gt;R2: D0+D1 R2-&gt;&gt;R3: C2+D1 R3-&gt;&gt;R0: D2+D3 Note over R0: æ¥æ”¶D2+D3ï¼ŒæŒæœ‰[A0,B0,C0,D*] Note over R1: æ¥æ”¶C0ï¼ŒæŒæœ‰[A1,B1,C0+C1,D*] Note over R2: æ¥æ”¶D0+D1ï¼ŒæŒæœ‰[A2,B2,C*,D*] Note over R3: æ¥æ”¶C2+D1ï¼ŒæŒæœ‰[A*,B3,C*,D3] Note over R0,R3: Step 3: æœ€åä¸€è½® R0-&gt;&gt;R1: B0 R1-&gt;&gt;R2: C0+C1 R2-&gt;&gt;R3: B2 R3-&gt;&gt;R0: C* Note over R0: æ¥æ”¶C*ï¼ŒæŒæœ‰[A0,B0,C*,D*] Note over R1: æ¥æ”¶B0ï¼ŒæŒæœ‰[A1,B0+B1,C*,D*] Note over R2: æ¥æ”¶C0+C1ï¼ŒæŒæœ‰[A2,B*,C*,D2] Note over R3: æ¥æ”¶B2ï¼ŒæŒæœ‰[A3,B2+B3,C3,D*] æ³¨æ„ï¼š ä¸Šè¿°ç®€åŒ–ç¤ºä¾‹æœªå®Œæ•´å±•ç¤ºã€‚å®é™…ä¸ŠReduceScatteréœ€è¦p-1&#x3D;3æ­¥ï¼Œæ¯æ­¥æ¯ä¸ªèŠ‚ç‚¹éƒ½åœ¨æŸä¸ªç‰¹å®šä½ç½®è¿›è¡Œè§„çº¦ã€‚æœ€ç»ˆï¼š Rank0æŒæœ‰Aå—çš„å®Œæ•´è§„çº¦ç»“æœA* Rank1æŒæœ‰Bå—çš„å®Œæ•´è§„çº¦ç»“æœB* Rank2æŒæœ‰Cå—çš„å®Œæ•´è§„çº¦ç»“æœC* Rank3æŒæœ‰Då—çš„å®Œæ•´è§„çº¦ç»“æœD* é˜¶æ®µ2: AllGatherï¼ˆ3æ­¥å®Œæˆï¼‰ 12345678910111213141516171819202122232425262728293031sequenceDiagram participant R0 as Rank0 participant R1 as Rank1 participant R2 as Rank2 participant R3 as Rank3 Note over R0,R3: æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1/4è§„çº¦ç»“æœ Note over R0: [A*,B*,-,-] Note over R1: [-,B*,C*,-] Note over R2: [-,-,C*,D*] Note over R3: [A*,-,-,D*] Note over R0,R3: Step 1: æ²¿ç¯ä¼ è¾“æ”¶é›† R0-&gt;&gt;R1: A* R1-&gt;&gt;R2: B* R2-&gt;&gt;R3: C* R3-&gt;&gt;R0: D* Note over R0,R3: Step 2: ç»§ç»­ä¼ è¾“ R0-&gt;&gt;R1: D* R1-&gt;&gt;R2: A* R2-&gt;&gt;R3: B* R3-&gt;&gt;R0: C* Note over R0,R3: Step 3: æœ€åä¸€è½® R0-&gt;&gt;R1: C* R1-&gt;&gt;R2: D* R2-&gt;&gt;R3: A* R3-&gt;&gt;R0: B* Note over R0,R3: æ‰€æœ‰èŠ‚ç‚¹æŒæœ‰å®Œæ•´ç»“æœ[A*,B*,C*,D*] æ‰§è¡Œæµç¨‹è¯¦ç»†æè¿°ï¼š é˜¶æ®µ1: ReduceScatterï¼ˆp-1&#x3D;3æ­¥å®Œæˆï¼‰ Ringç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼šæ•°æ®åˆ‡åˆ†ä¸ºpå—ï¼Œæ¯ä¸ªèŠ‚ç‚¹åœ¨æ¯ä¸€æ­¥å‘å³é‚»å±…å‘é€ä¸€ä¸ªå—ï¼Œä»å·¦é‚»å±…æ¥æ”¶ä¸€ä¸ªå—å¹¶è§„çº¦ã€‚ç»è¿‡p-1æ­¥åï¼Œæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰ä¸€ä¸ªå®Œæ•´è§„çº¦çš„æ•°æ®å—ã€‚ åˆå§‹çŠ¶æ€: Rank0æŒæœ‰[A0, B0, C0, D0] Rank1æŒæœ‰[A1, B1, C1, D1] Rank2æŒæœ‰[A2, B2, C2, D2] Rank3æŒæœ‰[A3, B3, C3, D3] Step 1: å‘é€ï¼šR0â†’D0â†’R1, R1â†’D1â†’R2, R2â†’D2â†’R3, R3â†’D3â†’R0 è§„çº¦ï¼šæ¯ä¸ªèŠ‚ç‚¹å°†æ¥æ”¶çš„Då—ä¸æœ¬åœ°Då—è§„çº¦ ç»“æœï¼šR0æŒæœ‰D0+D3, R1æŒæœ‰D0+D1, R2æŒæœ‰D1+D2, R3æŒæœ‰D2+D3 æ•°æ®æµå‘ï¼šç¯å½¢é¡ºæ—¶é’ˆæµåŠ¨ Step 2: å‘é€ï¼šR0â†’C0â†’R1, R1â†’(D0+D1)â†’R2, R2â†’(C2+D1)â†’R3, R3â†’(D2+D3)â†’R0 è§„çº¦ï¼šæ¯ä¸ªèŠ‚ç‚¹å°†æ¥æ”¶çš„å—ä¸æœ¬åœ°å¯¹åº”å—è§„çº¦ å…³é”®ï¼šR2å®ŒæˆDå—çš„å®Œæ•´è§„çº¦D* &#x3D; D0+D1+D2+D3 æ•°æ®æµå‘ï¼šç»§ç»­é¡ºæ—¶é’ˆï¼Œè§„çº¦å—é€æ­¥å®Œæˆ Step 3: å‘é€ï¼šR0â†’B0â†’R1, R1â†’(C0+C1)â†’R2, R2â†’B2â†’R3, R3â†’C*â†’R0 è§„çº¦ï¼šR0å®ŒæˆD*çš„æ¥æ”¶ï¼ŒR1å’ŒR2å®Œæˆæ›´å¤šè§„çº¦ æ³¨ï¼šå®Œæ•´ç®—æ³•éœ€ç»§ç»­æ‰§è¡Œç›´åˆ°æ‰€æœ‰å—è§„çº¦å®Œæˆ æœ€ç»ˆçŠ¶æ€ï¼ˆç»è¿‡p-1æ­¥åï¼‰: Rank0æŒæœ‰A* &#x3D; A0+A1+A2+A3 Rank1æŒæœ‰B* &#x3D; B0+B1+B2+B3 Rank2æŒæœ‰C* &#x3D; C0+C1+C2+C3 Rank3æŒæœ‰D* &#x3D; D0+D1+D2+D3 é˜¶æ®µ2: AllGatherï¼ˆp-1&#x3D;3æ­¥å®Œæˆï¼‰ AllGatheré˜¶æ®µçš„ç›®æ ‡ï¼šå°†æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰çš„å”¯ä¸€è§„çº¦ç»“æœå—æ”¶é›†åˆ°æ‰€æœ‰èŠ‚ç‚¹ã€‚ åˆå§‹çŠ¶æ€: Rank0æŒæœ‰[A*, -, -, -]ï¼ˆå®é™…åœ¨ä½ç½®0ï¼‰ Rank1æŒæœ‰[-, B*, -, -]ï¼ˆå®é™…åœ¨ä½ç½®1ï¼‰ Rank2æŒæœ‰[-, -, C*, -]ï¼ˆå®é™…åœ¨ä½ç½®2ï¼‰ Rank3æŒæœ‰[-, -, -, D*]ï¼ˆå®é™…åœ¨ä½ç½®3ï¼‰ Step 1: å‘é€ï¼šR0â†’A*â†’R1, R1â†’B*â†’R2, R2â†’C*â†’R3, R3â†’D*â†’R0 æ¥æ”¶ï¼šæ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸€ä¸ªæ–°çš„è§„çº¦å— ç»“æœï¼šR0æŒæœ‰[A*,-,-,D*], R1æŒæœ‰[A*,B*,-,-], R2æŒæœ‰[-,B*,C*,-], R3æŒæœ‰[-,-,C*,D*] Step 2: å‘é€ï¼šR0â†’D*â†’R1, R1â†’A*â†’R2, R2â†’B*â†’R3, R3â†’C*â†’R0 æ¥æ”¶ï¼šæ¯ä¸ªèŠ‚ç‚¹å†æ¥æ”¶ä¸€ä¸ªè§„çº¦å— ç»“æœï¼šR0æŒæœ‰[A*,-,C*,D*], R1æŒæœ‰[A*,B*,D*,-], R2æŒæœ‰[A*,B*,C*,-], R3æŒæœ‰[-,B*,C*,D*] Step 3: å‘é€ï¼šR0â†’C*â†’R1, R1â†’D*â†’R2, R2â†’A*â†’R3, R3â†’B*â†’R0 æ¥æ”¶ï¼šæ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶æœ€åä¸€ä¸ªç¼ºå¤±çš„å— ç»“æœï¼šæ‰€æœ‰èŠ‚ç‚¹æŒæœ‰[A*, B*, C*, D*] é€šä¿¡ç‰¹ç‚¹: æ¯æ­¥åªä½¿ç”¨ç¯ä¸Šçš„å•å‘é“¾è·¯ æ•°æ®æ²¿ç¯é¡ºæ—¶é’ˆæµåŠ¨ æ¯æ­¥ä¼ è¾“çš„æ•°æ®é‡ä¸ºn&#x2F;på­—èŠ‚ æ— éœ€è§„çº¦æ“ä½œï¼Œçº¯æ•°æ®ä¼ è¾“ å…¶ä»–åŸè¯­æ‰§è¡Œæ–¹å¼ï¼š Broadcast: æ•°æ®ä¸åˆ‡åˆ†ï¼Œæ²¿ç¯ä¼ è¾“p-1æ­¥ï¼Œæ¯æ­¥ä¼ è¾“å®Œæ•´æ•°æ® Reduce: ç±»ä¼¼Broadcastï¼Œæ²¿ç¯ä¼ è¾“å¹¶è§„çº¦ï¼Œæœ€ç»ˆæ ¹èŠ‚ç‚¹å¾—åˆ°ç»“æœ AllGather: ä»…æ‰§è¡Œé˜¶æ®µ2ï¼Œæ•°æ®åˆ‡åˆ†åæ²¿ç¯æ”¶é›† ReduceScatter: ä»…æ‰§è¡Œé˜¶æ®µ1ï¼Œæ•°æ®åˆ‡åˆ†åæ²¿ç¯è§„çº¦ 3.3.3 æ€§èƒ½æ¨¡å‹ æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ Scatter $(p-1)\\alpha + \\frac{p-1}{p}n\\beta$ æ²¿ç¯ä¼ è¾“p-1æ­¥ï¼Œæ¯æ­¥ä¼ è¾“1&#x2F;pæ•°æ® Gather $(p-1)\\alpha + \\frac{p-1}{p}n\\beta$ æ²¿ç¯ä¼ è¾“p-1æ­¥ï¼Œæ¯æ­¥ä¼ è¾“1&#x2F;pæ•°æ® Broadcast $(p-1)\\alpha + (p-1)n\\beta$ æ²¿ç¯ä¼ è¾“p-1æ­¥ï¼Œæ¯æ­¥ä¼ è¾“å®Œæ•´æ•°æ® Reduce $(p-1)\\alpha + (p-1)n\\beta + (p-1)n\\gamma$ æ²¿ç¯ä¼ è¾“p-1æ­¥ï¼Œæ¯æ­¥ä¼ è¾“å®Œæ•´æ•°æ®å¹¶è§„çº¦ ReduceScatter $(p-1)\\alpha + \\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ æ•°æ®åˆ‡åˆ†ä¸ºpå—ï¼Œæ²¿ç¯ä¼ è¾“p-1æ­¥å¹¶è§„çº¦ AllGather $(p-1)\\alpha + \\frac{p-1}{p}n\\beta$ æ•°æ®åˆ‡åˆ†ä¸ºpå—ï¼Œæ²¿ç¯ä¼ è¾“p-1æ­¥æ”¶é›† AllReduce $2(p-1)\\alpha + 2\\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter(p-1æ­¥) + AllGather(p-1æ­¥) 3.4 RHD (Recursive Halving-Doubling) ç®—æ³•3.4.1 ç®—æ³•åŸç†é€’å½’äºŒåˆ†å’Œå€å¢ç®—æ³•ï¼Œé€šè¿‡å¯¹æ•°çº§çš„é€šä¿¡æ­¥æ•°å®ç°é«˜æ•ˆé€šä¿¡ã€‚ 123456789101112131415graph TB subgraph &quot;é2çš„å¹‚æ¬¡ï¼šp=5 (2Â²+1)&quot; A1[5ä¸ªRank] --&gt; A2[Rank1æ•°æ®åˆå¹¶åˆ°Rank0] A2 --&gt; A3[å˜ä¸º4ä¸ªæœ‰æ•ˆRank: 2Â²] A3 --&gt; A4[æ‰§è¡Œæ ‡å‡†RHD] A4 --&gt; A5[Rank0æ•°æ®å¤åˆ¶åˆ°Rank1] A5 --&gt; A6[å®Œæˆ] end subgraph &quot;2çš„å¹‚æ¬¡ï¼šp=4&quot; B1[4ä¸ªRank] --&gt; B2[ä¸¤ä¸¤äº¤æ¢å¹¶è§„çº¦: logâ‚‚4=2æ­¥] B2 --&gt; B3[ReduceScatterå®Œæˆ] B3 --&gt; B4[ä¸¤ä¸¤æ‹¼æ¥: 2æ­¥] B4 --&gt; B5[AllGatherå®Œæˆ] end ç‰¹ç‚¹ï¼š æ—¶é—´å¤æ‚åº¦ï¼š $O(\\lceil \\log_2 N \\rceil)$ - å¯¹æ•°å¤æ‚åº¦ é€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡é›†ç¾¤ï¼ˆServeræ•°é‡å¤šï¼‰ Serveræ•°é‡ä¸º2çš„å¹‚æ¬¡æ—¶æ€§èƒ½æœ€ä¼˜ ä¸­å°æ•°æ®é‡é€šä¿¡ 3.4.2 é€šä¿¡æ­¥éª¤ç¤ºä¾‹ï¼ˆp&#x3D;4ï¼Œä»¥AllReduceä¸ºä¾‹ï¼‰è¯´æ˜ï¼š RHDç®—æ³•æ”¯æŒå¤šç§é›†åˆé€šä¿¡åŸè¯­ï¼ˆAllReduceã€ReduceScatterã€AllGatherã€Broadcastã€Reduceç­‰ï¼‰ï¼Œæ­¤å¤„ä»¥AllReduceä¸ºå…¸å‹ç¤ºä¾‹å±•ç¤ºé€šä¿¡æ­¥éª¤ã€‚ ReduceScatteré˜¶æ®µï¼ˆRecursive Halvingï¼‰ï¼š 123456789101112131415161718192021222324252627282930sequenceDiagram participant R0 as Rank0&lt;br/&gt;[A0,B0,C0,D0] participant R1 as Rank1&lt;br/&gt;[A1,B1,C1,D1] participant R2 as Rank2&lt;br/&gt;[A2,B2,C2,D2] participant R3 as Rank3&lt;br/&gt;[A3,B3,C3,D3] Note over R0,R3: Step 1: Distance=1, XORæ“ä½œ(0âŠ•1=1, 2âŠ•3=1) Note over R0,R3: æ¯å¯¹èŠ‚ç‚¹äº¤æ¢ååŠéƒ¨åˆ†æ•°æ®å¹¶è§„çº¦ R0-&gt;&gt;R1: å‘é€[C0,D0], æ¥æ”¶[C1,D1] R1-&gt;&gt;R0: å‘é€[C1,D1], æ¥æ”¶[C0,D0] R2-&gt;&gt;R3: å‘é€[C2,D2], æ¥æ”¶[C3,D3] R3-&gt;&gt;R2: å‘é€[C3,D3], æ¥æ”¶[C2,D2] Note over R0: æŒæœ‰[A0,B0,C0+C1,D0+D1] Note over R1: æŒæœ‰[A1,B1,C0+C1,D0+D1] Note over R2: æŒæœ‰[A2,B2,C2+C3,D2+D3] Note over R3: æŒæœ‰[A3,B3,C2+C3,D2+D3] Note over R0,R3: Step 2: Distance=2, XORæ“ä½œ(0âŠ•2=2, 1âŠ•3=2) Note over R0,R3: äº¤æ¢ç°æœ‰ååŠéƒ¨åˆ†å¹¶è§„çº¦ R0-&gt;&gt;R2: å‘é€[C0+C1,D0+D1], æ¥æ”¶[C2+C3,D2+D3] R2-&gt;&gt;R0: å‘é€[C2+C3,D2+D3], æ¥æ”¶[C0+C1,D0+D1] R1-&gt;&gt;R3: å‘é€[C0+C1,D0+D1], æ¥æ”¶[C2+C3,D2+D3] R3-&gt;&gt;R1: å‘é€[C2+C3,D2+D3], æ¥æ”¶[C0+C1,D0+D1] Note over R0: æŒæœ‰[A0,B0,C*,D*] (C*=C0+C1+C2+C3) Note over R1: æŒæœ‰[A1,B1,C*,D*] Note over R2: æŒæœ‰[A2,B2,C*,D*] Note over R3: æŒæœ‰[A3,B3,C*,D*] Note over R0,R3: ç»§ç»­å¤„ç†å‰åŠéƒ¨åˆ†ï¼ˆæ­¤å¤„çœç•¥ï¼‰ Note over R0,R3: æœ€ç»ˆæ¯ä¸ªRankæŒæœ‰ä¸åŒçš„å®Œæ•´è§„çº¦å— AllGatheré˜¶æ®µï¼ˆRecursive Doublingï¼‰ï¼š 12345678910111213141516171819202122232425262728sequenceDiagram participant R0 as Rank0 participant R1 as Rank1 participant R2 as Rank2 participant R3 as Rank3 Note over R0,R3: å‡è®¾ReduceScatterå: Note over R0: [A*,-,-,-] Note over R1: [-,B*,-,-] Note over R2: [-,-,C*,-] Note over R3: [-,-,-,D*] Note over R0,R3: Step 1: Distance=2, äº¤æ¢æ•°æ® R0-&gt;&gt;R2: å‘é€A*, æ¥æ”¶C* R2-&gt;&gt;R0: å‘é€C*, æ¥æ”¶A* R1-&gt;&gt;R3: å‘é€B*, æ¥æ”¶D* R3-&gt;&gt;R1: å‘é€D*, æ¥æ”¶B* Note over R0: [A*,-,C*,-] Note over R1: [-,B*,-,D*] Note over R2: [A*,-,C*,-] Note over R3: [-,B*,-,D*] Note over R0,R3: Step 2: Distance=1, äº¤æ¢æ•°æ® R0-&gt;&gt;R1: å‘é€[A*,C*], æ¥æ”¶[B*,D*] R1-&gt;&gt;R0: å‘é€[B*,D*], æ¥æ”¶[A*,C*] R2-&gt;&gt;R3: å‘é€[A*,C*], æ¥æ”¶[B*,D*] R3-&gt;&gt;R2: å‘é€[B*,D*], æ¥æ”¶[A*,C*] Note over R0,R3: æ‰€æœ‰èŠ‚ç‚¹æŒæœ‰[A*,B*,C*,D*] è¯¦ç»†æ‰§è¡Œæµç¨‹è¯´æ˜ï¼š ReduceScatteré˜¶æ®µè¯¦è§£ï¼ˆRecursive Halvingï¼‰ï¼š è¯¥é˜¶æ®µæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é€’å½’å¯¹åŠåˆ†ç»„ï¼ˆDistance Halvingï¼‰å°†æ•°æ®åˆ†æ•£è§„çº¦åˆ°å„ä¸ªèŠ‚ç‚¹ã€‚è·ç¦»distanceé‡‡ç”¨æŒ‡æ•°é€’å‡æ¨¡å¼ï¼šp&#x2F;2 â†’ p&#x2F;4 â†’ â€¦ â†’ 1ã€‚ åˆå§‹çŠ¶æ€ï¼š æ¯ä¸ªèŠ‚ç‚¹æŒæœ‰å®Œæ•´æ•°æ®[A,B,C,D]ï¼Œéœ€è¦å¯¹4ä¸ªå—åˆ†åˆ«è§„çº¦ï¼Œæœ€ç»ˆæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰å…¶ä¸­ä¸€ä¸ªå®Œæ•´è§„çº¦å—ã€‚ Step 1 (Distance&#x3D;2)ï¼š èŠ‚ç‚¹åˆ†ä¸ºä¸¤ç»„ï¼š{R0,R1} â†” {R2,R3}ã€‚é€šè¿‡XORæ‰¾åˆ°é€šä¿¡å¯¹ï¼ˆRank XOR 2ï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£å¤„ç†ä¸€åŠæ•°æ®ã€‚ R0 â†” R2é€šä¿¡ï¼šR0å‘é€ååŠéƒ¨åˆ†[C,D]å¹¶æ¥æ”¶åŒæ ·ä½ç½®çš„[C,D]ï¼Œå¯¹æ¥æ”¶æ•°æ®æ‰§è¡Œè§„çº¦æ“ä½œï¼ˆCâ†C+C, Dâ†D+Dï¼‰ï¼ŒåŒæ—¶æ¥æ”¶R2çš„å‰åŠéƒ¨åˆ†[A,B]å¹¶è§„çº¦åˆ°æœ¬åœ°å‰åŠéƒ¨åˆ† R1 â†” R3é€šä¿¡ï¼šåŒæ ·çš„äº¤æ¢å’Œè§„çº¦æ“ä½œ å¤„ç†åçŠ¶æ€ï¼š R0æŒæœ‰[A0+A2, B0+B2, C0+C2, D0+D2]ï¼Œä½†ä»…è´Ÿè´£å‰åŠéƒ¨åˆ†ï¼ˆååŠéƒ¨åˆ†å°†è¢«ä¸¢å¼ƒï¼‰ R1æŒæœ‰[A1+A3, B1+B3, C1+C3, D1+D3]ï¼Œä»…è´Ÿè´£å‰åŠéƒ¨åˆ† R2æŒæœ‰[A0+A2, B0+B2, C0+C2, D0+D2]ï¼Œä»…è´Ÿè´£ååŠéƒ¨åˆ† R3æŒæœ‰[A1+A3, B1+B3, C1+C3, D1+D3]ï¼Œä»…è´Ÿè´£ååŠéƒ¨åˆ† æ•°æ®ç¼©å‡ï¼šæ¯ä¸ªèŠ‚ç‚¹ä»4ä¸ªå—ç¼©å‡ä¸º2ä¸ªæœ‰æ•ˆå— Step 2 (Distance&#x3D;1)ï¼š åœ¨ä¸Šä¸€æ­¥çš„åŸºç¡€ä¸Šç»§ç»­å¯¹åŠåˆ†ç»„ã€‚R0â†”R1å¤„ç†å‰åŠéƒ¨åˆ†{A,B}ï¼ŒR2â†”R3å¤„ç†ååŠéƒ¨åˆ†{C,D}ã€‚ R0 â†” R1é€šä¿¡ï¼šäº¤æ¢å’Œè§„çº¦Bå—ï¼ˆR0æœ€ç»ˆæŒæœ‰å®Œæ•´è§„çº¦çš„Aå—ï¼ŒR1æŒæœ‰Bå—ï¼‰ R2 â†” R3é€šä¿¡ï¼šäº¤æ¢å’Œè§„çº¦Då—ï¼ˆR2æœ€ç»ˆæŒæœ‰å®Œæ•´è§„çº¦çš„Cå—ï¼ŒR3æŒæœ‰Då—ï¼‰ å¤„ç†åçŠ¶æ€ï¼š R0: [A0+A1+A2+A3, -, -, -]ï¼ˆç®€å†™ä¸º[A*, -, -, -]ï¼‰ R1: [-, B0+B1+B2+B3, -, -]ï¼ˆç®€å†™ä¸º[-, B*, -, -]ï¼‰ R2: [-, -, C0+C1+C2+C3, -]ï¼ˆç®€å†™ä¸º[-, -, C*, -]ï¼‰ R3: [-, -, -, D0+D1+D2+D3]ï¼ˆç®€å†™ä¸º[-, -, -, D*]ï¼‰ æ•°æ®ç¼©å‡ï¼šæ¯ä¸ªèŠ‚ç‚¹ä»2ä¸ªå—ç¼©å‡ä¸º1ä¸ªå®Œæ•´è§„çº¦å— ReduceScatterç‰¹ç‚¹ï¼šlogâ‚‚pæ­¥é€’å½’å¯¹åŠï¼Œæ¯æ­¥æ•°æ®é‡å‡åŠï¼Œæ‰€æœ‰èŠ‚ç‚¹åŒæ—¶å·¥ä½œï¼Œé€šä¿¡å’Œè®¡ç®—é«˜åº¦é‡å ã€‚ AllGatheré˜¶æ®µè¯¦è§£ï¼ˆRecursive Doublingï¼‰ï¼š è¯¥é˜¶æ®µæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é€’å½’åŠ å€è·ç¦»ï¼ˆDistance Doublingï¼‰æ”¶é›†æ‰€æœ‰è§„çº¦ç»“æœã€‚è·ç¦»distanceé‡‡ç”¨æŒ‡æ•°é€’å¢æ¨¡å¼ï¼š1 â†’ 2 â†’ 4 â†’ â€¦ã€‚ åˆå§‹çŠ¶æ€ï¼š ReduceScatterå®Œæˆåï¼Œæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰ä¸€ä¸ªå®Œæ•´è§„çº¦å—ï¼šR0æŒæœ‰[A*,-,-,-]ï¼ŒR1æŒæœ‰[-,B*,-,-]ï¼ŒR2æŒæœ‰[-,-,C*,-]ï¼ŒR3æŒæœ‰[-,-,-,D*]ã€‚ Step 1 (Distance&#x3D;2)ï¼š é¦–å…ˆåœ¨è·ç¦»ä¸º2çš„èŠ‚ç‚¹é—´äº¤æ¢æ•°æ®ã€‚é€šè¿‡XORæ‰¾åˆ°é€šä¿¡å¯¹ï¼ˆRank XOR 2ï¼‰ã€‚ R0 â†” R2é€šä¿¡ï¼šR0å‘é€Aå¹¶æ¥æ”¶Cï¼ŒR2å‘é€Cå¹¶æ¥æ”¶A R1 â†” R3é€šä¿¡ï¼šR1å‘é€Bå¹¶æ¥æ”¶Dï¼ŒR3å‘é€Då¹¶æ¥æ”¶B é€šä¿¡åçŠ¶æ€ï¼š R0: [A*, -, C*, -]ï¼ˆæŒæœ‰ç¬¬0å’Œç¬¬2å—çš„å®Œæ•´è§„çº¦ç»“æœï¼‰ R1: [-, B*, -, D*]ï¼ˆæŒæœ‰ç¬¬1å’Œç¬¬3å—ï¼‰ R2: [A*, -, C*, -]ï¼ˆæŒæœ‰ç¬¬0å’Œç¬¬2å—ï¼‰ R3: [-, B*, -, D*]ï¼ˆæŒæœ‰ç¬¬1å’Œç¬¬3å—ï¼‰ æ•°æ®å¢é•¿ï¼šæ¯ä¸ªèŠ‚ç‚¹ä»1ä¸ªå—å¢é•¿ä¸º2ä¸ªå— Step 2 (Distance&#x3D;1)ï¼š åœ¨è·ç¦»ä¸º1çš„ç›¸é‚»èŠ‚ç‚¹é—´äº¤æ¢æ•°æ®ï¼Œå®Œæˆæœ€ç»ˆæ”¶é›†ã€‚ R0 â†” R1é€šä¿¡ï¼šR0å‘é€[A*,C*]å¹¶æ¥æ”¶[B*,D*]ï¼ŒR1å‘é€[B*,D*]å¹¶æ¥æ”¶[A*,C*] R2 â†” R3é€šä¿¡ï¼šR2å‘é€[A*,C*]å¹¶æ¥æ”¶[B*,D*]ï¼ŒR3å‘é€[B*,D*]å¹¶æ¥æ”¶[A*,C*] æœ€ç»ˆçŠ¶æ€ï¼šæ‰€æœ‰èŠ‚ç‚¹æŒæœ‰[A*, B*, C*, D*]ï¼Œå³æ‰€æœ‰æ•°æ®çš„å®Œæ•´è§„çº¦ç»“æœ AllGatherç‰¹ç‚¹ï¼šlogâ‚‚pæ­¥é€’å½’åŠ å€ï¼Œæ¯æ­¥ä¼ è¾“æ•°æ®é‡å€å¢ï¼ˆ1å—â†’2å—â†’4å—ï¼‰ï¼Œæ— éœ€è®¡ç®—æ“ä½œï¼Œçº¯ç²¹çš„æ•°æ®æ”¶é›†ã€‚ RHDç®—æ³•æ•´ä½“ç‰¹ç‚¹ï¼š å¯¹æ•°çº§å¤æ‚åº¦ï¼šæ€»å…±2logâ‚‚pæ­¥é€šä¿¡ï¼ˆReduceScatter logâ‚‚pæ­¥ + AllGather logâ‚‚pæ­¥ï¼‰ XORé€šä¿¡æ¨¡å¼ï¼šæ¯æ­¥é€šè¿‡Rank XOR Distanceç¡®å®šé€šä¿¡å¯¹ï¼Œä¿è¯æ— å†²çªå¹¶è¡Œ æ•°æ®é‡å˜åŒ–ï¼šReduceScatteré€’å‡ï¼ˆn â†’ n&#x2F;2 â†’ n&#x2F;4 â†’ â€¦ï¼‰ï¼ŒAllGatheré€’å¢ï¼ˆn&#x2F;p â†’ 2n&#x2F;p â†’ 4n&#x2F;p â†’ â€¦ï¼‰ æœ€ä¼˜æ€§ï¼šå¯¹äº2çš„å¹‚æ¬¡èŠ‚ç‚¹æ•°ï¼Œæ˜¯ç†è®ºæœ€ä¼˜ç®—æ³•ï¼ˆæœ€å°‘é€šä¿¡æ­¥æ•°ï¼‰ é™åˆ¶æ¡ä»¶ï¼šä»…é€‚ç”¨äºèŠ‚ç‚¹æ•°ä¸º2çš„å¹‚æ¬¡ï¼ˆé2å¹‚æ¬¡éœ€è¦é¢å¤–æ­¥éª¤å¤„ç†ï¼‰ å…¶ä»–åŸè¯­æ‰§è¡Œæ–¹å¼ï¼š Broadcast: é‡‡ç”¨Distance Halvingç­–ç•¥ï¼Œä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’æ‰©æ•£ ReduceScatter: ä»…æ‰§è¡ŒReduceScatteré˜¶æ®µï¼ˆRecursive Halvingï¼‰ AllGather: ä»…æ‰§è¡ŒAllGatheré˜¶æ®µï¼ˆRecursive Doublingï¼‰ 3.4.3 æ€§èƒ½æ¨¡å‹2çš„å¹‚æ¬¡ï¼ˆp &#x3D; 2^kï¼‰ï¼š æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ Broadcast $\\lceil \\log_2 p \\rceil\\alpha + \\lceil \\log_2 p \\rceil n\\beta$ Distance Halvingç­–ç•¥ï¼Œlogâ‚‚pæ­¥ï¼Œæ¯æ­¥ä¼ è¾“å®Œæ•´nå­—èŠ‚æ•°æ® ReduceScatter $\\log_2 p \\cdot \\alpha + \\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ Vector Doubling + Distance Halvingï¼Œlogâ‚‚pæ­¥é€’å½’å¯¹åŠäº¤æ¢ AllGather $\\log_2 p \\cdot \\alpha + \\frac{p-1}{p}n\\beta$ Distance Doublingç­–ç•¥ï¼Œlogâ‚‚pæ­¥ï¼Œæ¯æ­¥æ•°æ®é‡å€å¢ AllReduce $2\\log_2 p \\cdot \\alpha + 2\\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter(logâ‚‚pæ­¥) + AllGather(logâ‚‚pæ­¥)ï¼Œè§„çº¦ä¸€æ¬¡ é2çš„å¹‚æ¬¡ï¼ˆéœ€è¦é¢å¤–æ­¥éª¤ï¼‰ï¼š $$\\text{AllReduce} &#x3D; (2\\lfloor \\log_2 p \\rfloor + 2)\\alpha + (2\\frac{pâ€™-1}{pâ€™} + 2)n\\beta + (\\frac{pâ€™-1}{pâ€™} + 1)n\\gamma$$ å…¶ä¸­ $pâ€™ &#x3D; 2^{\\lfloor \\log_2 p \\rfloor}$ 3.5 PairWise ç®—æ³•3.5.1 ç®—æ³•åŸç†123456789101112131415161718192021graph TB subgraph &quot;Step 1&quot; R0_1[Rank0] --&gt;|å‘é€| R1_1[Rank1] R1_1 --&gt;|å‘é€| R0_1 R2_1[Rank2] --&gt;|å‘é€| R3_1[Rank3] R3_1 --&gt;|å‘é€| R2_1 end subgraph &quot;Step 2&quot; R0_2[Rank0] --&gt;|å‘é€| R2_2[Rank2] R2_2 --&gt;|å‘é€| R0_2 R1_2[Rank1] --&gt;|å‘é€| R3_2[Rank3] R3_2 --&gt;|å‘é€| R1_2 end subgraph &quot;Step 3&quot; R0_3[Rank0] --&gt;|å‘é€| R3_3[Rank3] R3_3 --&gt;|å‘é€| R0_3 R1_3[Rank1] --&gt;|å‘é€| R2_3[Rank2] R2_3 --&gt;|å‘é€| R1_3 end ç®—æ³•åŸç†è¯´æ˜ï¼š PairWiseç®—æ³•æ˜¯ä¸“ä¸ºAllToAll&#x2F;AllToAllVç®—å­è®¾è®¡çš„é«˜æ•ˆé€šä¿¡ç­–ç•¥ï¼Œæ ¸å¿ƒç›®æ ‡æ˜¯é¿å…ç½‘ç»œç«¯å£æ‹¥å¡ã€‚åœ¨AllToAllæ“ä½œä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹éœ€è¦å‘å…¶ä»–æ‰€æœ‰èŠ‚ç‚¹å‘é€æ•°æ®ï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½å¯¼è‡´æŸä¸ªèŠ‚ç‚¹åŒæ—¶å‘å¤šä¸ªç›®æ ‡å‘é€ï¼ˆâ€ä¸€æ‰“å¤šâ€ç°è±¡ï¼‰ï¼Œé€ æˆç«¯å£å¸¦å®½ç«äº‰å’Œæ‹¥å¡ã€‚ æ ¸å¿ƒæœºåˆ¶ï¼šé…å¯¹äº¤æ¢ï¼ˆPairwise Exchangeï¼‰ PairWiseé€šè¿‡å°†pä¸ªèŠ‚ç‚¹çš„é€šä¿¡ç»„ç»‡ä¸º (p-1)è½®é…å¯¹äº¤æ¢ï¼Œæ¯è½®ç¡®ä¿ï¼š æ— å†²çªå¹¶è¡Œï¼šæ¯ä¸ªèŠ‚ç‚¹åœ¨æ¯è½®åªä¸ä¸€ä¸ªå¯¹ç«¯é€šä¿¡ ç«¯å£ç‹¬å ï¼šé¿å…å¤šæµç«äº‰å•ä¸ªç½‘ç»œç«¯å£ å¾ªç¯é…å¯¹ï¼šé€šè¿‡å›ºå®šçš„é…å¯¹æ¨¡å¼è¦†ç›–æ‰€æœ‰èŠ‚ç‚¹å¯¹ é…å¯¹æ¨¡å¼ï¼ˆä»¥4èŠ‚ç‚¹ä¸ºä¾‹ï¼‰ï¼š Step 1ï¼š{R0 â†” R1}, {R2 â†” R3} Rank0ä¸Rank1åŒå‘äº¤æ¢æ•°æ® Rank2ä¸Rank3åŒå‘äº¤æ¢æ•°æ® ä¸¤å¯¹é€šä¿¡å®Œå…¨å¹¶è¡Œï¼Œæ— å¹²æ‰° Step 2ï¼š{R0 â†” R2}, {R1 â†” R3} é…å¯¹æ–¹å¼æ”¹å˜ï¼ŒRank0ä¸Rank2äº¤æ¢ Rank1ä¸Rank3äº¤æ¢ ä¾ç„¶ä¿æŒä¸¤ä¸¤é…å¯¹ï¼Œæ— å†²çª Step 3ï¼š{R0 â†” R3}, {R1 â†” R2} æœ€åä¸€è½®é…å¯¹ï¼ŒRank0ä¸Rank3äº¤æ¢ Rank1ä¸Rank2äº¤æ¢ å®Œæˆæ‰€æœ‰èŠ‚ç‚¹å¯¹çš„é€šä¿¡ æ•°å­¦è§„å¾‹ï¼š å¯¹äºpä¸ªèŠ‚ç‚¹ï¼Œéœ€è¦ (p-1)è½® å®Œæˆæ‰€æœ‰èŠ‚ç‚¹å¯¹çš„äº¤æ¢ ç¬¬kè½®ï¼ˆk&#x3D;1,2,â€¦,p-1ï¼‰ï¼ŒèŠ‚ç‚¹iä¸èŠ‚ç‚¹(i+k) mod pé€šä¿¡ æ¯è½®p&#x2F;2å¯¹èŠ‚ç‚¹åŒæ—¶é€šä¿¡ï¼ˆpä¸ºå¶æ•°ï¼‰ ä¼˜åŠ¿åˆ†æï¼š é¿å…ç«¯å£æ‹¥å¡ï¼šä¼ ç»ŸAllToAllå¯èƒ½æŸèŠ‚ç‚¹åŒæ—¶å‘å¤šä¸ªç›®æ ‡å‘é€ï¼Œå¯¼è‡´å•ç«¯å£å¤šæµç«äº‰ï¼›PairWiseç¡®ä¿æ¯èŠ‚ç‚¹æ¯è½®åªæœ‰ä¸€æ¡è¿æ¥ RDMAå‹å¥½ï¼šRDMAç¯å¢ƒä¸‹ï¼Œç‚¹å¯¹ç‚¹ç‹¬å é€šä¿¡å¯è·å¾—æœ€ä½³æ€§èƒ½ å¤§æ•°æ®é‡ä¼˜åŒ–ï¼šæ•°æ®é‡å¤§æ—¶ï¼Œç«¯å£æ‹¥å¡å½±å“æ˜¾è‘—ï¼ŒPairWiseä¼˜åŠ¿æ˜æ˜¾ å¯é¢„æµ‹æ€§èƒ½ï¼šé€šä¿¡æ¨¡å¼å›ºå®šï¼Œå»¶è¿Ÿå’Œå¸¦å®½ä½¿ç”¨å¯ç²¾ç¡®é¢„æµ‹ å±€é™æ€§ï¼š æ­¥æ•°å¤šï¼šéœ€è¦p-1æ­¥ï¼Œç›¸æ¯”RHDçš„logâ‚‚pæ­¥è¦å¤šï¼ˆä½†æ¯æ­¥æ›´é«˜æ•ˆï¼‰ å°æ•°æ®ä¸é€‚ç”¨ï¼šå°æ•°æ®é‡æ—¶ï¼Œå¯åŠ¨å¼€é”€(Î±)å ä¸»å¯¼ï¼Œå¤šæ­¥æ•°åŠ£åŠ¿æ˜æ˜¾ ä¸“ç”¨ç®—å­ï¼šä¸»è¦ç”¨äºAllToAllï¼Œä¸é€‚ç”¨äºAllReduceç­‰éœ€è¦è§„çº¦çš„åœºæ™¯ ç‰¹ç‚¹ï¼š ä¸“ç”¨åœºæ™¯ï¼š AllToAllã€AllToAllVç®—å­ æ—¶é—´å¤æ‚åº¦ï¼š O(p-1) - çº¿æ€§å¤æ‚åº¦ å…³é”®ä¼˜åŠ¿ï¼š é¿å…â€ä¸€æ‰“å¤šâ€ç°è±¡ï¼ˆå•ç«¯å£å¤šæµç«äº‰ï¼‰ é€‚ç”¨åœºæ™¯ï¼š å¤§æ•°æ®é‡é€šä¿¡ RDMAç½‘ç»œç¯å¢ƒ éœ€é¿å…ç«¯å£æ‹¥å¡çš„åœºæ™¯ 3.5.2 æ€§èƒ½æ¨¡å‹å®šä¹‰ $n_{ij}$ ä¸ºèŠ‚ç‚¹iå‘é€ç»™èŠ‚ç‚¹jçš„æ•°æ®é‡ï¼š $$T &#x3D; (p-1)\\alpha + \\beta \\sum_{k&#x3D;1}^{p-1} \\max_{i}(n_{i,i+k})$$ 3.6 Star ç®—æ³•3.6.1 ç®—æ³•åŸç†12345678910111213graph TD Root((Root&lt;br/&gt;æ ¹èŠ‚ç‚¹)) Root ---|ä¸€æ­¥å®Œæˆ| R1((Rank1)) Root ---|ä¸€æ­¥å®Œæˆ| R2((Rank2)) Root ---|ä¸€æ­¥å®Œæˆ| R3((Rank3)) Root ---|ä¸€æ­¥å®Œæˆ| R4((Rank4)) style Root fill:#ff6b6b style R1 fill:#4ecdc4 style R2 fill:#4ecdc4 style R3 fill:#4ecdc4 style R4 fill:#4ecdc4 ç®—æ³•åŸç†è¯´æ˜ï¼š Starç®—æ³•æ˜¯æœ€ç®€å•ç›´æ¥çš„é›†åˆé€šä¿¡ç®—æ³•ï¼Œé‡‡ç”¨æ˜Ÿå‹æ‹“æ‰‘ç»“æ„ï¼Œæ‰€æœ‰é€šä¿¡éƒ½é€šè¿‡ä¸­å¿ƒæ ¹èŠ‚ç‚¹ï¼ˆRootï¼‰è¿›è¡Œã€‚é€‚ç”¨äºServerå†…é«˜å¸¦å®½å…¨è¿æ¥æˆ–æœ‰æ˜ç¡®æ ¹èŠ‚ç‚¹çš„é€šä¿¡åœºæ™¯ã€‚ æ ¸å¿ƒç‰¹ç‚¹ï¼šå•æ­¥å®Œæˆï¼ˆO(1)å¤æ‚åº¦ï¼‰ Starç®—æ³•çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä¸€æ­¥åˆ°ä½ï¼šæ ¹èŠ‚ç‚¹ç›´æ¥ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹é€šä¿¡ï¼Œæ— éœ€å¤šè·³è½¬å‘ã€‚è¿™åœ¨é«˜å¸¦å®½ã€ä½å»¶è¿Ÿçš„ç½‘ç»œç¯å¢ƒä¸‹æ€§èƒ½æœ€ä¼˜ã€‚ å…¸å‹åº”ç”¨åœºæ™¯ï¼š Broadcastï¼ˆå¹¿æ’­ï¼‰ï¼š RootèŠ‚ç‚¹æŒæœ‰æ•°æ®ï¼Œéœ€è¦å‘é€ç»™æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ æ‰§è¡Œï¼šRoot â†’ {R1, R2, R3, R4}åŒæ—¶å‘é€ ä¸€æ­¥å®Œæˆï¼Œè€—æ—¶ &#x3D; Î± + nÎ²ï¼ˆä¸€æ¬¡å»¶è¿Ÿ + ä¼ è¾“æ—¶é—´ï¼‰ Reduceï¼ˆè§„çº¦åˆ°æ ¹èŠ‚ç‚¹ï¼‰ï¼š æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®éœ€è¦è§„çº¦åˆ°Root æ‰§è¡Œï¼š{R1, R2, R3, R4} â†’ RootåŒæ—¶å‘é€å¹¶åœ¨Rootè§„çº¦ ä¸€æ­¥å®Œæˆï¼Œè€—æ—¶ &#x3D; Î± + nÎ² + nÎ³ï¼ˆå»¶è¿Ÿ + ä¼ è¾“ + è§„çº¦ï¼‰ Gatherï¼ˆæ”¶é›†åˆ°æ ¹èŠ‚ç‚¹ï¼‰ï¼š æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®æ”¶é›†åˆ°Rootï¼ˆæ— éœ€è§„çº¦ï¼‰ æ‰§è¡Œï¼š{R1, R2, R3, R4} â†’ RootåŒæ—¶å‘é€ ä¸€æ­¥å®Œæˆï¼Œè€—æ—¶ &#x3D; Î± + nÎ² Scatterï¼ˆåˆ†å‘ï¼‰ï¼š Rootå°†ä¸åŒæ•°æ®å—åˆ†å‘ç»™ä¸åŒèŠ‚ç‚¹ æ‰§è¡Œï¼šRoot â†’ {R1, R2, R3, R4}åŒæ—¶å‘é€ä¸åŒå— ä¸€æ­¥å®Œæˆï¼Œè€—æ—¶ &#x3D; Î± + (n&#x2F;p)Î²ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶n&#x2F;på¤§å°æ•°æ®ï¼‰ ç½‘ç»œæ‹“æ‰‘è¦æ±‚ï¼š ç‰©ç†å…¨è¿æ¥ï¼šRootä¸æ‰€æœ‰èŠ‚ç‚¹ç›´æ¥ç›¸è¿ï¼ˆå¦‚Serverå†…NVLink&#x2F;PCIeå…¨è¿æ¥ï¼‰ é«˜å¸¦å®½é“¾è·¯ï¼šé“¾è·¯å¸¦å®½è¶³å¤Ÿé«˜ï¼Œå¯æ”¯æŒRootåŒæ—¶å¤šæµå‘é€è€Œä¸æ‹¥å¡ ä½å»¶è¿Ÿï¼šå•è·³å»¶è¿Ÿå°ï¼Œä¸€æ­¥é€šä¿¡å¼€é”€å¯æ¥å— ä¼˜åŠ¿ï¼š æœ€ä¼˜æ—¶é—´å¤æ‚åº¦ï¼šO(1)ï¼Œç†è®ºä¸Šæœ€å¿« é€»è¾‘ç®€å•ï¼šæ— éœ€å¤æ‚è°ƒåº¦å’ŒåŒæ­¥ Serverå†…æœ€ä¼˜ï¼šServerå†…NPUé€šè¿‡NVLinkå…¨è¿æ¥ï¼ŒStaræ˜¯é¦–é€‰ å±€é™æ€§ï¼š æ ¹èŠ‚ç‚¹ç“¶é¢ˆï¼šRootéœ€è¦åŒæ—¶ä¸p-1ä¸ªèŠ‚ç‚¹é€šä¿¡ï¼Œå¸¦å®½å‹åŠ›å¤§ å‘é€å¸¦å®½ï¼šRootéœ€å‘é€(p-1)Ã—næ•°æ®ï¼ˆBroadcaståœºæ™¯ï¼‰ æ¥æ”¶å¸¦å®½ï¼šRootéœ€æ¥æ”¶(p-1)Ã—næ•°æ®ï¼ˆReduceåœºæ™¯ï¼‰ ä¸é€‚åˆServeré—´ï¼šè·¨Serveré€šä¿¡å¸¦å®½æœ‰é™ï¼ŒRootç“¶é¢ˆä¸¥é‡ æ— è´Ÿè½½å‡è¡¡ï¼šæ‰€æœ‰æµé‡é›†ä¸­åœ¨Rootï¼Œå…¶ä»–èŠ‚ç‚¹é“¾è·¯åˆ©ç”¨ç‡ä½ ä¸æ”¯æŒAllToAllï¼šAllToAlléœ€è¦æ‰€æœ‰èŠ‚ç‚¹å¯¹é€šä¿¡ï¼ŒStaræ— æ³•é«˜æ•ˆå®ç° é€‚ç”¨åŸè¯­é™åˆ¶ï¼š âœ… é€‚ç”¨ï¼šBroadcastã€Reduceã€Gatherã€Scatterï¼ˆæœ‰æ˜ç¡®æ ¹èŠ‚ç‚¹çš„å•å‘é€šä¿¡ï¼‰âŒ ä¸é€‚ç”¨ï¼šAllReduceã€AllGatherã€ReduceScatterã€AllToAllï¼ˆéœ€è¦æ‰€æœ‰èŠ‚ç‚¹é—´é€šä¿¡ï¼‰ ç‰¹ç‚¹ï¼š æ‹“æ‰‘ï¼š æ˜Ÿå‹æˆ–å…¨è¿æ¥ æ—¶é—´å¤æ‚åº¦ï¼š O(1) - å•æ­¥å®Œæˆ é€‚ç”¨ç®—å­ï¼š Broadcastã€Reduceã€Gatherã€Scatter é€‚ç”¨åœºæ™¯ï¼š Serverå†…é€šä¿¡ï¼Œæœ‰æ ¹èŠ‚ç‚¹çš„æ“ä½œ 3.6.2 æ€§èƒ½æ¨¡å‹$$T &#x3D; \\alpha + n\\beta$$ éå¸¸ç®€æ´ï¼Œä»…ä¸€æ­¥é€šä¿¡å®Œæˆã€‚ 3.7 NHR (Nonuniform Hierarchical Ring) ç®—æ³•3.7.1 ç®—æ³•åŸç†éå‡è¡¡çš„å±‚æ¬¡ç¯ç®—æ³•ï¼Œé€šè¿‡æ„å»ºNæ£µç”Ÿæˆæ ‘å®ç°é«˜æ•ˆé€šä¿¡ã€‚ 12345678910111213141516graph TB subgraph &quot;Rank Size = 4 (2çš„å¹‚æ¬¡)&quot; A1[åˆå§‹çŠ¶æ€] --&gt; A2[Step 1: äº¤æ¢1/2æ•°æ®] A2 --&gt; A3[Step 2: äº¤æ¢1/4æ•°æ®] A3 --&gt; A4[å®Œæˆ] end subgraph &quot;Rank Size = 5 (é2çš„å¹‚æ¬¡)&quot; B1[åˆå§‹çŠ¶æ€] --&gt; B2[Step 1: ä¸å‡åŒ€åˆ‡ç‰‡] B2 --&gt; B3[Step 2: å¤§éƒ¨åˆ†è¿ç»­æ”¶å‘] B3 --&gt; B4[Step 3: å°‘é‡ç¦»æ•£å¤„ç†] B4 --&gt; B5[å®Œæˆ] end Note1[æ ‘æ·±åº¦: âŒˆlogâ‚‚NâŒ‰] Note2[ä¼˜åŒ–: èšåˆå‘é€&lt;br/&gt;å‡å°‘ç½‘ç»œåŒ…æ•°] ç®—æ³•åŸç†è¯´æ˜ï¼š NHRï¼ˆNonuniform Hierarchical Ringï¼‰ç®—æ³•æ˜¯éå‡è¡¡çš„å±‚æ¬¡åŒ–ç¯ç®—æ³•ï¼Œä¸“é—¨è§£å†³èŠ‚ç‚¹æ•°ä¸æ˜¯2çš„å¹‚æ¬¡æ—¶çš„é«˜æ•ˆé€šä¿¡é—®é¢˜ã€‚ä¼ ç»ŸRHDç®—æ³•è¦æ±‚èŠ‚ç‚¹æ•°ä¸º2çš„å¹‚æ¬¡ï¼Œå¦åˆ™æ€§èƒ½å¤§å¹…ä¸‹é™ï¼›NHRé€šè¿‡Næ£µç”Ÿæˆæ ‘å’Œä¸å‡åŒ€åˆ‡ç‰‡ç­–ç•¥ï¼Œåœ¨ä»»æ„èŠ‚ç‚¹æ•°ä¸‹éƒ½èƒ½ä¿æŒå¯¹æ•°çº§å¤æ‚åº¦ã€‚ æ ¸å¿ƒåˆ›æ–°ï¼šNæ£µç”Ÿæˆæ ‘ï¼ˆN Spanning Treesï¼‰ NHRçš„å…³é”®æ€æƒ³æ˜¯æ„å»ºN &#x3D; âŒˆlogâ‚‚pâŒ‰æ£µç”Ÿæˆæ ‘ï¼Œæ¯æ£µæ ‘è´Ÿè´£ä¸€éƒ¨åˆ†æ•°æ®çš„é€šä¿¡ï¼š æ ‘çš„æ·±åº¦ï¼šâŒˆlogâ‚‚pâŒ‰ï¼ˆä¸èŠ‚ç‚¹æ•°å¯¹æ•°ç›¸å…³ï¼‰ æ ‘çš„ç»“æ„ï¼šæ¯æ£µæ ‘æ ¹æ®èŠ‚ç‚¹ç¼–å·å’Œæ­¥æ•°åŠ¨æ€ç¡®å®šçˆ¶å­å…³ç³» æ•°æ®åˆ†é…ï¼šå°†æ€»æ•°æ®å‡åŒ€æˆ–éå‡åŒ€åˆ‡åˆ†åˆ°Næ£µæ ‘ 2çš„å¹‚æ¬¡ vs é2çš„å¹‚æ¬¡å¯¹æ¯”ï¼š åœºæ™¯1ï¼šRank Size &#x3D; 4ï¼ˆ2çš„å¹‚æ¬¡ï¼‰ Step 1ï¼šäº¤æ¢1&#x2F;2æ•°æ®ï¼ˆè·ç¦»&#x3D;2ï¼‰ R0 â†” R2ï¼ŒR1 â†” R3 æ¯ä¸ªèŠ‚ç‚¹äº¤æ¢ä¸€åŠæ•°æ® Step 2ï¼šäº¤æ¢1&#x2F;4æ•°æ®ï¼ˆè·ç¦»&#x3D;1ï¼‰ R0 â†” R1ï¼ŒR2 â†” R3 æ¯ä¸ªèŠ‚ç‚¹å†äº¤æ¢å‰©ä½™çš„ä¸€åŠ å®Œæˆï¼š2æ­¥ï¼ˆlogâ‚‚4&#x3D;2ï¼‰ï¼Œæ¯æ­¥æ•°æ®é‡é€’å‡ åœºæ™¯2ï¼šRank Size &#x3D; 5ï¼ˆé2çš„å¹‚æ¬¡ï¼‰ é—®é¢˜ï¼šä¸èƒ½å‡åŒ€å¯¹åŠåˆ†ï¼Œéœ€è¦ä¸å‡åŒ€åˆ‡ç‰‡ Step 1ï¼šä¸å‡åŒ€åˆ‡ç‰‡ï¼ˆæŸäº›èŠ‚ç‚¹äº¤æ¢2&#x2F;5ï¼ŒæŸäº›äº¤æ¢3&#x2F;5ï¼‰ æ ¹æ®æ ‘ç»“æ„åŠ¨æ€ç¡®å®šäº¤æ¢é‡ Step 2ï¼šå¤§éƒ¨åˆ†è¿ç»­æ”¶å‘ï¼ˆåˆ©ç”¨è¿ç»­å†…å­˜ï¼Œå‡å°‘å°åŒ…ï¼‰ èšåˆå‘é€ä¼˜åŒ–ï¼Œå‡å°‘ç½‘ç»œåŒ…æ•° Step 3ï¼šå°‘é‡ç¦»æ•£å¤„ç†ï¼ˆå¤„ç†ä¸å¯¹é½éƒ¨åˆ†ï¼‰ å®Œæˆå‰©ä½™æ•°æ®äº¤æ¢ å®Œæˆï¼š3æ­¥ï¼ˆâŒˆlogâ‚‚5âŒ‰&#x3D;3ï¼‰ï¼Œè™½ç„¶ä¸å‡åŒ€ä½†ä¿æŒå¯¹æ•°å¤æ‚åº¦ å…³é”®æŠ€æœ¯ï¼šèšåˆå‘é€ä¼˜åŒ–ï¼ˆAggregated Sendï¼‰ NHRçš„å¦ä¸€ä¸ªä¼˜åŒ–æ˜¯é’ˆå¯¹å°æ•°æ®åŒ…åœºæ™¯ï¼š é—®é¢˜ï¼šå¤šæ£µæ ‘å¯èƒ½äº§ç”Ÿå¤§é‡å°æ•°æ®åŒ…ï¼Œç½‘ç»œåŒ…å¤´å¼€é”€å¤§ è§£å†³ï¼šåœ¨å°æ•°æ®åœºæ™¯ä¸‹ï¼Œé‡‡ç”¨å•æ£µæ ‘ç­–ç•¥ï¼Œå°†æ‰€æœ‰æ•°æ®é€šè¿‡ä¸€æ£µæ ‘ä¼ è¾“ æ•ˆæœï¼šå‡å°‘ç½‘ç»œåŒ…æ•°é‡ï¼Œé™ä½åè®®æ ˆå¼€é”€ æµé‡åˆ†å¸ƒä¼˜åŒ–ï¼š NHRç®—æ³•è®¾è®¡æ—¶è€ƒè™‘äº†ç‰©ç†ä½ç½®ç›¸è¿‘æ€§ï¼š æœ€å¤§æµé‡ï¼šå°½é‡å®‰æ’åœ¨ç‰©ç†ä½ç½®ç›¸è¿‘çš„èŠ‚ç‚¹é—´ï¼ˆå¦‚åŒServerå†…ï¼‰ å‡å°‘å†²çªï¼šé€šè¿‡æ ‘ç»“æ„é¿å…å¤šæµç«äº‰åŒä¸€é“¾è·¯ å±‚æ¬¡åŒ–å‹å¥½ï¼šé€‚é…Serverå†…+Serveré—´çš„å±‚æ¬¡åŒ–ç½‘ç»œ é€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡é›†ç¾¤ï¼šServeræ•°é‡å¤šï¼ŒèŠ‚ç‚¹æ•°å¾€å¾€ä¸æ˜¯2çš„å¹‚æ¬¡ éå¯¹ç§°æ‹“æ‰‘ï¼šèŠ‚ç‚¹æ•°ä»»æ„ï¼ˆ5ã€6ã€7ç­‰ï¼‰ï¼Œä¸å—2çš„å¹‚æ¬¡é™åˆ¶ å°æ•°æ®åŒ…é€šä¿¡ï¼šèšåˆå‘é€ä¼˜åŒ–æå‡å°åŒ…æ€§èƒ½ å±‚æ¬¡åŒ–ç½‘ç»œï¼šæµé‡åˆ†å¸ƒä¼˜åŒ–é€‚é…æ”¶æ•›æ¯”ç½‘ç»œ ä¸RHDå¯¹æ¯”ï¼š ç»´åº¦ RHD NHR èŠ‚ç‚¹æ•°è¦æ±‚ å¿…é¡»æ˜¯2çš„å¹‚æ¬¡ ä»»æ„èŠ‚ç‚¹æ•° æ—¶é—´å¤æ‚åº¦ O(logâ‚‚p) O(âŒˆlogâ‚‚pâŒ‰) æ•°æ®åˆ‡åˆ† å‡åŒ€ å¯èƒ½ä¸å‡åŒ€ å°æ•°æ®ä¼˜åŒ– æ—  å•æ£µæ ‘ç­–ç•¥ å®ç°å¤æ‚åº¦ ä½ ä¸­ç­‰ ç®—æ³•ä¼˜åŠ¿ï¼š é€šç”¨æ€§å¼ºï¼šä»»æ„èŠ‚ç‚¹æ•°éƒ½èƒ½é«˜æ•ˆè¿è¡Œ å¯¹æ•°å¤æ‚åº¦ï¼šä¿æŒâŒˆlogâ‚‚pâŒ‰æ­¥é€šä¿¡ï¼Œæ¥è¿‘ç†è®ºæœ€ä¼˜ çµæ´»ä¼˜åŒ–ï¼šå¯æ ¹æ®æ•°æ®é‡ã€ç½‘ç»œæ‹“æ‰‘è°ƒæ•´ç­–ç•¥ å·¥ç¨‹å®ç”¨ï¼šå¤§è§„æ¨¡é›†ç¾¤ä¸­èŠ‚ç‚¹æ•°å˜åŒ–å¸¸è§ï¼ŒNHRé€‚åº”æ€§å¥½ ç‰¹ç‚¹ï¼š æ—¶é—´å¤æ‚åº¦ï¼š $O(\\lceil \\log_2 N \\rceil)$ å…³é”®ä¼˜åŠ¿ï¼š æ— è®ºèŠ‚ç‚¹æ•°æ˜¯å¦ä¸º2çš„å¹‚æ¬¡ï¼Œå‡ä¿æŒå¯¹æ•°å¤æ‚åº¦ æœ€å¤§æµé‡é›†ä¸­åœ¨ç‰©ç†ä½ç½®ç›¸è¿‘èŠ‚ç‚¹é—´ å‡å°‘æµé‡å†²çª å°æ•°æ®åŒ…åœºæ™¯ä¼˜åŒ–ï¼ˆå•æ£µæ ‘ç­–ç•¥ï¼‰ é€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡é›†ç¾¤ï¼ŒServeræ•°é‡å¤š 3.7.2 æ€§èƒ½æ¨¡å‹ æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ ReduceScatter $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ Næ£µç”Ÿæˆæ ‘ï¼Œæ ‘æ·±åº¦âŒˆlogâ‚‚pâŒ‰ï¼Œèšåˆå‘é€å‡å°‘åŒ…æ•° AllGather $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta$ ä¸ReduceScatterå¯¹ç§°ï¼ŒâŒˆlogâ‚‚pâŒ‰æ­¥æ”¶é›†ï¼Œæ— è§„çº¦å¼€é”€ AllReduce $2\\lceil \\log_2 p \\rceil\\alpha + 2\\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter(âŒˆlogâ‚‚pâŒ‰æ­¥) + AllGather(âŒˆlogâ‚‚pâŒ‰æ­¥) Scatter $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta$ å°æ•°æ®åŒ…åœºæ™¯ä¼˜åŒ–ï¼Œé‡‡ç”¨å•æ£µæ ‘ç­–ç•¥ï¼ŒâŒˆlogâ‚‚pâŒ‰æ­¥å®Œæˆ Broadcast $2\\lceil \\log_2 p \\rceil\\alpha + 2\\frac{p-1}{p}n\\beta$ Scatter(âŒˆlogâ‚‚pâŒ‰æ­¥) + AllGather(âŒˆlogâ‚‚pâŒ‰æ­¥)å®ç° 3.8 NB (Nonuniform Bruck) ç®—æ³•3.8.1 ç®—æ³•åŸç†éå‡åŒ€çš„æ•°æ®å—é€šä¿¡ç®—æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ­¥é•¿çš„å¤šé‡ç¯çŠ¶ç»“æ„å®ç°é«˜æ•ˆé€šä¿¡ã€‚ 123456789101112131415graph TB subgraph &quot;Rank Size = 4&quot; A1[åˆå§‹çŠ¶æ€] --&gt; A2[Step 1: æ­¥é•¿=1] A2 --&gt; A3[Step 2: æ­¥é•¿=2] A3 --&gt; A4[å®Œæˆ logâ‚‚4=2æ­¥] end subgraph &quot;Rank Size = 5&quot; B1[åˆå§‹çŠ¶æ€] --&gt; B2[Step 1: æ­¥é•¿=1] B2 --&gt; B3[Step 2: æ­¥é•¿=2] B3 --&gt; B4[Step 3: æ­¥é•¿=4] B4 --&gt; B5[å®Œæˆ âŒˆlogâ‚‚5âŒ‰=3æ­¥] end Note1[å…³é”®: æ¯æ­¥å‘é€&lt;br/&gt;âŒŠN-1+2^k/2^k+1âŒ‹ ä»½æ•°æ®] ç®—æ³•åŸç†è¯´æ˜ï¼š NBï¼ˆNonuniform Bruckï¼‰ç®—æ³•æ˜¯éå‡åŒ€Bruckç®—æ³•ï¼Œæ˜¯å¯¹ç»å…¸Bruckç®—æ³•çš„æ”¹è¿›ï¼Œä¸“é—¨è§£å†³ä»»æ„èŠ‚ç‚¹æ•°ä¸‹çš„é«˜æ•ˆé›†åˆé€šä¿¡ã€‚ä¸NHRç±»ä¼¼ï¼ŒNBä¹Ÿæ˜¯ä¸ºäº†å…‹æœRHDç®—æ³•å¯¹2çš„å¹‚æ¬¡èŠ‚ç‚¹æ•°çš„é™åˆ¶ï¼Œä½†é‡‡ç”¨äº†ä¸åŒçš„æŠ€æœ¯è·¯çº¿ï¼šåŠ¨æ€æ­¥é•¿å¤šé‡ç¯ã€‚ æ ¸å¿ƒæœºåˆ¶ï¼šåŠ¨æ€æ­¥é•¿é€’å¢ï¼ˆDynamic Distance Incrementï¼‰ NBç®—æ³•çš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯æ­¥é•¿æŒ‰æŒ‡æ•°é€’å¢ï¼š1 â†’ 2 â†’ 4 â†’ 8 â†’ â€¦ ç¬¬kæ­¥ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸è·ç¦»ä¸º2^(k-1)çš„èŠ‚ç‚¹é€šä¿¡ é€šä¿¡æ¨¡å¼ï¼šèŠ‚ç‚¹iä¸èŠ‚ç‚¹(i+2^(k-1)) mod pé€šä¿¡ æ•°æ®é‡ï¼šç¬¬kæ­¥å‘é€ âŒŠ(p-1+2^k)&#x2F;2^(k+1)âŒ‹ ä»½æ•°æ®ï¼ˆéå‡åŒ€ï¼‰ 2çš„å¹‚æ¬¡ vs é2çš„å¹‚æ¬¡å¯¹æ¯”ï¼š åœºæ™¯1ï¼šRank Size &#x3D; 4ï¼ˆ2çš„å¹‚æ¬¡ï¼‰ åˆå§‹çŠ¶æ€ï¼šæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1ä»½æ•°æ®ï¼Œéœ€æ”¶é›†å…¶ä»–3ä»½ Step 1ï¼ˆæ­¥é•¿&#x3D;1ï¼‰ï¼š R0 â†” R1ï¼ŒR2 â†” R3 æ¯ä¸ªèŠ‚ç‚¹äº¤æ¢1ä»½æ•°æ®ï¼Œç°åœ¨å„æŒæœ‰2ä»½ Step 2ï¼ˆæ­¥é•¿&#x3D;2ï¼‰ï¼š R0 â†” R2ï¼ŒR1 â†” R3 æ¯ä¸ªèŠ‚ç‚¹äº¤æ¢2ä»½æ•°æ®ï¼Œç°åœ¨å„æŒæœ‰4ä»½ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰ å®Œæˆï¼š2æ­¥ï¼ˆlogâ‚‚4&#x3D;2ï¼‰ï¼Œæ¯æ­¥æ•°æ®é‡åŠ å€ åœºæ™¯2ï¼šRank Size &#x3D; 5ï¼ˆé2çš„å¹‚æ¬¡ï¼‰ åˆå§‹çŠ¶æ€ï¼šæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰1ä»½æ•°æ®ï¼Œéœ€æ”¶é›†å…¶ä»–4ä»½ Step 1ï¼ˆæ­¥é•¿&#x3D;1ï¼‰ï¼š R0 â†” R1ï¼ŒR1 â†” R2ï¼ŒR2 â†” R3ï¼ŒR3 â†” R4ï¼ŒR4 â†” R0ï¼ˆç¯çŠ¶ï¼‰ æ¯ä¸ªèŠ‚ç‚¹äº¤æ¢1ä»½ï¼Œç°åœ¨å„æŒæœ‰2ä»½ æ•°æ®é‡ï¼šâŒŠ(5-1+2)&#x2F;4âŒ‹ &#x3D; 1ä»½ Step 2ï¼ˆæ­¥é•¿&#x3D;2ï¼‰ï¼š R0 â†” R2ï¼ŒR1 â†” R3ï¼ŒR2 â†” R4ï¼ŒR3 â†” R0ï¼ŒR4 â†” R1 æ¯ä¸ªèŠ‚ç‚¹äº¤æ¢2ä»½ï¼Œç°åœ¨å„æŒæœ‰3æˆ–4ä»½ æ•°æ®é‡ï¼šâŒŠ(5-1+4)&#x2F;8âŒ‹ &#x3D; 1ä»½ï¼ˆä½†å®é™…äº¤æ¢çš„æ˜¯2ä»½ï¼Œå› ä¸ºæœ‰éƒ¨åˆ†é‡å¤ï¼‰ Step 3ï¼ˆæ­¥é•¿&#x3D;4ï¼‰ï¼š R0 â†” R4ï¼ŒR1 â†” R0ï¼ŒR2 â†” R1ï¼ŒR3 â†” R2ï¼ŒR4 â†” R3 è¡¥é½å‰©ä½™æ•°æ®ï¼Œæ‰€æœ‰èŠ‚ç‚¹æŒæœ‰å…¨éƒ¨5ä»½ æ•°æ®é‡ï¼šâŒŠ(5-1+8)&#x2F;16âŒ‹ &#x3D; 0æˆ–1ä»½ï¼ˆå°‘é‡è¡¥å……ï¼‰ å®Œæˆï¼š3æ­¥ï¼ˆâŒˆlogâ‚‚5âŒ‰&#x3D;3ï¼‰ï¼Œé€šè¿‡éå‡åŒ€æ•°æ®é‡åˆ†é…å®Œæˆ å…³é”®å…¬å¼ï¼šæ¯æ­¥å‘é€æ•°æ®é‡ ç¬¬kæ­¥å‘é€æ•°æ®é‡ &#x3D; âŒŠ(N-1+2^k)&#x2F;2^(k+1)âŒ‹ ä»½ å…¬å¼è§£é‡Šï¼š N-1ï¼šæ€»å…±éœ€è¦æ”¶é›†çš„å…¶ä»–èŠ‚ç‚¹æ•°æ®ä»½æ•° 2^kï¼šå½“å‰æ­¥çš„â€è¦†ç›–èŒƒå›´â€è¡¥å¿ 2^(k+1)ï¼šå½’ä¸€åŒ–å› å­ å‘ä¸‹å–æ•´ï¼šç¡®ä¿æ•°æ®é‡ä¸ºæ•´æ•° ç¤ºä¾‹ï¼ˆN&#x3D;5ï¼‰ï¼š k&#x3D;1: âŒŠ(5-1+2)&#x2F;4âŒ‹ &#x3D; âŒŠ7&#x2F;4âŒ‹ &#x3D; 1 k&#x3D;2: âŒŠ(5-1+4)&#x2F;8âŒ‹ &#x3D; âŒŠ8&#x2F;8âŒ‹ &#x3D; 1 k&#x3D;3: âŒŠ(5-1+8)&#x2F;16âŒ‹ &#x3D; âŒŠ12&#x2F;16âŒ‹ &#x3D; 0ï¼ˆå‰©ä½™æ•°æ®å¾ˆå°‘ï¼‰ ä¸RHDå’ŒNHRå¯¹æ¯”ï¼š ç»´åº¦ RHD NHR NB èŠ‚ç‚¹æ•°è¦æ±‚ 2çš„å¹‚æ¬¡ ä»»æ„ ä»»æ„ æ—¶é—´å¤æ‚åº¦ O(logâ‚‚p) O(âŒˆlogâ‚‚pâŒ‰) O(âŒˆlogâ‚‚pâŒ‰) é€šä¿¡æ¨¡å¼ XOR Næ£µç”Ÿæˆæ ‘ åŠ¨æ€æ­¥é•¿ç¯ é¢å¤–é€šä¿¡é‡ æ—  å°‘é‡ å‡ ä¹æ—  å®ç°å¤æ‚åº¦ ä½ ä¸­ ä¸­ NBçš„å…³é”®ä¼˜åŠ¿ï¼š é¿å…é¢å¤–é€šä¿¡é‡å¢é•¿ï¼š RHDåœ¨é2å¹‚æ¬¡èŠ‚ç‚¹æ—¶éœ€è¦é¢å¤–é€šä¿¡æ­¥éª¤ NHRå¯èƒ½äº§ç”Ÿä¸å‡åŒ€åˆ‡ç‰‡å¯¼è‡´éƒ¨åˆ†é€šä¿¡é‡å¢åŠ  NBé€šè¿‡åŠ¨æ€è°ƒæ•´æ¯æ­¥æ•°æ®é‡ï¼Œæœ€å°åŒ–é¢å¤–å¼€é”€ ç¯çŠ¶ç»“æ„ç®€å•ï¼š ç›¸æ¯”NHRçš„Næ£µæ ‘ï¼ŒNBçš„ç¯çŠ¶ç»“æ„æ›´ç›´è§‚ å®ç°ä¸Šæ›´å®¹æ˜“ç†è§£å’Œè°ƒè¯• æ•°å­¦ç²¾ç¡®æ€§ï¼š é€šè¿‡ç²¾ç¡®å…¬å¼è®¡ç®—æ¯æ­¥æ•°æ®é‡ ä¿è¯ç†è®ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„é€šä¿¡é‡ é€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡é›†ç¾¤ï¼šèŠ‚ç‚¹æ•°ä»»æ„ï¼Œä¸å—2çš„å¹‚æ¬¡é™åˆ¶ é€šä¿¡é‡æ•æ„Ÿåœºæ™¯ï¼šéœ€è¦ä¸¥æ ¼æ§åˆ¶é€šä¿¡é‡ï¼Œé¿å…é¢å¤–å¼€é”€ å¯¹æ•°å¤æ‚åº¦è¦æ±‚ï¼šè¦æ±‚âŒˆlogâ‚‚pâŒ‰æ­¥å®Œæˆï¼Œæ¥è¿‘ç†è®ºæœ€ä¼˜ ç®—æ³•æµç¨‹ç‰¹ç‚¹ï¼š é€æ­¥èšåˆï¼šæ¯ä¸€æ­¥éƒ½åœ¨å‰ä¸€æ­¥åŸºç¡€ä¸Šèšåˆæ›´å¤šæ•°æ® æ­¥é•¿å€å¢ï¼š1â†’2â†’4â†’â€¦ï¼Œç±»ä¼¼äºŒè¿›åˆ¶å±•å¼€ éå‡åŒ€ä½†æœ€ä¼˜ï¼šè™½ç„¶æ¯æ­¥æ•°æ®é‡å¯èƒ½ä¸åŒï¼Œä½†æ€»é€šä¿¡é‡æ¥è¿‘ç†è®ºä¸‹ç•Œ ç‰¹ç‚¹ï¼š æ—¶é—´å¤æ‚åº¦ï¼š $O(\\lceil \\log_2 N \\rceil)$ å…³é”®ä¼˜åŠ¿ï¼š ä¸åŒèŠ‚ç‚¹æ•°ä¸‹å‡ä¿æŒå¯¹æ•°é€šä¿¡æ­¥æ•° é¿å…é¢å¤–é€šä¿¡æ•°æ®é‡å¢é•¿ï¼ˆç›¸æ¯”RHDï¼‰ é€‚ç”¨åœºæ™¯ï¼š å¤§è§„æ¨¡é›†ç¾¤ï¼ŒServeræ•°é‡å¤š 3.8.2 æ€§èƒ½æ¨¡å‹ æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ ReduceScatter $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ åŠ¨æ€æ­¥é•¿å¤šé‡ç¯ï¼ŒâŒˆlogâ‚‚pâŒ‰æ­¥ï¼Œç¬¬kæ­¥ä¼ è¾“âŒŠ(p-1+2^k)&#x2F;2^(k+1)âŒ‹ä»½æ•°æ® AllGather $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta$ æ­¥é•¿é€’å¢(1â†’2â†’4â†’â€¦)ï¼ŒâŒˆlogâ‚‚pâŒ‰æ­¥ï¼Œæ— é¢å¤–é€šä¿¡é‡ AllReduce $2\\lceil \\log_2 p \\rceil\\alpha + 2\\frac{p-1}{p}n\\beta + \\frac{p-1}{p}n\\gamma$ ReduceScatter(âŒˆlogâ‚‚pâŒ‰æ­¥) + AllGather(âŒˆlogâ‚‚pâŒ‰æ­¥) Scatter $\\lceil \\log_2 p \\rceil\\alpha + \\frac{p-1}{p}n\\beta$ åŠ¨æ€æ­¥é•¿æ•£å‘ï¼ŒâŒˆlogâ‚‚pâŒ‰æ­¥å®Œæˆï¼Œæ¯æ­¥ä¼ è¾“é‡ä¸å‡åŒ€ Broadcast $2\\lceil \\log_2 p \\rceil\\alpha + 2\\frac{p-1}{p}n\\beta$ Scatter(âŒˆlogâ‚‚pâŒ‰æ­¥) + AllGather(âŒˆlogâ‚‚pâŒ‰æ­¥)å®ç° 3.9 AHC (Asymmetric Hierarchical Concatenate) ç®—æ³•3.9.1 ç®—æ³•åŸç†å±‚æ¬¡åŒ–é›†åˆé€šä¿¡ç®—æ³•ï¼Œä¸“é—¨å¤„ç†éå¯¹ç§°å±‚æ¬¡åŒ–ç½‘ç»œæ‹“æ‰‘ã€‚ 123456789101112131415161718192021graph TB subgraph &quot;Phase 1: ç»„å†…ReduceScatter&quot; A1[Group1: 2å¡] --&gt; A2[å¹¶è¡ŒReduceScatter] A3[Group2: 3å¡] --&gt; A2 end subgraph &quot;Phase 2: æ„å»ºé€»è¾‘åŒå·å¡&quot; B1[æ•°æ®åˆ‡åˆ†: LCMÃ—Gå—] --&gt; B2[Groupé—´å¯¹åº”å…³ç³»] B2 --&gt; B3[é€»è¾‘åŒå·å¡AllReduce] end subgraph &quot;Phase 3: ç»„å†…AllGather&quot; C1[Group1 AllGather] --&gt; C2[å®Œæˆ] C3[Group2 AllGather] --&gt; C2 end A2 --&gt; B1 B3 --&gt; C1 B3 --&gt; C3 Note1[LCM: æœ€å°å…¬å€æ•°&lt;br/&gt;ç¤ºä¾‹: 2å’Œ3 â†’ LCM=6] ç‰¹ç‚¹ï¼š é€‚ç”¨åœºæ™¯ï¼š å±‚æ¬¡åŒ–ç½‘ç»œæ‹“æ‰‘ ä¸åŒå±‚æ¬¡é—´NPUæ•°é‡ä¸å¯¹ç§° å±‚æ¬¡é—´å­˜åœ¨å¸¦å®½æ”¶æ•› å…³é”®æŠ€æœ¯ï¼š åŸºäºæ‹“æ‰‘çš„è®¡ç®—å•å…ƒé‡æ–°åˆ†ç»„ é€»è¾‘åŒå·å¡æ¦‚å¿µ éå‡åŒ€æ•°æ®å—åˆ‡åˆ† 3.9.2 æ‰§è¡Œæµç¨‹ç¤ºä¾‹ï¼ˆä»¥AllReduceä¸ºä¾‹ï¼‰è¯´æ˜ï¼š AHCç®—æ³•ä¸»è¦åº”ç”¨äºAllReduceã€ReduceScatterç­‰éœ€è¦è·¨å±‚æ¬¡é€šä¿¡çš„åœºæ™¯ï¼Œæ­¤å¤„ä»¥AllReduceä¸ºå…¸å‹ç¤ºä¾‹ã€‚ åœºæ™¯ï¼š 5ä¸ªRankï¼Œåˆ†ä¸º2ç»„ï¼ˆGroup1: 2å¡ï¼ŒGroup2: 3å¡ï¼‰ 123456789101112131415161718192021sequenceDiagram participant G1R0 as Group1-Rank0 participant G1R1 as Group1-Rank1 participant G2R0 as Group2-Rank0 participant G2R1 as Group2-Rank1 participant G2R2 as Group2-Rank2 Note over G1R0,G2R2: Step 1: ç»„å†…ReduceScatter G1R0-&gt;&gt;G1R1: ReduceScatter (2å¡) G2R0-&gt;&gt;G2R1: ReduceScatter (3å¡) G2R0-&gt;&gt;G2R2: ReduceScatter (3å¡) Note over G1R0,G2R2: Step 2: é€»è¾‘åŒå·å¡AllReduce Note right of G1R0: æ•°æ®åˆ‡åˆ†ä¸ºLCM(2,3)Ã—2=12å— G1R0-&gt;&gt;G2R0: å¯¹åº”æ•°æ®å—AllReduce G1R1-&gt;&gt;G2R1: å¯¹åº”æ•°æ®å—AllReduce Note over G1R0,G2R2: Step 3: ç»„å†…AllGather G1R0-&gt;&gt;G1R1: AllGather G2R0-&gt;&gt;G2R1: AllGather G2R0-&gt;&gt;G2R2: AllGather è¯¦ç»†æ‰§è¡Œæµç¨‹è¯´æ˜ï¼š AHCç®—æ³•ä¸“é—¨è§£å†³å±‚æ¬¡åŒ–éå¯¹ç§°ç½‘ç»œæ‹“æ‰‘çš„é›†åˆé€šä¿¡é—®é¢˜ã€‚å½“ä¸åŒå±‚æ¬¡é—´çš„è®¡ç®—å•å…ƒæ•°é‡ä¸å¯¹ç§°æ—¶ï¼ˆå¦‚Group1æœ‰2å¡ï¼ŒGroup2æœ‰3å¡ï¼‰ï¼Œä¼ ç»Ÿç®—æ³•æ— æ³•ç›´æ¥åº”ç”¨ã€‚AHCé€šè¿‡é€»è¾‘åŒå·å¡æ¦‚å¿µå’ŒLCMæ•°æ®åˆ‡åˆ†å®ç°é«˜æ•ˆé€šä¿¡ã€‚ åˆå§‹çŠ¶æ€ï¼š Group1: Rank0å’ŒRank1å„æŒæœ‰å®Œæ•´æ•°æ®[A,B,C,D,E,F] Group2: Rank0ã€Rank1ã€Rank2å„æŒæœ‰å®Œæ•´æ•°æ®[A,B,C,D,E,F] ç›®æ ‡ï¼šæ‰€æœ‰5ä¸ªRankæœ€ç»ˆæŒæœ‰æ‰€æœ‰æ•°æ®çš„è§„çº¦ç»“æœ Phase 1: ç»„å†…ReduceScatteré˜¶æ®µ æ¯ä¸ªGroupå†…éƒ¨ç‹¬ç«‹æ‰§è¡ŒReduceScatterï¼Œå°†æ•°æ®è§„çº¦å¹¶åˆ†æ•£åˆ°å„ä¸ªæˆå‘˜èŠ‚ç‚¹ã€‚ Group1 (2å¡) æ‰§è¡Œï¼š ä½¿ç”¨Ringæˆ–RHDç®—æ³•åœ¨G1R0å’ŒG1R1é—´æ‰§è¡ŒReduceScatter G1R0æŒæœ‰å‰åŠéƒ¨åˆ†è§„çº¦ç»“æœï¼š[A*, B*, C*] G1R1æŒæœ‰ååŠéƒ¨åˆ†è§„çº¦ç»“æœï¼š[D*, E*, F*] æ³¨ï¼šæ­¤å¤„çš„*è¡¨ç¤ºGroup1å†…éƒ¨çš„å±€éƒ¨è§„çº¦ï¼ˆä»…2ä¸ªèŠ‚ç‚¹çš„è§„çº¦ï¼‰ Group2 (3å¡) æ‰§è¡Œï¼š ä½¿ç”¨Ringç®—æ³•åœ¨G2R0ã€G2R1ã€G2R2é—´æ‰§è¡ŒReduceScatter G2R0æŒæœ‰ç¬¬1ä»½ï¼š[A*, B*] G2R1æŒæœ‰ç¬¬2ä»½ï¼š[C*, D*] G2R2æŒæœ‰ç¬¬3ä»½ï¼š[E*, F*] æ³¨ï¼šæ­¤å¤„çš„*è¡¨ç¤ºGroup2å†…éƒ¨çš„å±€éƒ¨è§„çº¦ï¼ˆä»…3ä¸ªèŠ‚ç‚¹çš„è§„çº¦ï¼‰ é˜¶æ®µç‰¹ç‚¹ï¼šå„ç»„å†…éƒ¨å¹¶è¡Œæ‰§è¡Œï¼Œæ— è·¨ç»„é€šä¿¡ï¼Œå……åˆ†åˆ©ç”¨ç»„å†…é«˜å¸¦å®½é“¾è·¯ Phase 2: è·¨ç»„é€»è¾‘åŒå·å¡AllReduceé˜¶æ®µ è¿™æ˜¯AHCç®—æ³•çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé€šè¿‡LCM(Least Common Multiple)æ•°æ®åˆ‡åˆ†å’Œé€»è¾‘åŒå·å¡æ˜ å°„å®ç°è·¨ç»„é€šä¿¡ã€‚ LCMåˆ‡åˆ†åŸç†ï¼š Group1æœ‰2å¡ï¼ŒGroup2æœ‰3å¡ï¼ŒLCM(2,3) &#x3D; 6 æ•°æ®è¢«åˆ‡åˆ†ä¸º LCM Ã— G &#x3D; 6 Ã— 2 &#x3D; 12 ä¸ªé€»è¾‘å— è¿™æ ·ç¡®ä¿æ¯ä¸ªç‰©ç†èŠ‚ç‚¹éƒ½èƒ½è´Ÿè´£æ•´æ•°ä¸ªé€»è¾‘å— é€»è¾‘åŒå·å¡æ˜ å°„ï¼š G1R0ï¼ˆé€»è¾‘ç¼–å·0ï¼‰å¯¹åº” G2R0ï¼ˆé€»è¾‘ç¼–å·0ï¼‰ï¼šè´Ÿè´£å—0-5çš„è§„çº¦ G1R1ï¼ˆé€»è¾‘ç¼–å·1ï¼‰å¯¹åº” G2R1ï¼ˆé€»è¾‘ç¼–å·1ï¼‰ï¼šè´Ÿè´£å—6-11çš„è§„çº¦ G2R2ä½œä¸ºé¢å¤–èŠ‚ç‚¹ï¼Œå°†å…¶æ•°æ®åˆ†é…ç»™å¯¹åº”çš„é€»è¾‘åŒå·å¡å¤„ç† è·¨ç»„AllReduceæ‰§è¡Œï¼š G1R0 â†” G2R0ä¹‹é—´å¯¹å¯¹åº”æ•°æ®å—æ‰§è¡ŒAllReduceï¼ˆè§„çº¦å¹¶äº¤æ¢ç»“æœï¼‰ G1R1 â†” G2R1ä¹‹é—´å¯¹å¯¹åº”æ•°æ®å—æ‰§è¡ŒAllReduce G2R2çš„æ•°æ®é€šè¿‡ç¯çŠ¶é€šä¿¡æˆ–ç›´æ¥å‘é€æ–¹å¼å‚ä¸è§„çº¦ æ‰§è¡Œåï¼Œé€»è¾‘åŒå·å¡æŒæœ‰è·¨ç»„å®Œæ•´è§„çº¦ç»“æœ é˜¶æ®µç‰¹ç‚¹ï¼šè·¨ç»„é€šä¿¡é‡æœ€å°åŒ–ï¼Œä»…åœ¨é€»è¾‘å¯¹åº”èŠ‚ç‚¹é—´è¿›è¡Œï¼Œé¿å…å…¨è¿æ¥é€šä¿¡ Phase 3: ç»„å†…AllGatheré˜¶æ®µ å°†è·¨ç»„è§„çº¦çš„ç»“æœåœ¨å„ç»„å†…éƒ¨æ”¶é›†ï¼Œä½¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æŒæœ‰å®Œæ•´çš„æœ€ç»ˆç»“æœã€‚ Group1 AllGatherï¼š G1R0å’ŒG1R1æ‰§è¡ŒAllGather å°†å„è‡ªæŒæœ‰çš„éƒ¨åˆ†è§„çº¦ç»“æœäº¤æ¢ æœ€ç»ˆG1R0å’ŒG1R1éƒ½æŒæœ‰å®Œæ•´çš„å…¨å±€è§„çº¦ç»“æœ[A*, B*, C*, D*, E*, F*] Group2 AllGatherï¼š G2R0ã€G2R1ã€G2R2æ‰§è¡ŒAllGather ä¸‰ä¸ªèŠ‚ç‚¹é—´äº¤æ¢å„è‡ªæŒæœ‰çš„è§„çº¦ç»“æœ æœ€ç»ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½æŒæœ‰å®Œæ•´çš„å…¨å±€è§„çº¦ç»“æœ[A*, B*, C*, D*, E*, F*] é˜¶æ®µç‰¹ç‚¹ï¼šå†æ¬¡åˆ©ç”¨ç»„å†…é«˜å¸¦å®½ï¼Œå¿«é€Ÿå®Œæˆæ•°æ®å¹¿æ’­ï¼Œæ— é¢å¤–è§„çº¦è®¡ç®— AHCç®—æ³•æ•´ä½“ç‰¹ç‚¹ï¼š éå¯¹ç§°æ‹“æ‰‘é€‚åº”æ€§ï¼šé€šè¿‡LCMåˆ‡åˆ†è§£å†³ä¸åŒç»„å¡æ•°ä¸ä¸€è‡´é—®é¢˜ ä¸‰é˜¶æ®µåˆ†æ²»ï¼šç»„å†…â†’è·¨ç»„â†’ç»„å†…ï¼Œæœ€å°åŒ–é«˜å»¶è¿Ÿè·¨ç»„é€šä¿¡é‡ é€»è¾‘åŒå·å¡åˆ›æ–°ï¼šè™šæ‹ŸåŒ–ç‰©ç†èŠ‚ç‚¹æ˜ å°„ï¼Œå®ç°è´Ÿè½½å‡è¡¡ å¸¦å®½åˆ†å±‚åˆ©ç”¨ï¼šç»„å†…ç”¨é«˜å¸¦å®½é“¾è·¯ï¼Œè·¨ç»„ç”¨æœ‰é™å¸¦å®½é“¾è·¯ï¼Œå……åˆ†é€‚åº”æ”¶æ•›æ¯”ç½‘ç»œ å¤æ‚åº¦æƒè¡¡ï¼šè™½ç„¶å¢åŠ äº†æ•°æ®åˆ‡åˆ†å¤æ‚åº¦ï¼Œä½†æ˜¾è‘—å‡å°‘äº†è·¨å±‚é€šä¿¡å¼€é”€ï¼Œåœ¨å±‚æ¬¡åŒ–ç½‘ç»œä¸­æ€§èƒ½ä¼˜å¼‚ å…¸å‹åº”ç”¨åœºæ™¯ï¼š å¤šæœºå¤šå¡è®­ç»ƒï¼ˆæœºå™¨é—´å¸¦å®½ &lt;&lt; æœºå™¨å†…å¸¦å®½ï¼‰ Podå†…è®­ç»ƒï¼ˆPodé—´å¸¦å®½æ”¶æ•›ï¼‰ è¾¹ç¼˜è®¡ç®—é›†ç¾¤ï¼ˆä¸åŒè¾¹ç¼˜èŠ‚ç‚¹è®¡ç®—èƒ½åŠ›ä¸å¯¹ç§°ï¼‰ 3.9.3 æ€§èƒ½æ¨¡å‹é‡‡ç”¨NBç®—æ³•ä½œä¸ºç»„å†…å’Œç»„é—´ç®—æ³•æ—¶ï¼š $$T_{ReduceScatter} &#x3D; 2(\\lceil \\log_2(m+d) \\rceil + \\lceil \\log_2 G \\rceil)\\alpha + 2(\\frac{m+d-1}{m+d} + \\frac{(G-1)C}{Gm})n\\beta + (\\frac{m+d-1}{m+d} + \\frac{G-1}{Gm})n\\gamma$$ å‚æ•°è¯´æ˜ï¼š mï¼šæœ€å°åˆ†ç»„å¡æ•° dï¼šæœ€å¤§åˆ†ç»„ä¸æœ€å°åˆ†ç»„çš„å·®å€¼ Gï¼šåˆ†ç»„æ•° Cï¼šç»„é—´å¸¦å®½ç›¸å¯¹ç»„å†…å¸¦å®½çš„æ”¶æ•›æ¯” 3.10 Pipeline ç®—æ³•3.10.1 ç®—æ³•åŸç†æµæ°´çº¿å¹¶è¡Œç®—æ³•ï¼Œå……åˆ†åˆ©ç”¨Serverå†…å’ŒServeré—´é“¾è·¯çš„å¹¶å‘èƒ½åŠ›ã€‚ 123456789101112131415graph TB subgraph &quot;ä¼ ç»Ÿåˆ†çº§ç®—æ³•&quot; A1[Serveré—´é€šä¿¡] --&gt; A2[é“¾è·¯åˆ©ç”¨ç‡] A2 --&gt; A3[Serverå†…ç©ºé—²] A3 --&gt; A4[å¸¦å®½æµªè´¹] end subgraph &quot;Pipelineç®—æ³•&quot; B1[Serveré—´ä¼ è¾“] -.å¹¶å‘.-&gt; B2[Serverå†…ä¼ è¾“] B2 --&gt; B3[é“¾è·¯å……åˆ†åˆ©ç”¨] B3 --&gt; B4[å¸¦å®½åˆ©ç”¨ç‡æå‡] end style A4 fill:#ffcccc style B4 fill:#ccffcc æ ¸å¿ƒæ€æƒ³ï¼š æŒ–æ˜é€šä¿¡ç®—æ³•çš„æ•°æ®ä¾èµ–ï¼Œé€šè¿‡æµæ°´å¹¶è¡Œè§£å†³å¸¦å®½åˆ©ç”¨ä¸è¶³é—®é¢˜ã€‚ 3.10.2 æµæ°´çº¿æ‰§è¡Œç¤ºä¾‹ï¼ˆä»¥AllGatherä¸ºä¾‹ï¼‰è¯´æ˜ï¼š Pipelineç®—æ³•ä¸»è¦åº”ç”¨äºAllReduceã€AllGatherã€ReduceScatterç­‰å¤§æ•°æ®é‡åœºæ™¯ï¼Œæ­¤å¤„ä»¥AllGatherä¸ºå…¸å‹ç¤ºä¾‹å±•ç¤ºæµæ°´çº¿å¹¶å‘æœºåˆ¶ã€‚ 12345678910111213141516sequenceDiagram participant S0R0 as Server0-Rank0 participant S0R1 as Server0-Rank1 participant S1R2 as Server1-Rank2 participant S1R3 as Server1-Rank3 Note over S0R0,S1R3: Step 1: Serveré—´Ring + Serverå†…ä¼ è¾“ S1R2-&gt;&gt;S0R0: ç»¿è‰²å— (Serveré—´) S0R0-&gt;&gt;S0R1: ç»¿è‰²å— (Serverå†…å¹¶å‘) Note over S0R0,S1R3: Step 2: ç»§ç»­Ring + Serverå†…ä¼ è¾“ S0R0-&gt;&gt;S1R2: çº¢è‰²å— (Serveré—´) S0R0-&gt;&gt;S0R1: ä¸Šä¸€æ­¥æ¥æ”¶çš„å— (Serverå†…) S1R2-&gt;&gt;S1R3: ä¸Šä¸€æ­¥æ¥æ”¶çš„å— (Serverå†…) Note over S0R0,S1R3: æ¯ä¸€æ­¥éƒ½å®ç°Serveré—´å’ŒServerå†…å¹¶å‘ è¯¦ç»†æ‰§è¡Œæµç¨‹è¯´æ˜ï¼š Pipelineç®—æ³•çš„æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³å¸¦å®½åˆ©ç”¨ç‡ä¸è¶³é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å±‚æ¬¡åŒ–ç½‘ç»œæ‹“æ‰‘ä¸­ï¼ˆServerå†…é«˜å¸¦å®½ + Serveré—´ä½å¸¦å®½ï¼‰ï¼Œä¼ ç»Ÿåˆ†çº§ç®—æ³•ä¼šå¯¼è‡´æŸä¸€æ—¶åˆ»åªæœ‰ä¸€å±‚é“¾è·¯å·¥ä½œï¼Œå¦ä¸€å±‚é“¾è·¯ç©ºé—²ã€‚Pipelineé€šè¿‡æŒ–æ˜æ•°æ®ä¾èµ–å…³ç³»ï¼Œå®ç°è·¨å±‚å¹¶å‘ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰é“¾è·¯å¸¦å®½ã€‚ åœºæ™¯è®¾ç½®ï¼š 4ä¸ªRankåˆ†å¸ƒåœ¨2ä¸ªServerä¸Šï¼šServer0åŒ…å«Rank0å’ŒRank1ï¼ŒServer1åŒ…å«Rank2å’ŒRank3 Serverå†…å¸¦å®½ï¼šé«˜ï¼ˆå¦‚NVLink 600GB&#x2F;sï¼‰ Serveré—´å¸¦å®½ï¼šä½ï¼ˆå¦‚RDMA 100Gb&#x2F;s &#x3D; 12.5GB&#x2F;sï¼Œæ”¶æ•›æ¯”çº¦48:1ï¼‰ åˆå§‹çŠ¶æ€ï¼šæ¯ä¸ªRankæŒæœ‰ä¸åŒæ•°æ®å—ï¼Œéœ€æ‰§è¡ŒAllGatheræ”¶é›†æ‰€æœ‰æ•°æ® ä¼ ç»Ÿåˆ†çº§ç®—æ³•çš„é—®é¢˜ï¼š é˜¶æ®µ1ï¼šServeré—´é€šä¿¡æ—¶ï¼ŒServerå†…é“¾è·¯å®Œå…¨ç©ºé—² é˜¶æ®µ2ï¼šServerå†…é€šä¿¡æ—¶ï¼ŒServeré—´é“¾è·¯å®Œå…¨ç©ºé—² ç»“æœï¼šé“¾è·¯åˆ©ç”¨ç‡ä½ï¼Œæ€»æ—¶é—´ &#x3D; Serveré—´æ—¶é—´ + Serverå†…æ—¶é—´ Pipelineç®—æ³•çš„åˆ›æ–°ï¼š é€šè¿‡æ•°æ®ä¾èµ–åˆ†æï¼Œå‘ç°ï¼šå½“Rank0ä»Rank2æ¥æ”¶åˆ°ä¸€ä¸ªæ•°æ®å—åï¼Œå¯ä»¥ç«‹å³å°†è¯¥æ•°æ®å—è½¬å‘ç»™Serverå†…çš„Rank1ï¼Œè€Œæ— éœ€ç­‰å¾…æ‰€æœ‰Serveré—´é€šä¿¡å®Œæˆã€‚è¿™æ ·Serveré—´å’ŒServerå†…é€šä¿¡å¯ä»¥æµæ°´çº¿å¹¶å‘ã€‚ Step-by-Stepæ‰§è¡Œæµç¨‹ï¼ˆAllGatherç¤ºä¾‹ï¼‰ï¼š åˆå§‹çŠ¶æ€ï¼š S0R0æŒæœ‰ï¼š[çº¢è‰²å—] S0R1æŒæœ‰ï¼š[è“è‰²å—] S1R2æŒæœ‰ï¼š[ç»¿è‰²å—] S1R3æŒæœ‰ï¼š[é»„è‰²å—] ç›®æ ‡ï¼šæ‰€æœ‰RankæŒæœ‰[çº¢ã€è“ã€ç»¿ã€é»„]å…¨éƒ¨æ•°æ® Step 1ï¼šç¬¬ä¸€è½®å¹¶å‘ä¼ è¾“ Serveré—´Ringé€šä¿¡ï¼š S1R2 â†’ S0R0ï¼šå‘é€ç»¿è‰²å—ï¼ˆè·¨Serverï¼Œèµ°ä½å¸¦å®½é“¾è·¯ï¼‰ S0R0 â†’ S1R2ï¼šå‘é€çº¢è‰²å—ï¼ˆè·¨Serverï¼‰ Serverå†…å¹¶å‘é€šä¿¡ï¼ˆä¸ä¸Šè¿°åŒæ—¶è¿›è¡Œï¼‰ï¼š S0R0 â†’ S0R1ï¼šå‘é€çº¢è‰²å—ï¼ˆServerå†…ï¼Œèµ°é«˜å¸¦å®½é“¾è·¯ï¼‰ S1R2 â†’ S1R3ï¼šå‘é€ç»¿è‰²å—ï¼ˆServerå†…ï¼‰ Step 1åçŠ¶æ€ï¼š S0R0ï¼š[çº¢ã€ç»¿] S0R1ï¼š[è“ã€çº¢] S1R2ï¼š[ç»¿ã€çº¢] S1R3ï¼š[é»„ã€ç»¿] å¹¶å‘æ•ˆæœï¼šServeré—´ä¼ è¾“ç»¿è‰²å—çš„åŒæ—¶ï¼ŒServerå†…ä¹Ÿåœ¨ä¼ è¾“çº¢è‰²&#x2F;ç»¿è‰²å—ï¼Œä¸¤å±‚é“¾è·¯éƒ½åœ¨å·¥ä½œ Step 2ï¼šç¬¬äºŒè½®å¹¶å‘ä¼ è¾“ Serveré—´Ringé€šä¿¡ï¼š S0R0 â†’ S1R2ï¼šå‘é€[çº¢ã€ç»¿]ä¸­çš„ç»¿è‰²å—ï¼ˆS0R0åˆšæ”¶åˆ°çš„ï¼‰ S1R2 â†’ S0R0ï¼šå‘é€[ç»¿ã€çº¢]ä¸­çš„çº¢è‰²å— æ³¨æ„ï¼šS0R0å¯ä»¥ç«‹å³è½¬å‘ä¸Šä¸€æ­¥åˆšæ”¶åˆ°çš„ç»¿è‰²å—ï¼Œæ— éœ€ç­‰å¾… Serverå†…å¹¶å‘é€šä¿¡ï¼š S0R0 â†’ S0R1ï¼šå‘é€ç»¿è‰²å—ï¼ˆS0R0åœ¨Step 1æ”¶åˆ°çš„ï¼‰ S0R1 â†’ S0R0ï¼šå‘é€è“è‰²å— S1R2 â†’ S1R3ï¼šå‘é€çº¢è‰²å—ï¼ˆS1R2åœ¨Step 1æ”¶åˆ°çš„ï¼‰ S1R3 â†’ S1R2ï¼šå‘é€é»„è‰²å— Step 2åçŠ¶æ€ï¼š S0R0ï¼š[çº¢ã€ç»¿ã€è“] S0R1ï¼š[è“ã€çº¢ã€ç»¿] S1R2ï¼š[ç»¿ã€çº¢ã€é»„] S1R3ï¼š[é»„ã€ç»¿ã€çº¢] Step 3ï¼šç¬¬ä¸‰è½®å¹¶å‘ä¼ è¾“ Serveré—´Ringé€šä¿¡ï¼š S0R0 â†’ S1R2ï¼šå‘é€è“è‰²å— S1R2 â†’ S0R0ï¼šå‘é€é»„è‰²å— Serverå†…å¹¶å‘é€šä¿¡ï¼š S0R0 â†’ S0R1ï¼šå‘é€è“è‰²å—æˆ–é»„è‰²å— S1R2 â†’ S1R3ï¼šå‘é€è“è‰²å—æˆ–é»„è‰²å— æœ€ç»ˆçŠ¶æ€ï¼šæ‰€æœ‰RankæŒæœ‰[çº¢ã€è“ã€ç»¿ã€é»„]å®Œæ•´æ•°æ® Pipelineæ ¸å¿ƒæœºåˆ¶ï¼š æ•°æ®æµæ°´ï¼šæ•°æ®å—åƒæµæ°´çº¿ä¸€æ ·æµåŠ¨ï¼Œåˆšåˆ°è¾¾çš„æ•°æ®ç«‹å³è½¬å‘ï¼Œæ— éœ€ç­‰å¾…æ‰¹æ¬¡å®Œæˆ åŒå±‚å¹¶å‘ï¼šæ¯ä¸ªæ—¶é—´ç‰‡å†…ï¼ŒServeré—´å’ŒServerå†…é“¾è·¯åŒæ—¶ä¼ è¾“ä¸åŒæ•°æ® ä¾èµ–è§£è€¦ï¼šé€šè¿‡åˆ†ææ•°æ®ä¾èµ–ï¼Œå°†â€Serveré—´å®Œæˆâ†’Serverå†…å¼€å§‹â€çš„ä¸²è¡Œä¾èµ–è§£è€¦ä¸ºå¹¶å‘ å¸¦å®½å……åˆ†åˆ©ç”¨ï¼šé«˜å¸¦å®½é“¾è·¯ï¼ˆServerå†…ï¼‰å’Œä½å¸¦å®½é“¾è·¯ï¼ˆServeré—´ï¼‰åŒæ—¶å·¥ä½œï¼Œæ€»æ—¶é—´çº¦ç­‰äº max(Serveré—´æ—¶é—´, Serverå†…æ—¶é—´)ï¼Œè€ŒéäºŒè€…ä¹‹å’Œ æ€§èƒ½å¯¹æ¯”ï¼ˆå‡è®¾æ•°æ®é‡Sï¼ŒServeré—´å¸¦å®½Î²_interï¼ŒServerå†…å¸¦å®½Î²_intraï¼‰ï¼š ä¼ ç»Ÿåˆ†çº§ç®—æ³•æ€»æ—¶é—´ï¼š$T &#x3D; \\frac{S}{\\beta_{inter}} + \\frac{S}{\\beta_{intra}}$ Pipelineç®—æ³•æ€»æ—¶é—´ï¼š$T \\approx \\max(\\frac{S}{\\beta_{inter}}, \\frac{S}{\\beta_{intra}})$ åŠ é€Ÿæ¯”ï¼šå½“ Î²_intra &gt;&gt; Î²_inter æ—¶ï¼ŒåŠ é€Ÿæ¯”æ¥è¿‘ $\\frac{\\beta_{intra} + \\beta_{inter}}{\\beta_{intra}} \\approx 1 + \\frac{\\beta_{inter}}{\\beta_{intra}}$ ä¾‹å¦‚æ”¶æ•›æ¯”48:1çš„åœºæ™¯ï¼Œç†è®ºåŠ é€Ÿæ¯”å¯è¾¾1.02å€ï¼Œä½†å®é™…å¤§æ•°æ®é‡åœºæ™¯ä¸‹åŠ é€Ÿæ›´æ˜æ˜¾ã€‚ Pipelineç®—æ³•ç‰¹ç‚¹ï¼š é€‚ç”¨åœºæ™¯å¹¿æ³›ï¼šAllReduceã€AllGatherã€ReduceScatterç­‰å¤šç§åŸè¯­éƒ½å¯åº”ç”¨ æ”¶æ•›æ¯”æ•æ„Ÿï¼šæ”¶æ•›æ¯”è¶Šå¤§ï¼ˆServerå†…å¤–å¸¦å®½å·®å¼‚è¶Šå¤§ï¼‰ï¼ŒPipelineä¼˜åŠ¿è¶Šæ˜æ˜¾ å®ç°å¤æ‚åº¦é«˜ï¼šéœ€è¦ç²¾ç»†ç®¡ç†æ•°æ®ä¾èµ–å’Œè°ƒåº¦ï¼Œä»£ç å¤æ‚åº¦è¾ƒé«˜ å†…å­˜å¼€é”€ï¼šéœ€è¦é¢å¤–ç¼“å†²åŒºå­˜å‚¨æµæ°´çº¿ä¸­çš„ä¸­é—´æ•°æ® å»¶è¿Ÿéšè—ï¼šé€šè¿‡å¹¶å‘éšè—è·¨å±‚é€šä¿¡å»¶è¿Ÿï¼Œç‰¹åˆ«é€‚åˆå¤§æ•°æ®é‡åœºæ™¯ å…³é”®ç‰¹æ€§ï¼š AllReduce: Serveré—´Ring(ReduceScatter+AllGather) å¹¶å‘ Serverå†…FullMesh ReduceScatter: Serveré—´Ring å¹¶å‘ Serverå†…ä¼ è¾“ AllGather: Serveré—´Ring å¹¶å‘ Serverå†…ä¼ è¾“ 3.10.3 æ€§èƒ½æ¨¡å‹ æ“ä½œ è€—æ—¶å…¬å¼ è¯´æ˜ ReduceScatter $\\max(\\frac{s}{p}\\beta_{inter} + \\alpha_{inter}, \\frac{s}{p}\\beta_{intra} + \\alpha_{intra}) \\times (p_{inter}-1) + \\frac{s}{p}\\beta_{intra} + \\alpha_{intra}$ Serveré—´Ringå’ŒServerå†…Meshå¹¶å‘ï¼Œæ¯æ­¥å–è¾ƒæ…¢è€…ï¼Œæœ€åè¡¥ä¸€æ¬¡Serverå†…ä¼ è¾“ AllGather $\\max(\\frac{s}{p}\\beta_{inter} + \\alpha_{inter}, \\frac{s}{p}\\beta_{intra} + \\alpha_{intra}) \\times (p_{inter}-1) + \\frac{s}{p}\\beta_{intra} + \\alpha_{intra}$ ä¸ReduceScatterå¯¹ç§°ï¼Œæµæ°´çº¿å¼å¹¶å‘æ”¶é›†æ•°æ® AllReduce $2 \\times (\\max(\\frac{s}{p}\\beta_{inter} + \\alpha_{inter}, \\frac{s}{p}\\beta_{intra} + \\alpha_{intra}) \\times (p_{inter}-1) + \\frac{s}{p}\\beta_{intra} + \\alpha_{intra})$ ReduceScatter + AllGatherï¼Œä¸¤é˜¶æ®µæµæ°´å¹¶å‘å……åˆ†åˆ©ç”¨å¸¦å®½ å‚æ•°è¯´æ˜ï¼š sï¼šæ€»æ•°æ®é‡ pï¼šæ€»å¡æ•° $p_{inter}$ï¼šServeræ•°é‡ $\\beta_{inter}$ã€$\\alpha_{inter}$ï¼šServeré—´é“¾è·¯å‚æ•° $\\beta_{intra}$ã€$\\alpha_{intra}$ï¼šServerå†…é“¾è·¯å‚æ•° 3.11 ç®—æ³•é€‰æ‹©ç­–ç•¥123456789101112131415161718192021222324252627282930313233343536graph TD Start[é›†åˆé€šä¿¡è¯·æ±‚] --&gt; CheckScope&#123;é€šä¿¡èŒƒå›´&#125; CheckScope --&gt;|Serverå†…| IntraServer[Serverå†…ç®—æ³•é€‰æ‹©] CheckScope --&gt;|Serveré—´| InterServer[Serveré—´ç®—æ³•é€‰æ‹©] IntraServer --&gt; CheckSize1&#123;æ•°æ®é‡&#125; CheckSize1 --&gt;|å°| Star[Starç®—æ³•] CheckSize1 --&gt;|å¤§| Mesh[Meshç®—æ³•] InterServer --&gt; CheckOp&#123;ç®—å­ç±»å‹&#125; CheckOp --&gt;|AllToAllç³»åˆ—| PairWise[PairWiseç®—æ³•] CheckOp --&gt;|å…¶ä»–| CheckNodes&#123;èŠ‚ç‚¹æ•°&#125; CheckNodes --&gt;|å°è§„æ¨¡| Ring[Ringç®—æ³•] CheckNodes --&gt;|å¤§è§„æ¨¡| CheckPower&#123;æ˜¯å¦2çš„å¹‚&#125; CheckPower --&gt;|æ˜¯| RHD[RHDç®—æ³•] CheckPower --&gt;|å¦| CheckData&#123;æ•°æ®é‡&#125; CheckData --&gt;|å°| RHD CheckData --&gt;|å¤§| NHR_NB&#123;NHRæˆ–NB&#125; CheckNodes --&gt;|å±‚æ¬¡åŒ–&lt;br/&gt;éå¯¹ç§°| AHC[AHCç®—æ³•] CheckSize1 --&gt;|å¤§æ•°æ®é‡&lt;br/&gt;å¤šæœºå¤šå¡| Pipeline[Pipelineç®—æ³•] style Star fill:#ffe6e6 style Mesh fill:#ffe6e6 style Ring fill:#e6f3ff style RHD fill:#e6f3ff style PairWise fill:#fff4e6 style NHR_NB fill:#e6ffe6 style AHC fill:#f3e6ff style Pipeline fill:#ffffcc 4. é›†åˆé€šä¿¡åŸè¯­4.1 æ ¸å¿ƒåŸè¯­åˆ—è¡¨ åŸè¯­ æè¿° ä¸»è¦ç®—æ³• AllReduce æ‰€æœ‰èŠ‚ç‚¹è§„çº¦åå¹¿æ’­ç»“æœ Mesh, Ring, RHD, NHR, NB, AHC, Pipeline AllGather æ‰€æœ‰èŠ‚ç‚¹æ”¶é›†å…¨éƒ¨æ•°æ® Mesh, Ring, RHD, NHR, NB, Pipeline ReduceScatter è§„çº¦åæ•£å‘åˆ°å„èŠ‚ç‚¹ Mesh, Ring, RHD, NHR, NB, AHC, Pipeline Broadcast æ ¹èŠ‚ç‚¹å‘æ‰€æœ‰èŠ‚ç‚¹å¹¿æ’­ Mesh, Ring, RHD, NHR, NB, Star Reduce æ‰€æœ‰èŠ‚ç‚¹å‘æ ¹èŠ‚ç‚¹è§„çº¦ Mesh, Ring, RHD, Star Scatter æ ¹èŠ‚ç‚¹æ•£å‘æ•°æ®åˆ°å„èŠ‚ç‚¹ Mesh, Ring, NHR, NB, Star Gather æ‰€æœ‰èŠ‚ç‚¹å‘æ ¹èŠ‚ç‚¹æ”¶é›† Mesh, Ring, Star AllToAll æ‰€æœ‰èŠ‚ç‚¹é—´å…¨äº¤æ¢ PairWise AllToAllV æ‰€æœ‰èŠ‚ç‚¹é—´å˜é•¿å…¨äº¤æ¢ PairWise 4.2 åŸè¯­è¯­ä¹‰è¯´æ˜123456789101112131415161718192021222324graph LR subgraph &quot;AllReduce&quot; AR1[R0:A0] --&gt; ARS[Sum] AR2[R1:A1] --&gt; ARS AR3[R2:A2] --&gt; ARS ARS --&gt; ARR1[R0:Sum] ARS --&gt; ARR2[R1:Sum] ARS --&gt; ARR3[R2:Sum] end subgraph &quot;AllGather&quot; AG1[R0:A0] --&gt; AGR1[R0:A0+A1+A2] AG2[R1:A1] --&gt; AGR2[R1:A0+A1+A2] AG3[R2:A2] --&gt; AGR3[R2:A0+A1+A2] end subgraph &quot;ReduceScatter&quot; RS1[R0:A0] --&gt; RSS[Sumåˆ‡åˆ†] RS2[R1:A1] --&gt; RSS RS3[R2:A2] --&gt; RSS RSS --&gt; RSR1[R0:Sum_part0] RSS --&gt; RSR2[R1:Sum_part1] RSS --&gt; RSR3[R2:Sum_part2] end 5. é€šä¿¡æ¡†æ¶è®¾è®¡5.1 é€šä¿¡åŸŸç®¡ç†12345678910111213141516171819202122232425262728classDiagram class Communicator &#123; -int commId -int rank -int size -CommunicatorType type +Create() +Destroy() +GetRank() +GetSize() &#125; class CommunicatorGroup &#123; -List~Communicator~ comms +CreateGroup() +SplitGroup() +FreeGroup() &#125; class RankInfo &#123; -int rankId -int deviceId -string hostName -NetworkInfo netInfo &#125; Communicator &quot;1&quot; --&gt; &quot;*&quot; RankInfo CommunicatorGroup &quot;1&quot; --&gt; &quot;*&quot; Communicator 5.2 ç®—å­æ‰§è¡Œæµç¨‹1234567891011121314151617181920212223242526272829sequenceDiagram participant User as ç”¨æˆ·è°ƒç”¨ participant Framework as é€šä¿¡æ¡†æ¶ participant AlgoSelector as ç®—æ³•é€‰æ‹©å™¨ participant Algorithm as é€šä¿¡ç®—æ³• participant Platform as é€šä¿¡å¹³å° User-&gt;&gt;Framework: HcclAllReduce(...) Framework-&gt;&gt;Framework: å‚æ•°æ ¡éªŒ Framework-&gt;&gt;AlgoSelector: è¯·æ±‚ç®—æ³•é€‰æ‹© AlgoSelector-&gt;&gt;AlgoSelector: åˆ†æé€šä¿¡åŸŸä¿¡æ¯ AlgoSelector-&gt;&gt;AlgoSelector: è¯„ä¼°æ•°æ®é‡ AlgoSelector-&gt;&gt;AlgoSelector: åº”ç”¨é€‰æ‹©ç­–ç•¥ AlgoSelector--&gt;&gt;Framework: è¿”å›ç®—æ³•ç±»å‹ Framework-&gt;&gt;Algorithm: æ‰§è¡Œç®—æ³• Algorithm-&gt;&gt;Algorithm: è®¡ç®—èµ„æºéœ€æ±‚ Algorithm-&gt;&gt;Algorithm: ç”Ÿæˆä»»åŠ¡ç¼–æ’ Algorithm-&gt;&gt;Platform: ç”³è¯·èµ„æº Platform--&gt;&gt;Algorithm: è¿”å›èµ„æºå¥æŸ„ Algorithm-&gt;&gt;Platform: ä¸‹å‘ä»»åŠ¡ Platform-&gt;&gt;Platform: æ‰§è¡Œé€šä¿¡ Platform--&gt;&gt;Algorithm: è¿”å›æ‰§è¡Œç»“æœ Algorithm--&gt;&gt;Framework: ç®—æ³•æ‰§è¡Œå®Œæˆ Framework--&gt;&gt;User: è¿”å›ç»“æœ 5.3 ç®—æ³•é€‰æ‹©å™¨è®¾è®¡123456789101112131415161718192021222324252627graph TB Input[è¾“å…¥å‚æ•°] --&gt; Analyzer[å‚æ•°åˆ†æå™¨] Analyzer --&gt; Topo[æ‹“æ‰‘ä¿¡æ¯åˆ†æ] Analyzer --&gt; Data[æ•°æ®é‡åˆ†æ] Analyzer --&gt; Op[ç®—å­ç±»å‹åˆ†æ] Topo --&gt; Rules[é€‰æ‹©è§„åˆ™åº“] Data --&gt; Rules Op --&gt; Rules Rules --&gt; Model[æ€§èƒ½æ¨¡å‹è¯„ä¼°] Model --&gt; Decision[å†³ç­–å¼•æ“] Decision --&gt; Output[è¾“å‡ºç®—æ³•] subgraph &quot;è§„åˆ™åº“&quot; R1[æ‹“æ‰‘è§„åˆ™] R2[æ•°æ®é‡è§„åˆ™] R3[ç®—å­è§„åˆ™] R4[å†å²ä¼˜åŒ–] end Rules -.åŒ…å«.-&gt; R1 Rules -.åŒ…å«.-&gt; R2 Rules -.åŒ…å«.-&gt; R3 Rules -.åŒ…å«.-&gt; R4 6. ç¼–è¯‘ä¸æ„å»ºç³»ç»Ÿ6.1 ç¼–è¯‘æµç¨‹1234567891011121314151617181920212223graph TB Start[å¼€å§‹ç¼–è¯‘] --&gt; CheckEnv&#123;æ£€æŸ¥ç¯å¢ƒ&#125; CheckEnv --&gt;|CANNå·²å®‰è£…| CheckDeps[æ£€æŸ¥ä¾èµ–] CheckEnv --&gt;|æœªå®‰è£…| Error1[æŠ¥é”™é€€å‡º] CheckDeps --&gt; CMake[CMakeé…ç½®] CMake --&gt; Gen[ç”Ÿæˆæ„å»ºæ–‡ä»¶] Gen --&gt; CompileKernel&#123;ç¼–è¯‘æ¨¡å¼&#125; CompileKernel --&gt;|--aicpu| Kernel[ç¼–è¯‘ccl_kernel.so] CompileKernel --&gt;|é»˜è®¤| Full[å®Œæ•´ç¼–è¯‘] Kernel --&gt; InstallK[å®‰è£…Kernel] Full --&gt; CompileFramework[ç¼–è¯‘é€šä¿¡æ¡†æ¶] CompileFramework --&gt; CompileAlgo[ç¼–è¯‘é€šä¿¡ç®—æ³•] CompileAlgo --&gt; Package[æ‰“åŒ….runæ–‡ä»¶] Package --&gt; Output[ç”Ÿæˆè¾“å‡º] InstallK --&gt; Output Output --&gt; End[ç¼–è¯‘å®Œæˆ] 6.2 ä¸»è¦ç¼–è¯‘é€‰é¡¹1234567891011121314151617# åŸºç¡€ç¼–è¯‘bash build.sh --nlohmann_path /path/to/nlohmann/include# ä»…ç¼–è¯‘AICPU Kernelbash build.sh --nlohmann_path /path --aicpu# ç¼–è¯‘å¹¶è¿è¡Œæµ‹è¯•bash build.sh --nlohmann_path /path --test# ä½¿èƒ½åœ°å€æ¶ˆæ¯’å™¨ï¼ˆç”¨äºå†…å­˜æ£€æµ‹ï¼‰bash build.sh --nlohmann_path /path --asan# ä½¿èƒ½ä»£ç è¦†ç›–ç‡bash build.sh --nlohmann_path /path --cov# æŒ‡å®šCANNåŒ…è·¯å¾„bash build.sh --nlohmann_path /path -p /usr/local/Ascend/ascend-toolkit/latest 6.3 ä¾èµ–å…³ç³»12345678910111213141516171819graph TB HCCL[HCCLåº“] --&gt; CANN[CANNå¼€å‘å¥—ä»¶åŒ…] HCCL --&gt; SDK[CANN SDKåŒ…] HCCL --&gt; JSON[nlohmann/json] CANN --&gt; Runtime[CANN Runtime] CANN --&gt; Driver[NPUé©±åŠ¨] CANN --&gt; Firmware[NPUå›ºä»¶] SDK --&gt; GTest[Google Test] Build[æ„å»ºç³»ç»Ÿ] --&gt; Python[Python â‰¥ 3.7] Build --&gt; GCC[GCC â‰¥ 7.3] Build --&gt; CMake[CMake â‰¥ 3.16] style HCCL fill:#ff6b6b style CANN fill:#4ecdc4 style SDK fill:#95e1d3 style JSON fill:#f38181 7. æµ‹è¯•ä¸éªŒè¯7.1 æµ‹è¯•åˆ†å±‚æ¶æ„1234567891011121314151617181920212223242526272829303132333435graph TB subgraph &quot;å•å…ƒæµ‹è¯•å±‚&quot; UT1[ç®—æ³•å•å…ƒæµ‹è¯•] UT2[æ¡†æ¶å•å…ƒæµ‹è¯•] UT3[å·¥å…·ç±»å•å…ƒæµ‹è¯•] end subgraph &quot;é›†æˆæµ‹è¯•å±‚&quot; IT1[ç®—å­é›†æˆæµ‹è¯•] IT2[å¤šç®—æ³•è”åˆæµ‹è¯•] IT3[å¼‚å¸¸åœºæ™¯æµ‹è¯•] end subgraph &quot;ç³»ç»Ÿæµ‹è¯•å±‚&quot; ST1[å•æœºå¤šå¡æµ‹è¯•] ST2[å¤šæœºå¤šå¡æµ‹è¯•] ST3[æ€§èƒ½åŸºå‡†æµ‹è¯•] end subgraph &quot;å·¥å…·æµ‹è¯•å±‚&quot; TT1[HCCL Testå·¥å…·] TT2[æ€§èƒ½åˆ†æå·¥å…·] end UT1 --&gt; IT1 UT2 --&gt; IT2 UT3 --&gt; IT3 IT1 --&gt; ST1 IT2 --&gt; ST2 IT3 --&gt; ST3 ST1 --&gt; TT1 ST2 --&gt; TT1 ST3 --&gt; TT2 7.2 HCCL Test å·¥å…·åŠŸèƒ½æµ‹è¯•ç¤ºä¾‹ï¼š 12# 8å¡AllReduceåŠŸèƒ½æµ‹è¯•mpirun -n 8 ./bin/all_reduce_test -b 8K -e 64M -f 2 -d fp32 -o sum -p 8 å‚æ•°è¯´æ˜ï¼š -b: èµ·å§‹æ•°æ®é‡ï¼ˆ8KBï¼‰ -e: ç»“æŸæ•°æ®é‡ï¼ˆ64MBï¼‰ -f: å¢é‡ç³»æ•°ï¼ˆæ¯æ¬¡2å€ï¼‰ -d: æ•°æ®ç±»å‹ï¼ˆfp32ï¼‰ -o: è§„çº¦æ“ä½œï¼ˆsumï¼‰ -p: å‚ä¸NPUæ•°é‡ï¼ˆ8ï¼‰ è¾“å‡ºæŒ‡æ ‡ï¼š check_result: åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆsuccess&#x2F;failï¼‰ aveg_time: å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆå¾®ç§’ï¼‰ alg_bandwidth: ç®—æ³•å¸¦å®½ï¼ˆGB&#x2F;sï¼‰ data_size: å•NPUæ•°æ®é‡ï¼ˆBytesï¼‰ 7.3 LLTæµ‹è¯•å‘½ä»¤1234567891011# è¿è¡Œæ‰€æœ‰LLTæµ‹è¯•sh build.sh --nlohmann_path /path/to/nlohmann/include --test# è¿è¡Œç‰¹å®šæµ‹è¯•å¥—ä»¶sh build.sh --nlohmann_path /path --open_hccl_testsh build.sh --nlohmann_path /path --executor_hccl_testsh build.sh --nlohmann_path /path --executor_reduce_hccl_testsh build.sh --nlohmann_path /path --executor_pipeline_hccl_test# ä½¿èƒ½å†…å­˜æ£€æµ‹sh build.sh --nlohmann_path /path --test --asan 8. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥8.1 ç®—æ³•çº§ä¼˜åŒ–123456789101112131415161718mindmap root((æ€§èƒ½ä¼˜åŒ–)) ç®—æ³•é€‰æ‹© æ‹“æ‰‘æ„ŸçŸ¥ æ•°æ®é‡è‡ªé€‚åº” è´Ÿè½½å‡è¡¡ å†…å­˜ä¼˜åŒ– é›¶æ‹·è´æŠ€æœ¯ å†…å­˜æ± ç®¡ç† DMAç›´æ¥è®¿é—® å¹¶å‘ä¼˜åŒ– æµæ°´çº¿å¹¶è¡Œ å¤šæµå¹¶å‘ å¼‚æ­¥æ‰§è¡Œ ç½‘ç»œä¼˜åŒ– æ‹¥å¡æ§åˆ¶ æµé‡è°ƒåº¦ QoSä¿è¯ 8.2 å…³é”®æ€§èƒ½æŒ‡æ ‡ æŒ‡æ ‡ è¯´æ˜ ç›®æ ‡ å¸¦å®½åˆ©ç”¨ç‡ å®é™…å¸¦å®½&#x2F;ç†è®ºå¸¦å®½ &gt; 90% é€šä¿¡å»¶è¿Ÿ ç«¯åˆ°ç«¯é€šä¿¡æ—¶é—´ æœ€å°åŒ– å¯æ‰©å±•æ€§ èŠ‚ç‚¹æ•°å¢åŠ æ—¶çš„æ€§èƒ½ä¿æŒ æ¥è¿‘çº¿æ€§ è´Ÿè½½å‡è¡¡ å„èŠ‚ç‚¹è´Ÿè½½æ–¹å·® &lt; 10% å†…å­˜å¼€é”€ é¢å¤–å†…å­˜æ¶ˆè€— &lt; 20% 8.3 æ€§èƒ½è°ƒä¼˜å‚æ•°ç¯å¢ƒå˜é‡ï¼š 123456789101112# ç®—æ³•é€‰æ‹©ç­–ç•¥export HCCL_ALGO=&lt;algo_name&gt;# æµæ°´çº¿æ·±åº¦export HCCL_PIPELINE_DEPTH=&lt;depth&gt;# å¹¶å‘æµæ•°é‡export HCCL_STREAM_NUM=&lt;num&gt;# æ—¥å¿—çº§åˆ«export ASCEND_SLOG_PRINT_TO_STDOUT=1export ASCEND_GLOBAL_LOG_LEVEL=&lt;level&gt; 9. å®‰å…¨ä¸å¯é æ€§9.1 å®‰å…¨æªæ–½12345678910111213141516171819202122232425262728293031323334353637383940graph TB subgraph &quot;ç¼–è¯‘å®‰å…¨&quot; S1[æ ˆä¿æŠ¤ -fstack-protector] S2[ä½ç½®æ— å…³ä»£ç  -fPIC] S3[RELROä¿æŠ¤] S4[NXä¿æŠ¤] end subgraph &quot;è¿è¡Œå®‰å…¨&quot; R1[è¾“å…¥å‚æ•°æ ¡éªŒ] R2[å†…å­˜è¾¹ç•Œæ£€æŸ¥] R3[èµ„æºæ³„æ¼æ£€æµ‹] R4[å¼‚å¸¸æ•è·ä¸å¤„ç†] end subgraph &quot;é€šä¿¡å®‰å…¨&quot; C1[ç«¯å£è®¤è¯] C2[æ•°æ®å®Œæ•´æ€§æ ¡éªŒ] C3[è®¿é—®æ§åˆ¶] end S1 --&gt; Build[æ„å»ºäº§ç‰©] S2 --&gt; Build S3 --&gt; Build S4 --&gt; Build Build --&gt; Deploy[éƒ¨ç½²] R1 --&gt; Runtime[è¿è¡Œæ—¶] R2 --&gt; Runtime R3 --&gt; Runtime R4 --&gt; Runtime Deploy --&gt; Runtime C1 --&gt; Comm[é€šä¿¡å±‚] C2 --&gt; Comm C3 --&gt; Comm Runtime --&gt; Comm 9.2 é”™è¯¯å¤„ç†æœºåˆ¶12345678910111213141516171819stateDiagram-v2 [*] --&gt; Normal: åˆå§‹åŒ– Normal --&gt; DetectError: æ£€æµ‹åˆ°é”™è¯¯ DetectError --&gt; Classify: åˆ†ç±»é”™è¯¯ Classify --&gt; Recoverable: å¯æ¢å¤é”™è¯¯ Classify --&gt; Fatal: è‡´å‘½é”™è¯¯ Recoverable --&gt; Retry: é‡è¯•æœºåˆ¶ Retry --&gt; Success: é‡è¯•æˆåŠŸ Retry --&gt; Fallback: é‡è¯•å¤±è´¥ Fallback --&gt; Alternative: åˆ‡æ¢å¤‡ç”¨æ–¹æ¡ˆ Alternative --&gt; Normal Success --&gt; Normal Fatal --&gt; Log: è®°å½•æ—¥å¿— Log --&gt; Cleanup: èµ„æºæ¸…ç† Cleanup --&gt; [*]: é€€å‡º 9.3 æ—¥å¿—ä¸è°ƒè¯•æ—¥å¿—çº§åˆ«ï¼š ERROR: é”™è¯¯ä¿¡æ¯ WARNING: è­¦å‘Šä¿¡æ¯ INFO: ä¸€èˆ¬ä¿¡æ¯ DEBUG: è°ƒè¯•ä¿¡æ¯ å…³é”®æ—¥å¿—ç‚¹ï¼š é€šä¿¡åŸŸåˆ›å»º&#x2F;é”€æ¯ ç®—æ³•é€‰æ‹©å†³ç­– èµ„æºç”³è¯·&#x2F;é‡Šæ”¾ ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ 10. ç‰ˆæœ¬ç®¡ç†ä¸å…¼å®¹æ€§10.1 ç‰ˆæœ¬ç­–ç•¥1234567891011timeline title HCCLç‰ˆæœ¬æ¼”è¿› section CANN 8.x æ ‡ç­¾v8.0.0 : åŸºç¡€ç®—æ³• : Mesh/Ring/RHD æ ‡ç­¾v8.0.1 : æ–°å¢PairWise/Star section CANN 9.x æ ‡ç­¾v9.0.0 : æ–°å¢NHR/NB æ ‡ç­¾v9.0.1 : æ–°å¢AHC section CANN 10.x æ ‡ç­¾v10.0.0 : Pipelineä¼˜åŒ– æ ‡ç­¾v10.0.1 : æ€§èƒ½ä¼˜åŒ– 10.2 å…¼å®¹æ€§çŸ©é˜µ HCCLç‰ˆæœ¬ CANNç‰ˆæœ¬ å›ºä»¶ç‰ˆæœ¬ æ”¯æŒç¡¬ä»¶ v8.0.x 8.0.x å¯¹åº”CANN Atlas 800&#x2F;900 v9.0.x 9.0.x å¯¹åº”CANN Atlas 800&#x2F;900 v10.0.x 10.0.x å¯¹åº”CANN Atlas 800&#x2F;900&#x2F;æ–°ç¡¬ä»¶ 10.3 å‡çº§ä¸å›æ»š12345# å®‰è£…è‡ªå®šä¹‰HCCLåŒ…./CANN-hccl_alg-&lt;version&gt;-linux.&lt;arch&gt;.run# å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬./CANN-hccl_alg-&lt;version&gt;-linux.&lt;arch&gt;.run --rollback æ³¨æ„ï¼š å›æ»šä»…æ”¯æŒå›é€€åˆ°ä¸Šä¸€æ¬¡å®‰è£…çš„ç‰ˆæœ¬çŠ¶æ€ã€‚ 11. æ‰©å±•ä¸å®šåˆ¶å¼€å‘11.1 æ–°ç®—æ³•é›†æˆæµç¨‹1234567891011121314151617graph TB Start[å¼€å§‹] --&gt; Design[ç®—æ³•è®¾è®¡] Design --&gt; Implement[å®ç°ç®—æ³•ç±»] Implement --&gt; Interface[å®ç°ç®—æ³•æ¥å£] Interface --&gt; Resource[èµ„æºè®¡ç®—é€»è¾‘] Resource --&gt; Schedule[ä»»åŠ¡ç¼–æ’é€»è¾‘] Schedule --&gt; Register[æ³¨å†Œåˆ°æ¡†æ¶] Register --&gt; Selector[æ›´æ–°é€‰æ‹©å™¨è§„åˆ™] Selector --&gt; UnitTest[å•å…ƒæµ‹è¯•] UnitTest --&gt; IntegTest[é›†æˆæµ‹è¯•] IntegTest --&gt; PerfTest[æ€§èƒ½æµ‹è¯•] PerfTest --&gt; Doc[æ–‡æ¡£ç¼–å†™] Doc --&gt; End[å®Œæˆ] 11.2 ç®—æ³•æ¥å£è§„èŒƒ123456789101112131415161718// ä¼ªä»£ç ç¤ºä¾‹class CollectiveAlgorithm &#123;public: // åˆå§‹åŒ–ç®—æ³• virtual Status Init(const AlgorithmConfig&amp; config) = 0; // è®¡ç®—èµ„æºéœ€æ±‚ virtual Status CalculateResource(ResourceInfo&amp; resource) = 0; // ç”Ÿæˆä»»åŠ¡ç¼–æ’ virtual Status GenerateTaskSchedule(TaskSchedule&amp; schedule) = 0; // æ‰§è¡Œç®—æ³• virtual Status Execute(const ExecuteContext&amp; context) = 0; // æ¸…ç†èµ„æº virtual Status Cleanup() = 0;&#125;; 11.3 è´¡çŒ®æŒ‡å— Issueè®¨è®ºï¼š æ–°ç‰¹æ€§éœ€å…ˆé€šè¿‡Issueè®¨è®ºæ–¹æ¡ˆ CLAç­¾ç½²ï¼š é¦–æ¬¡è´¡çŒ®éœ€ç­¾ç½²CLAåè®® ä»£ç è§„èŒƒï¼š éµå¾ªé¡¹ç›®ä»£ç è§„èŒƒ æµ‹è¯•è¦†ç›–ï¼š æä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯• æ–‡æ¡£æ›´æ–°ï¼š åŒæ­¥æ›´æ–°ç›¸å…³æ–‡æ¡£ PRæ¨¡æ¿ï¼š æŒ‰æ¨¡æ¿å¡«å†™PRä¿¡æ¯ 12. å‚è€ƒèµ„æ–™12.1 å®˜æ–¹æ–‡æ¡£ é›†åˆé€šä¿¡ç”¨æˆ·æŒ‡å— é›†åˆé€šä¿¡æºç å®šåˆ¶å¼€å‘æŒ‡å— ç¯å¢ƒå˜é‡å‚è€ƒ HCCLæ€§èƒ½æµ‹è¯•å·¥å…·ç”¨æˆ·æŒ‡å— 12.2 æŠ€æœ¯æ–‡ç«  HCCLâ€”æ˜‡è…¾é«˜æ€§èƒ½é›†åˆé€šä¿¡åº“ç®€ä»‹ HCCLé›†åˆé€šä¿¡ç®—æ³•å¼€å‘Hello Worldç¤ºä¾‹ HCCLé›†åˆé€šä¿¡å¸¸è§é—®é¢˜å®šä½æ€è·¯ æ·±åº¦å­¦ä¹ çš„åˆ†å¸ƒå¼è®­ç»ƒä¸é›†åˆé€šä¿¡ï¼ˆä¸€ï¼‰ æ·±åº¦å­¦ä¹ çš„åˆ†å¸ƒå¼è®­ç»ƒä¸é›†åˆé€šä¿¡ï¼ˆäºŒï¼‰ 12.3 åŸ¹è®­è§†é¢‘ æ˜‡è…¾é›†åˆé€šä¿¡ç³»åˆ—æ•™ç¨‹â€”â€”ä»€ä¹ˆæ˜¯HCCL æ˜‡è…¾é›†åˆé€šä¿¡ç³»åˆ—æ•™ç¨‹â€”â€”å¸¸è§é›†åˆé€šä¿¡åŸè¯­ æ˜‡è…¾é›†åˆé€šä¿¡ç³»åˆ—æ•™ç¨‹â€”â€”é›†åˆé€šä¿¡å…¸å‹ç®—æ³• HCCLè®¾è®¡åŸç†å’Œå®ç°ç³»åˆ— 12.4 æ€§èƒ½åŸºå‡†å…¸å‹åœºæ™¯æ€§èƒ½ï¼ˆä»¥AllReduceä¸ºä¾‹ï¼‰ï¼š èŠ‚ç‚¹æ•° æ•°æ®é‡ ç®—æ³• å¸¦å®½åˆ©ç”¨ç‡ å»¶è¿Ÿ 8 (å•æœº) 1GB Mesh &gt;95% &lt;1ms 16 (2æœº) 1GB Pipeline &gt;90% &lt;2ms 64 (8æœº) 1GB NHR &gt;85% &lt;5ms 128 (16æœº) 1GB NB &gt;80% &lt;10ms 13. æ€»ç»“13.1 æ ¸å¿ƒä¼˜åŠ¿ ä¸°å¯Œçš„ç®—æ³•åº“ï¼š9ç§ç®—æ³•è¦†ç›–å„ç§åœºæ™¯ æ™ºèƒ½ç®—æ³•é€‰æ‹©ï¼šåŸºäºÎ±-Î²æ¨¡å‹çš„æ€§èƒ½è¯„ä¼° å±‚æ¬¡åŒ–è®¾è®¡ï¼šæ¸…æ™°çš„ä¸‰å±‚æ¶æ„ é«˜æ€§èƒ½å®ç°ï¼šå……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§ å¼€æ”¾å¯æ‰©å±•ï¼šæ”¯æŒè‡ªå®šä¹‰ç®—æ³•å¼€å‘ 13.2 åº”ç”¨åœºæ™¯123456789101112131415mindmap root((HCCLåº”ç”¨)) æ·±åº¦å­¦ä¹  åˆ†å¸ƒå¼è®­ç»ƒ æ•°æ®å¹¶è¡Œ æ¨¡å‹å¹¶è¡Œ æµæ°´çº¿å¹¶è¡Œ é«˜æ€§èƒ½è®¡ç®— ç§‘å­¦è®¡ç®— å¤§è§„æ¨¡ä»¿çœŸ å›¾è®¡ç®— å¤§æ•°æ® åˆ†å¸ƒå¼å¤„ç† MapReduce Sparké›†æˆ 13.3 æœªæ¥å±•æœ› ç®—æ³•ä¼˜åŒ–ï¼šæŒç»­ä¼˜åŒ–ç°æœ‰ç®—æ³•æ€§èƒ½ æ–°ç®—æ³•å¼•å…¥ï¼šå¼•å…¥æ›´å¤šå…ˆè¿›ç®—æ³• æ™ºèƒ½è°ƒåº¦ï¼šåŸºäºAIçš„ç®—æ³•é€‰æ‹© å¼‚æ„æ”¯æŒï¼šæ”¯æŒæ›´å¤šç¡¬ä»¶å¹³å° ç”Ÿæ€å»ºè®¾ï¼šä¸æ›´å¤šæ¡†æ¶æ·±åº¦é›†æˆ","categories":[{"name":"ç³»ç»Ÿæ¶æ„åˆ†æ","slug":"ç³»ç»Ÿæ¶æ„åˆ†æ","permalink":"https://nash635.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/"}],"tags":[{"name":"HCCL","slug":"HCCL","permalink":"https://nash635.github.io/tags/HCCL/"},{"name":"é›†åˆé€šä¿¡","slug":"é›†åˆé€šä¿¡","permalink":"https://nash635.github.io/tags/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1/"},{"name":"åä¸º","slug":"åä¸º","permalink":"https://nash635.github.io/tags/%E5%8D%8E%E4%B8%BA/"},{"name":"æ˜‡è…¾","slug":"æ˜‡è…¾","permalink":"https://nash635.github.io/tags/%E6%98%87%E8%85%BE/"},{"name":"AllReduce","slug":"AllReduce","permalink":"https://nash635.github.io/tags/AllReduce/"}]},{"title":"DeepEP æ¶æ„åˆ†æ","slug":"DeepEP_DeepDive","date":"2025-11-17T02:00:00.000Z","updated":"2025-11-17T14:42:49.955Z","comments":true,"path":"2025/11/17/DeepEP_DeepDive/","permalink":"https://nash635.github.io/2025/11/17/DeepEP_DeepDive/","excerpt":"","text":"DeepEP æ¶æ„åˆ†ææ–‡æ¡£1. é¡¹ç›®æ¦‚è¿°DeepEP æ˜¯ä¸€ä¸ªä¸“ä¸ºæ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)å’Œä¸“å®¶å¹¶è¡Œ(Expert Parallelism, EP)è®¾è®¡çš„é«˜æ€§èƒ½é€šä¿¡åº“ã€‚å®ƒæä¾›äº†é«˜ååé‡å’Œä½å»¶è¿Ÿçš„All-to-All GPUå†…æ ¸ï¼Œä¸“é—¨ä¼˜åŒ–äº†MoEæ¨¡å‹ä¸­çš„dispatchå’Œcombineæ“ä½œã€‚ è¯¥é¡¹ç›®ç”±DeepSeekå›¢é˜Ÿå¼€å‘ï¼Œæ˜¯æ”¯æ’‘DeepSeek-V3å¤§è§„æ¨¡MoEè®­ç»ƒå’Œæ¨ç†çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚ æ ¸å¿ƒç‰¹æ€§ é«˜ååé‡å†…æ ¸: æ”¯æŒNVLinkåŸŸåˆ°RDMAåŸŸçš„éå¯¹ç§°å¸¦å®½è½¬å‘ ä½å»¶è¿Ÿå†…æ ¸: ä¸»è¦ä½¿ç”¨RDMAé€šä¿¡ï¼Œé€‚ç”¨äºæ¨ç†è§£ç ä»»åŠ¡ï¼ˆå¯é…ç½®æ˜¯å¦ä½¿ç”¨NVLinkåŠ é€Ÿï¼‰ å¤šç²¾åº¦æ”¯æŒ: æ”¯æŒFP8ã€BF16ç­‰ä½ç²¾åº¦æ“ä½œ é€šä¿¡è®¡ç®—é‡å : åŸºäºhookçš„æ–¹æ³•ï¼Œä¸å ç”¨SMèµ„æº å¯æ‰©å±•æ€§: æ”¯æŒèŠ‚ç‚¹å†…(NVLink)å’ŒèŠ‚ç‚¹é—´(RDMA)é€šä¿¡ æ€§èƒ½æŒ‡æ ‡èŠ‚ç‚¹å†…é€šä¿¡ (H800, ~160 GB&#x2F;s NVLink): 8ä¸ªEP ranks: ~153-158 GB&#x2F;s èŠ‚ç‚¹é—´é€šä¿¡ (H800 + CX7 IB 400Gb&#x2F;s, ~50 GB&#x2F;s RDMA): 16-64ä¸ªEP ranks: ~43-58 GB&#x2F;s ä½å»¶è¿Ÿæ¨¡å¼: 8 EP ranks: 77-114 us 256 EP ranks: 194-360 us ä¾èµ–çš„å…³é”®æŠ€æœ¯DeepEP æ„å»ºåœ¨å¤šä¸ªå…ˆè¿›çš„GPUé€šä¿¡æŠ€æœ¯ä¹‹ä¸Šï¼š 1. NVSHMEM (NVIDIA Symmetric Memory)NVSHMEM æ˜¯ NVIDIA æä¾›çš„åˆ†å¸ƒå¼GPUå†…å­˜è®¿é—®åº“ï¼Œå®ç°äº† OpenSHMEM æ ‡å‡†çš„ GPU æ‰©å±•ã€‚ æ ¸å¿ƒèƒ½åŠ›: å¯¹ç§°å†…å­˜æ¨¡å‹: æ‰€æœ‰GPUå¯ä»¥ç›´æ¥è®¿é—®å½¼æ­¤çš„æ˜¾å­˜ï¼Œæ— éœ€CPUå‚ä¸ å•è¾¹é€šä¿¡: æ”¯æŒ PUT&#x2F;GET æ“ä½œï¼Œå‘èµ·æ–¹å¯ä»¥ç›´æ¥è¯»å†™è¿œç«¯GPUå†…å­˜ é›†åˆé€šä¿¡: æä¾› barrierã€broadcastã€reduction ç­‰åŸè¯­ å¤šä¼ è¾“æ”¯æŒ: åŒæ—¶æ”¯æŒ NVLink (èŠ‚ç‚¹å†…) å’Œ InfiniBand RDMA (èŠ‚ç‚¹é—´) åœ¨DeepEPä¸­çš„åº”ç”¨: èŠ‚ç‚¹é—´æ•°æ®ä¼ è¾“çš„åº•å±‚å®ç° ä½å»¶è¿Ÿæ¨¡å¼çš„æ ¸å¿ƒé€šä¿¡æœºåˆ¶ RDMA buffer çš„å¯¹ç§°å†…å­˜ç®¡ç† 2. IBGDA (InfiniBand GPU Direct Async)IBGDA æ˜¯ NVIDIA ä¸ Mellanox åˆä½œå¼€å‘çš„æŠ€æœ¯ï¼Œå…è®¸ GPU ç›´æ¥å‘èµ· RDMA æ“ä½œã€‚ æ ¸å¿ƒèƒ½åŠ›: é›¶CPUå¼€é”€: GPU å¯ä»¥ç›´æ¥æ“ä½œ InfiniBand HCA (Host Channel Adapter) å¤šQPå¹¶è¡Œ: æ”¯æŒæ¯ä¸ªGPUä½¿ç”¨å¤šä¸ªQueue PairåŒæ—¶ä¼ è¾“ ä½å»¶è¿Ÿ: ç»•è¿‡CPUï¼Œå‡å°‘PCIeå¾€è¿”å»¶è¿Ÿ åœ¨DeepEPä¸­çš„åº”ç”¨: èŠ‚ç‚¹é—´dispatch&#x2F;combineçš„é«˜æ€§èƒ½æ•°æ®ä¼ è¾“ ä½å»¶è¿Ÿæ¨¡å¼çš„å¿«é€ŸRDMAæ“ä½œ å¤šQPå¹¶è¡Œä»¥æå‡å¸¦å®½åˆ©ç”¨ç‡ 3. NVLinkNVIDIA çš„é«˜é€ŸGPUäº’è”æŠ€æœ¯ï¼Œæä¾›èŠ‚ç‚¹å†…GPUä¹‹é—´çš„ç›´æ¥è¿æ¥ã€‚ æ ¸å¿ƒèƒ½åŠ›: é«˜å¸¦å®½: H800å•å‘å¸¦å®½ ~160 GB&#x2F;s per GPU ä½å»¶è¿Ÿ: æ¯”PCIeå»¶è¿Ÿä½ä¸€ä¸ªæ•°é‡çº§ Peer-to-Peer: GPUä¹‹é—´å¯ä»¥ç›´æ¥è®¿é—®å½¼æ­¤çš„æ˜¾å­˜ åœ¨DeepEPä¸­çš„åº”ç”¨: èŠ‚ç‚¹å†… dispatch&#x2F;combine çš„ä¸»è¦ä¼ è¾“è·¯å¾„ èŠ‚ç‚¹é—´é€šä¿¡çš„æœ¬åœ°èšåˆ&#x2F;åˆ†å‘ ä½å»¶è¿Ÿæ¨¡å¼çš„å¯é€‰åŠ é€Ÿè·¯å¾„ 4. CUDA IPC (Inter-Process Communication)CUDAæä¾›çš„è¿›ç¨‹é—´å…±äº«GPUå†…å­˜æœºåˆ¶ã€‚ æ ¸å¿ƒèƒ½åŠ›: å†…å­˜å…±äº«: ä¸åŒè¿›ç¨‹å¯ä»¥è®¿é—®åŒä¸€å—GPUå†…å­˜ é›¶æ‹·è´: é€šè¿‡å¥æŸ„ (handle) æ˜ å°„ï¼Œé¿å…æ•°æ®å¤åˆ¶ åœ¨DeepEPä¸­çš„åº”ç”¨: èŠ‚ç‚¹å†…å¤šè¿›ç¨‹çš„bufferå…±äº« Barrierä¿¡å·çš„å…±äº«å†…å­˜å®ç° 5. CUDA Fabric API (å¯é€‰)æ–°ä¸€ä»£GPUå†…å­˜ç®¡ç†APIï¼Œæ”¯æŒæ›´çµæ´»çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚ æ ¸å¿ƒèƒ½åŠ›: ç»Ÿä¸€å¯»å€: æä¾›è·¨GPUçš„ç»Ÿä¸€è™šæ‹Ÿåœ°å€ç©ºé—´ ç»†ç²’åº¦æ§åˆ¶: æ›´å¥½çš„å†…å­˜è®¿é—®æƒé™ç®¡ç† åœ¨DeepEPä¸­çš„åº”ç”¨: ä½œä¸ºCUDA IPCçš„æ›¿ä»£æ–¹æ¡ˆ æ”¯æŒæ›´å¤§è§„æ¨¡çš„GPUé›†ç¾¤ 6. TMA (Tensor Memory Accelerator, SM90+)Hopperæ¶æ„å¼•å…¥çš„ç¡¬ä»¶åŠ é€Ÿå†…å­˜æ‹·è´å•å…ƒã€‚ æ ¸å¿ƒèƒ½åŠ›: ç¡¬ä»¶åŠ é€Ÿ: ä¸“ç”¨ç¡¬ä»¶å•å…ƒå¤„ç†å¼ é‡æ‹·è´ é«˜å¸¦å®½: æ›´é«˜æ•ˆåœ°åˆ©ç”¨æ˜¾å­˜å¸¦å®½ å¼‚æ­¥æ‰§è¡Œ: ä¸å ç”¨SMè®¡ç®—èµ„æº åœ¨DeepEPä¸­çš„åº”ç”¨: H100&#x2F;H800ä¸Šçš„å‘é‡åŒ–æ•°æ®ä¼ è¾“ é«˜åådispatch&#x2F;combineä¼˜åŒ– 2. ç³»ç»Ÿæ¶æ„2.1 æ•´ä½“æ¶æ„å›¾123456789101112131415161718192021222324252627282930313233343536373839404142434445graph TB subgraph PythonAPI[&quot; Python API å±‚ &quot;] Buffer[&quot;Buffer&quot;] EventOverlap[&quot;EventOverlap&quot;] Config[&quot;Config&quot;] end subgraph CppRuntime[&quot; C++ Runtime å±‚ &quot;] BufferMgmt[&quot;Bufferç®¡ç†&quot;] MemAlloc[&quot;å†…å­˜åˆ†é…&quot;] ProcSync[&quot;è¿›ç¨‹åŒæ­¥&quot;] EventMgmt[&quot;äº‹ä»¶ç®¡ç†&quot;] end subgraph CUDAKernel[&quot; CUDA Kernel å±‚ &quot;] direction LR Intranode[&quot;Intranode&lt;br/&gt;èŠ‚ç‚¹å†…é€šä¿¡&quot;] Internode[&quot;Internode&lt;br/&gt;èŠ‚ç‚¹é—´é€šä¿¡&quot;] LowLatency[&quot;Internode LL&lt;br/&gt;ä½å»¶è¿Ÿæ¨¡å¼&quot;] end subgraph Hardware[&quot; ç¡¬ä»¶é€šä¿¡å±‚ &quot;] direction LR NVLink[&quot;NVLink&quot;] NVSHMEM[&quot;NVSHMEM&quot;] IBGDA[&quot;IBGDA&quot;] end Buffer -.&quot;PyBind11&quot;.-&gt; BufferMgmt EventOverlap -.&quot;PyBind11&quot;.-&gt; EventMgmt Config -.&quot;PyBind11&quot;.-&gt; BufferMgmt BufferMgmt --&gt; Intranode MemAlloc --&gt; Internode ProcSync --&gt; LowLatency EventMgmt --&gt; Intranode Intranode --&gt; NVLink Internode --&gt; NVSHMEM LowLatency --&gt; IBGDA style PythonAPI fill:#4fc3f7 style CppRuntime fill:#ffb74d style CUDAKernel fill:#ba68c8 style Hardware fill:#81c784 2.2 æ•°æ®æµæ¶æ„1234567891011121314151617181920212223242526272829303132333435363738394041424344454647flowchart TB Start[&quot;è¾“å…¥å¼ é‡ X&lt;br/&gt;[num_tokens, hidden]&quot;] TopK[&quot;topk_idx&lt;br/&gt;[num_tokens, num_topk]&quot;] subgraph Layout[&quot; 1. Layout è®¡ç®— &quot;] L1[&quot;è®¡ç®— tokenâ†’rank æ˜ å°„&quot;] L2[&quot;ç”Ÿæˆ prefix sum&quot;] L3[&quot;ç»Ÿè®¡ token æ•°é‡&quot;] L1 --&gt; L2 --&gt; L3 end subgraph Dispatch[&quot; 2. Dispatch é˜¶æ®µ &quot;] D0[&quot;All-to-All Scatter&quot;] D1[&quot;ä¼ è¾“ hidden data&quot;] D2[&quot;ä¼ è¾“ scales/metadata&quot;] D3[&quot;ä¼ è¾“ topk info&quot;] D0 --&gt; D1 --&gt; D2 --&gt; D3 end subgraph Expert[&quot; 3. Expert è®¡ç®— &quot;] E1[&quot;å„ rank å¤„ç†&lt;br/&gt;åˆ†é…çš„ experts&quot;] end subgraph Combine[&quot; 4. Combine é˜¶æ®µ &quot;] C1[&quot;All-to-All Gather&quot;] C2[&quot;æŒ‰ metadata è·¯ç”±&quot;] C3[&quot;åº”ç”¨ topk_weights&quot;] C4[&quot;å¯é€‰ bias åŠ æ³•&quot;] C1 --&gt; C2 --&gt; C3 --&gt; C4 end End[&quot;è¾“å‡ºå¼ é‡ Y&lt;br/&gt;[num_tokens, hidden]&quot;] Start --&gt; Layout TopK --&gt; Layout Layout --&gt; Dispatch Dispatch --&gt; Expert Expert --&gt; Combine Combine --&gt; End style Start fill:#4fc3f7 style TopK fill:#4fc3f7 style Layout fill:#ffb74d style Dispatch fill:#ba68c8 style Expert fill:#81c784 style Combine fill:#f06292 style End fill:#4fc3f7 3. æ ¸å¿ƒæ¨¡å—è¯¦è§£3.1 Buffer ç®¡ç†æ¨¡å—ä½ç½®: deep_ep/buffer.py + csrc/deep_ep.hpp/cpp æ ¸å¿ƒèŒè´£ å†…å­˜ç®¡ç† NVLink Buffer: èŠ‚ç‚¹å†…é€šä¿¡ç¼“å†²åŒº RDMA Buffer: èŠ‚ç‚¹é—´é€šä¿¡ç¼“å†²åŒº (é€šè¿‡NVSHMEM) æ”¯æŒ Fabric API (GPU Direct Storage) è¿›ç¨‹åŒæ­¥ IPC Handle åŒæ­¥ (CUDA IPC &#x2F; Fabric) NVSHMEM åˆå§‹åŒ–å’Œ Unique ID äº¤æ¢ Barrier æœºåˆ¶ é€šä¿¡æµç®¡ç† ç‹¬ç«‹çš„ communication stream äº‹ä»¶åŒæ­¥æœºåˆ¶ è®¡ç®—é€šä¿¡é‡å æ”¯æŒ å…³é”®æ•°æ®ç»“æ„1234567891011121314151617181920212223struct Buffer &#123; // ç¼“å†²åŒºæŒ‡é’ˆ void* buffer_ptrs[NUM_MAX_NVL_PEERS]; // NVLink buffers void* rdma_buffer_ptr; // NVSHMEM buffer // åŒæ­¥ä¿¡å· int* barrier_signal_ptrs[NUM_MAX_NVL_PEERS]; // æ¥æ”¶è®¡æ•°å™¨ (CPU-GPU é€šä¿¡) volatile int* moe_recv_counter; volatile int* moe_recv_expert_counter; volatile int* moe_recv_rdma_counter; // æ‹“æ‰‘ä¿¡æ¯ int rank, rdma_rank, nvl_rank; int num_ranks, num_rdma_ranks, num_nvl_ranks; // ä½å»¶è¿Ÿæ¨¡å¼ç‰¹å®š bool low_latency_mode; int low_latency_buffer_idx; // åŒç¼“å†² int* mask_buffer_ptr; // åŠ¨æ€ rank å±è”½ int* sync_buffer_ptr; // è‡ªå®šä¹‰ barrier&#125;; åˆå§‹åŒ–æµç¨‹12345678910111213141516# 1. åˆ›å»º Buffer å¯¹è±¡buffer = deep_ep.Buffer( group=dist_group, num_nvl_bytes=nvl_size, num_rdma_bytes=rdma_size, low_latency_mode=False)# 2. è‡ªåŠ¨æ‰§è¡ŒåŒæ­¥# - äº¤æ¢ device IDs# - äº¤æ¢ IPC handles# - åˆå§‹åŒ– NVSHMEM (å¦‚æœéœ€è¦)# - è®¾ç½® IBGDA QP æ•°é‡# 3. Buffer å¯ç”¨assert buffer.runtime.is_available() 3.2 Intranode Kernels (èŠ‚ç‚¹å†…é€šä¿¡)ä½ç½®: csrc/kernels/intranode.cu æ ¸å¿ƒåŸç†é€šä¿¡æ¨¡å¼: NVLink peer-to-peer ç›´æ¥å†…å­˜è®¿é—® Dispatch æµç¨‹: 123456789101112131415161718192021222324252627282930flowchart TB subgraph Phase1[&quot; é˜¶æ®µ1: notify_dispatch &quot;] direction TB SM0[&quot;SM 0: åŒæ­¥å’Œå…ƒæ•°æ®&quot;] SM0_1[&quot;æ‰§è¡Œ barrier&quot;] SM0_2[&quot;ç»Ÿè®¡ tokens&quot;] SM0_3[&quot;è®¡ç®— prefix sum&quot;] SMN[&quot;SM 1-N: channel åˆ†å¸ƒ&quot;] SMN_1[&quot;å¤„ç†ç›®æ ‡ rank&quot;] SMN_2[&quot;è®¡ç®— prefix matrix&quot;] SM0 --&gt; SM0_1 --&gt; SM0_2 --&gt; SM0_3 SMN --&gt; SMN_1 --&gt; SMN_2 end subgraph Phase2[&quot; é˜¶æ®µ2: dispatch æ•°æ®ä¼ è¾“ &quot;] direction TB D1[&quot;æ¯ä¸ª SM è´Ÿè´£ä¸€ä¸ª rank&quot;] D2[&quot;channel åˆ’åˆ†è´Ÿè½½&quot;] D3[&quot;NVLink å†™å…¥å¯¹ç«¯&quot;] D4[&quot;åœ¨çº¿ç±»å‹è½¬æ¢&quot;] D1 --&gt; D2 --&gt; D3 --&gt; D4 end Phase1 ==&gt; Phase2 style Phase1 fill:#4fc3f7 style Phase2 fill:#ba68c8 å…³é”®ä¼˜åŒ–æŠ€æœ¯ Channel å¹¶è¡Œ 1234// å°† tokens åˆ†æˆå¤šä¸ª channelï¼Œæ¯ä¸ª channel ç‹¬ç«‹å¤„ç†int token_start_idx, token_end_idx;get_channel_task_range(num_tokens, num_channels, channel_id, token_start_idx, token_end_idx); Barrier ä¼˜åŒ– 12345template &lt;int kNumRanks, bool init&gt;__device__ void barrier_block(int** barrier_signal_ptrs, int rank) &#123; // ä½¿ç”¨ GPU å†…å­˜çš„åŸå­æ“ä½œå®ç°å¿«é€Ÿ barrier // é¿å… CPU å‚ä¸&#125; å¯¹é½å’Œå‘é‡åŒ– æ•°æ®å¯¹é½åˆ° 128 bytes ä½¿ç”¨ int4 å‘é‡åŠ è½½&#x2F;å­˜å‚¨ TMA (Tensor Memory Accelerator) æ”¯æŒ (SM90+) Combine æµç¨‹123451. ä»å¤šä¸ªæº rank æ”¶é›†æ•°æ®2. æŒ‰ç…§ src_idx æ’åºé‡ç»„3. åº”ç”¨ topk_weights åŠ æƒ4. ç´¯åŠ åˆ°è¾“å‡ºå¼ é‡ (ä½¿ç”¨ atomicAdd)5. å¯é€‰æ·»åŠ  bias 3.3 Internode Kernels (èŠ‚ç‚¹é—´é€šä¿¡)ä½ç½®: csrc/kernels/internode.cu æ ¸å¿ƒåŸç†é€šä¿¡æ¨¡å¼: NVSHMEM + NVLink æ··åˆ RDMA é€šä¿¡: è·¨èŠ‚ç‚¹æ•°æ®ä¼ è¾“ NVLink é€šä¿¡: èŠ‚ç‚¹å†…èšåˆ&#x2F;åˆ†å‘ æ‹“æ‰‘ç»“æ„123456789101112131415161718192021222324252627graph TB subgraph RDMA0[&quot; RDMA Rank 0 (èŠ‚ç‚¹0) &quot;] direction LR N00[&quot;GPU0&quot;] N01[&quot;GPU1&quot;] N02[&quot;GPU2&quot;] N0X[&quot;...&quot;] N07[&quot;GPU7&quot;] N00 -.&quot;NVLink&quot;.-&gt; N01 -.&quot;NVLink&quot;.-&gt; N02 -.&quot;NVLink&quot;.-&gt; N0X -.&quot;NVLink&quot;.-&gt; N07 end subgraph RDMA1[&quot; RDMA Rank 1 (èŠ‚ç‚¹1) &quot;] direction LR N10[&quot;GPU0&quot;] N11[&quot;GPU1&quot;] N12[&quot;GPU2&quot;] N1X[&quot;...&quot;] N17[&quot;GPU7&quot;] N10 -.&quot;NVLink&quot;.-&gt; N11 -.&quot;NVLink&quot;.-&gt; N12 -.&quot;NVLink&quot;.-&gt; N1X -.&quot;NVLink&quot;.-&gt; N17 end RDMA0 &lt;==&quot; RDMA IB &quot;===&gt; RDMA1 style RDMA0 fill:#4fc3f7 style RDMA1 fill:#ffb74d Dispatch æµç¨‹1234567891011121314151617181920212223242526272829flowchart TB subgraph Stage1[&quot; é˜¶æ®µ1: RDMA å…ƒæ•°æ®äº¤æ¢ &quot;] S1_1[&quot;æ”¶é›†æœ¬åœ° GPU ç»Ÿè®¡&quot;] S1_2[&quot;NVSHMEM PUT å¹¿æ’­&quot;] S1_3[&quot;åŒæ­¥ç­‰å¾…&quot;] S1_1 --&gt; S1_2 --&gt; S1_3 end subgraph Stage2[&quot; é˜¶æ®µ2: RDMA æ•°æ®ä¼ è¾“ &quot;] S2_1[&quot;è¯»å– NVLink buffer&quot;] S2_2[&quot;FP8 é‡åŒ–(å¯é€‰)&quot;] S2_3[&quot;NVSHMEM PUT è¿œç«¯&quot;] S2_4[&quot;IBGDA å¤š QP åŠ é€Ÿ&quot;] S2_1 --&gt; S2_2 --&gt; S2_3 --&gt; S2_4 end subgraph Stage3[&quot; é˜¶æ®µ3: NVLink æœ¬åœ°åˆ†å‘ &quot;] S3_1[&quot;è¯»å– RDMA buffer&quot;] S3_2[&quot;åˆ†å‘åˆ°æœ¬åœ° ranks&quot;] S3_3[&quot;å†™å…¥æ¥æ”¶ buffer&quot;] S3_1 --&gt; S3_2 --&gt; S3_3 end Stage1 ==&gt; Stage2 Stage2 ==&gt; Stage3 style Stage1 fill:#81c784 style Stage2 fill:#ffb74d style Stage3 fill:#ba68c8 SourceMeta ç¼–ç 12345678struct SourceMeta &#123; int src_rdma_rank; // æº RDMA rank int is_token_in_nvl_rank_bits; // 8-bit mask for 8 NVL ranks // ç¼–ç ç¤ºä¾‹: token æ¥è‡ª RDMA rank 2 çš„ GPU 0, 3, 5 // src_rdma_rank = 2 // is_token_in_nvl_rank_bits = 0b00101001&#125;; è¿™ä¸ªå…ƒæ•°æ®åœ¨ combine é˜¶æ®µç”¨äºè·¯ç”±æ•°æ®è¿”å›æ­£ç¡®çš„ GPUã€‚ å…³é”®ä¼˜åŒ– IBGDA (InfiniBand GPU Direct Async) 123nvshmemi_ibgda_put_nbi_warp&lt;true&gt;( dst_addr, src_addr, size, dst_rank, qp_id, lane_id, 0); ç›´æ¥ä» GPU å‘èµ· RDMA æ“ä½œ å¤š QP å¹¶è¡Œä¼ è¾“ Warp çº§åˆ«çš„åä½œ åŒå±‚ Buffer RDMA buffer: èŠ‚ç‚¹é—´æ•°æ®ç¼“å†² NVLink buffer: èŠ‚ç‚¹å†…æ•°æ®åˆ†å‘ åŠ¨æ€ Channel åˆ†é… æ ¹æ®è´Ÿè½½åŠ¨æ€åˆ†é… channel å¹³è¡¡ NVLink å’Œ RDMA å¸¦å®½ 3.4 Low-Latency Kernels (ä½å»¶è¿Ÿå†…æ ¸)ä½ç½®: csrc/kernels/internode_ll.cu è®¾è®¡ç›®æ ‡ä¸“ä¸ºæ¨ç†è§£ç åœºæ™¯ä¼˜åŒ–: å° batch size (é€šå¸¸ 128-512 tokens) æä½å»¶è¿Ÿè¦æ±‚ (&lt; 200 us) ä¸»è¦ä½¿ç”¨ RDMA é€šä¿¡ï¼Œåœ¨å…è®¸çš„é…ç½®ä¸‹ä¹Ÿä¼šåˆ©ç”¨ NVLink åŠ é€Ÿ æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ NVSHMEM_DISABLE_P2P æ§åˆ¶æ˜¯å¦ä½¿ç”¨ NVLink ä¸»è¦ä½¿ç”¨ RDMA é€šä¿¡ï¼Œåœ¨å…è®¸çš„é…ç½®ä¸‹ä¹Ÿä¼šåˆ©ç”¨ NVLink åŠ é€Ÿ æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ NVSHMEM_DISABLE_P2P æ§åˆ¶æ˜¯å¦ä½¿ç”¨ NVLink æ ¸å¿ƒç‰¹æ€§ åŒç¼“å†²æœºåˆ¶ 1234int low_latency_buffer_idx; // 0 or 1// dispatch ä½¿ç”¨ buffer 0ï¼Œcombine ä½¿ç”¨ buffer 1// ä¸‹ä¸€æ¬¡è¿­ä»£åˆ‡æ¢ Hook-based é€šä¿¡è®¡ç®—é‡å  123456789101112# dispatch è¿”å› recv_hookrecv_x, handle, event, recv_hook = buffer.low_latency_dispatch(...)# åœ¨ expert è®¡ç®—å‰è°ƒç”¨ hookif recv_hook: recv_hook() # è½®è¯¢ RDMA æ¥æ”¶å®Œæˆ# expert è®¡ç®—output = expert_forward(recv_x)# combine (åŒæ ·æœ‰ hook)result, event, recv_hook = buffer.low_latency_combine(...) åŠ¨æ€ Rank å±è”½ 1234567// ç”¨äºå®¹é”™: å¦‚æœæŸä¸ª rank è¶…æ—¶ï¼ŒåŠ¨æ€å±è”½å®ƒvoid low_latency_update_mask_buffer(int rank_to_mask, bool mask);// barrier å®ç°ä¼šè·³è¿‡è¢«å±è”½çš„ rankif (is_rank_masked(mask_buffer_ptr, dst_rank)) &#123; continue; // è·³è¿‡è¿™ä¸ª rank&#125; Dispatch æµç¨‹12345678910111213141516171819202122232425flowchart TB subgraph SendPhase[&quot; å‘é€é˜¶æ®µ LOW_LATENCY_SEND &quot;] direction TB Send1[&quot;è¯»å– topk_idx&quot;] Send2[&quot;æŒ‰ expert åˆ†ç»„&quot;] Send3[&quot;FP8 é‡åŒ–(å¯é€‰)&quot;] Send4[&quot;RDMA PUT å›ºå®š buffer&quot;] Send5[&quot;æ›´æ–° atomic counter&quot;] Send1 --&gt; Send2 --&gt; Send3 --&gt; Send4 --&gt; Send5 end subgraph RecvPhase[&quot; æ¥æ”¶é˜¶æ®µ LOW_LATENCY_RECV &quot;] direction TB Recv1[&quot;è½®è¯¢ atomic counter&quot;] Recv2[&quot;ç­‰å¾…é¢„æœŸå€¼&quot;] Recv3[&quot;æ”¶é›†ç»Ÿè®¡(å¯é€‰)&quot;] Recv1 --&gt; Recv2 --&gt; Recv3 end SendPhase ==&gt; RecvPhase style SendPhase fill:#4fc3f7 style RecvPhase fill:#81c784 Combine æµç¨‹12345ç±»ä¼¼ dispatchï¼Œä½†æ•°æ®æµåå‘:1. Expert è¾“å‡ºå†™å…¥ RDMA buffer2. æ ¹æ® src_info è·¯ç”±å›æº token3. åº”ç”¨ topk_weights4. å¯é€‰é›¶æ‹·è´æ¨¡å¼ (ç›´æ¥å†™å…¥è¾“å‡ºå¼ é‡) å…³é”®ä¼˜åŒ– é¢„åˆ†é…å›ºå®šå¤§å° Buffer æ¯ä¸ª rank ä¸ºæ¯ä¸ª expert é¢„åˆ†é…å›ºå®šç©ºé—´ é¿å…åŠ¨æ€å†…å­˜åˆ†é…å’Œå¤æ‚çš„åœ°å€è®¡ç®— Warp-Group å¹¶è¡Œ 123// æ¯ä¸ª expert ç”±ä¸€ä¸ª warp-group (å¤šä¸ª warp) å¤„ç†const auto warp_group_id = warp_id / num_warps_per_group;const auto responsible_expert_idx = sm_id * num_warp_groups + warp_group_id; è½®è¯¢ vs ä¸­æ–­ ä½¿ç”¨ä¸»åŠ¨è½®è¯¢ (polling) è€Œéä¸­æ–­ æ›´ä½çš„å»¶è¿Ÿï¼Œä»£ä»·æ˜¯æŒç»­å ç”¨ CPU&#x2F;GPU è¶…æ—¶å’Œå®¹é”™ 12345678910auto start_time = clock64();uint64_t wait_recv_cost = 0;while (condition &amp;&amp; (wait_recv_cost = clock64() - start_time) &lt;= NUM_TIMEOUT_CYCLES) &#123; // polling&#125;if (wait_recv_cost &gt; NUM_TIMEOUT_CYCLES) &#123; // è¶…æ—¶å¤„ç†: å±è”½è¯¥ rank atomicExch(mask_buffer_ptr + dst_rank, 1);&#125; 3.5 Layout è®¡ç®—æ¨¡å—ä½ç½®: csrc/kernels/layout.cu æ ¸å¿ƒåŠŸèƒ½åœ¨ dispatch ä¹‹å‰è®¡ç®—è·¯ç”±ä¿¡æ¯: 123456num_tokens_per_rank, # [num_ranks] æ¯ä¸ª rank æ¥æ”¶çš„ token æ•°num_tokens_per_rdma_rank, # [num_rdma_ranks] æ¯ä¸ª RDMA rank æ¥æ”¶æ•°num_tokens_per_expert, # [num_experts] æ¯ä¸ª expert æ¥æ”¶æ•°is_token_in_rank, # [num_tokens, num_ranks] bool çŸ©é˜µlayout_event # CUDA event= buffer.get_dispatch_layout(topk_idx, num_experts) ç®—æ³•æµç¨‹12345678910111213141516171819202122232425262728293031flowchart TB Input[&quot;è¾“å…¥: topk_idx&lt;br/&gt;[num_tokens, num_topk]&quot;] subgraph Step1[&quot; æ­¥éª¤1: æ‰«æ top-k experts &quot;] direction TB S1_1[&quot;éå†æ‰€æœ‰ tokens&quot;] S1_2[&quot;è·å– expert_id&quot;] S1_3[&quot;è®¡ç®—ç›®æ ‡ rank&quot;] S1_4[&quot;æ ‡è®° is_token_in_rank&quot;] S1_1 --&gt; S1_2 --&gt; S1_3 --&gt; S1_4 end subgraph Step2[&quot; æ­¥éª¤2: ç»Ÿè®¡ token æ•°é‡ &quot;] direction TB S2_1[&quot;sum per rank&quot;] S2_2[&quot;count per expert&quot;] S2_1 --&gt; S2_2 end subgraph Step3[&quot; æ­¥éª¤3: å¯¹é½å¤„ç† &quot;] S3_1[&quot;align_up to&lt;br/&gt;expert_alignment&quot;] end Input --&gt; Step1 Step1 --&gt; Step2 Step2 --&gt; Step3 style Input fill:#4fc3f7 style Step1 fill:#ffb74d style Step2 fill:#ba68c8 style Step3 fill:#81c784 ä¼˜åŒ–æŠ€æœ¯ Warp-level reduction Shared memory èšåˆ Coalesced memory access 3.6 äº‹ä»¶å’ŒåŒæ­¥æœºåˆ¶ä½ç½®: csrc/event.hpp + deep_ep/utils.py EventOverlap ç±»1234567891011121314151617class EventOverlap: &quot;&quot;&quot;é€šä¿¡è®¡ç®—é‡å çš„ä¾¿åˆ©å°è£…&quot;&quot;&quot; def __init__(self, event: EventHandle): self.event = event def current_stream_wait(self): &quot;&quot;&quot;å½“å‰æµç­‰å¾…äº‹ä»¶å®Œæˆ&quot;&quot;&quot; self.event.current_stream_wait() # æ”¯æŒ with è¯­æ³• def __enter__(self): return self def __exit__(self, ...): if self.event is not None: self.event.current_stream_wait() ä½¿ç”¨æ¨¡å¼12345678910111213141516# æ¨¡å¼ 1: æ‰‹åŠ¨åŒæ­¥event = buffer.dispatch(...)# ... å…¶ä»–è®¡ç®— ...event.current_stream_wait() # ç­‰å¾… dispatch å®Œæˆ# æ¨¡å¼ 2: with è¯­æ³•event = buffer.dispatch(...)with event: # è¿™é‡Œçš„è®¡ç®—ä¸ dispatch é‡å  expert_computation()# é€€å‡º with æ—¶è‡ªåŠ¨ç­‰å¾…# æ¨¡å¼ 3: async æ¨¡å¼event = buffer.dispatch(..., async_mode=True)# dispatch åœ¨ç‹¬ç«‹ stream ä¸Šæ‰§è¡Œ# ä¸»æµç»§ç»­è¿è¡Œ 4. å…³é”®æŠ€æœ¯ç»†èŠ‚4.1 FP8 é‡åŒ–DeepEP æ”¯æŒä¸¤ç§ FP8 æ ¼å¼: E4M3: å¸¸è§„ FP8 (4-bit exponent, 3-bit mantissa) UE8M0: ç‰¹æ®Šæ ¼å¼ (æ— ç¬¦å· 8-bit æ•´æ•°ä½œä¸º scale) Per-Token é‡åŒ–123456// dispatch æ—¶é‡åŒ–scale = max(abs(token)) / 448.0 // E4M3 maxquantized_token = token / scale// combine æ—¶åé‡åŒ–dequantized_token = quantized_token * scale Scale å­˜å‚¨ä¼˜åŒ–1234567// æ¯ 128 ä¸ªå…ƒç´ ä¸€ä¸ª scale (channel-wise)constexpr int kNumPerChannels = 128;int num_scales = hidden / kNumPerChannels;// UE8M0: å°† float scale ç¼–ç ä¸º uint8uint8_t encode_ue8m0(float scale);float decode_ue8m0(uint8_t encoded_scale); 4.2 IBGDA (InfiniBand GPU Direct Async)åŸç†IBGDA å…è®¸ GPU ç›´æ¥å‘èµ· RDMA æ“ä½œï¼Œæ— éœ€ CPU å‚ä¸: 12345678ä¼ ç»Ÿ NVSHMEM:GPU â†’ PCIe â†’ CPU â†’ IB HCA â†’ Network â†“ RDMA æ“ä½œæ’é˜ŸIBGDA:GPU â†’ Direct Access to IB HCA â†’ Network (é€šè¿‡ BAR æ˜ å°„) å¤š QP å¹¶è¡Œ123456789101112// é…ç½®å¤šä¸ª Queue Pair (QP) ç”¨äºå¹¶è¡Œä¼ è¾“int qps_per_rank = num_rc_per_pe * num_devices_initialized;// å¹¶è¡Œå‘èµ·å¤šä¸ª QP ä¸Šçš„ä¼ è¾“for (int qp_id = 0; qp_id &lt; qps_per_rank; qp_id++) &#123; nvshmemi_ibgda_put_nbi_warp( dst_addr, src_addr, size, dst_rank, qp_id, ... );&#125;// Quiet: ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆnvshmemi_ibgda_quiet(dst_rank, qp_id); 4.3 Barrier å®ç°GPU-only Barrier12345678910111213141516171819202122232425262728293031template &lt;int kNumRanks, bool init = false&gt;__device__ void barrier_block(int** barrier_signal_ptrs, int rank) &#123; __shared__ int barrier_signals[kNumRanks]; if (init) &#123; // åˆå§‹åŒ–: æ¯ä¸ª rank é‡ç½®è‡ªå·±çš„ä¿¡å· if (threadIdx.x == 0) &#123; for (int i = 0; i &lt; kNumRanks; i++) &#123; barrier_signals[i] = 0; &#125; &#125; &#125; // Phase 1: é€šçŸ¥å…¶ä»– ranks if (threadIdx.x &lt; kNumRanks &amp;&amp; threadIdx.x != rank) &#123; atomicAdd(barrier_signal_ptrs[threadIdx.x] + rank, 1); &#125; __syncthreads(); // Phase 2: ç­‰å¾…å…¶ä»– ranks é€šçŸ¥ if (threadIdx.x == 0) &#123; for (int i = 0; i &lt; kNumRanks; i++) &#123; if (i != rank) &#123; while (barrier_signal_ptrs[rank][i] &lt; expected_count) &#123; // spin wait &#125; &#125; &#125; &#125; __syncthreads();&#125; 4.4 å†…å­˜è®¿é—®ä¼˜åŒ–Load&#x2F;Store æŒ‡ä»¤é€‰æ‹©12345678910111213141516171819202122232425// Global memory with cache controltemplate &lt;typename T&gt;__device__ T ld_volatile_global(const T* addr) &#123; return *((volatile T*)addr);&#125;template &lt;typename T&gt;__device__ void st_na_global(T* addr, T val) &#123; // Non-allocating store (ä¸æ±¡æŸ“ L2 cache) #ifndef DISABLE_AGGRESSIVE_PTX_INSTRS asm volatile(&quot;st.global.cs.L2::no_allocate %0, %1;&quot; : : &quot;l&quot;(addr), &quot;r&quot;(val)); #else *addr = val; #endif&#125;// System-level atomics (è·¨ CPU-GPU)template &lt;typename T&gt;__device__ T ld_acquire_sys_global(const T* addr) &#123; T val; asm volatile(&quot;ld.acquire.sys.global.b32 %0, [%1];&quot; : &quot;=r&quot;(val) : &quot;l&quot;(addr)); return val;&#125; TMA (Tensor Memory Accelerator)12345678// SM90+ ç‰¹æ€§: ç¡¬ä»¶åŠ é€Ÿçš„å¼ é‡å†…å­˜æ‹·è´#ifndef DISABLE_SM90_FEATUREStemplate &lt;int kNumBytes&gt;__device__ void tma_load(void* dst, const void* src) &#123; // ä½¿ç”¨ TMA ç¡¬ä»¶å•å…ƒ // æ›´é«˜å¸¦å®½ï¼Œæ›´ä½å»¶è¿Ÿ&#125;#endif 4.5 Warp-level åŸè¯­12345678910111213141516171819202122// Warp reduce sum__device__ int warp_reduce_sum(int val) &#123; #pragma unroll for (int offset = 16; offset &gt; 0; offset /= 2) &#123; val += __shfl_down_sync(0xffffffff, val, offset); &#125; return val;&#125;// Elect one thread in warp (é€šå¸¸æ˜¯ lane 0)__device__ bool elect_one_sync() &#123; return __match_any_sync(0xffffffff, 1) == 0xffffffff &amp;&amp; get_lane_id() == 0;&#125;// Warp-level copy#define UNROLLED_WARP_COPY(num, lane_id, count, dst, src, ld_fn, st_fn) \\ _Pragma(&quot;unroll&quot;) \\ for (int i = lane_id * num; i &lt; count; i += 32 * num) &#123; \\ auto tmp = ld_fn(src + i); \\ st_fn(dst + i, tmp); \\ &#125; 5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥5.1 é€šä¿¡è®¡ç®—é‡å ç­–ç•¥ 1: ç‹¬ç«‹ Stream 1234567# dispatch å’Œ expert è®¡ç®—åœ¨ä¸åŒ streamcomm_stream = buffer.get_comm_stream()event = buffer.dispatch(..., async_mode=True)# comm_stream ä¸Šæ‰§è¡Œ dispatch# ä¸»æµç»§ç»­æ‰§è¡Œå…¶ä»–è®¡ç®—with event: expert_forward() # ç­‰å¾… dispatch å®Œæˆ ç­–ç•¥ 2: Hook-based (ä½å»¶è¿Ÿæ¨¡å¼) 123456recv_x, handle, event, recv_hook = buffer.low_latency_dispatch(...)# dispatch ç«‹å³è¿”å›# åœ¨è®¡ç®—å‰è°ƒç”¨ hook ç¡®ä¿æ•°æ®åˆ°è¾¾if recv_hook: recv_hook() # è½®è¯¢æ¥æ”¶expert_output = expert_forward(recv_x) 5.2 è´Ÿè½½å‡è¡¡Channel æœºåˆ¶: 12345// å°† tokens åˆ†æˆ num_channels ä¸ª channel// æ¯ä¸ª SM/warp å¤„ç†ä¸€ä¸ª channel// åŠ¨æ€å¹³è¡¡å„ channel è´Ÿè½½get_channel_task_range(num_tokens, num_channels, channel_id, start_idx, end_idx); SM åˆ†é…: 123# å¯é…ç½® SM æ•°é‡Buffer.set_num_sms(20) # ä½¿ç”¨ 20 ä¸ª SMconfig = Config(num_sms=20, ...) 5.3 å†…å­˜å¸¦å®½ä¼˜åŒ– å¯¹é½: æ‰€æœ‰æ•°æ®å¯¹é½åˆ° 128 bytes Coalescing: è¿ç»­çº¿ç¨‹è®¿é—®è¿ç»­å†…å­˜ å‘é‡åŒ–: ä½¿ç”¨ int4&#x2F;int2 å‘é‡ load&#x2F;store Cache æ§åˆ¶: ä½¿ç”¨ PTX æŒ‡ä»¤æ§åˆ¶ L2 cache 5.4 å»¶è¿Ÿä¼˜åŒ– (ä½å»¶è¿Ÿæ¨¡å¼) é¢„åˆ†é…å›ºå®š buffer: é¿å…åŠ¨æ€åœ°å€è®¡ç®— è½®è¯¢æ¥æ”¶: ä¸»åŠ¨è½®è¯¢è€Œéè¢«åŠ¨ç­‰å¾… ä¸»è¦ä½¿ç”¨ RDMA: å‡å°‘ NVLink hopï¼Œä½†åœ¨å…è®¸çš„é…ç½®ä¸‹ä¹Ÿä¼šåˆ©ç”¨ NVLink åŠ é€Ÿ è¶…æ—¶æœºåˆ¶: å¿«é€Ÿæ£€æµ‹å’Œè·³è¿‡æ…¢ rank 6. ä½¿ç”¨ç¤ºä¾‹6.1 åŸºæœ¬ç”¨æ³• (èŠ‚ç‚¹å†…)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.distributed as distimport deep_ep# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒdist.init_process_group(backend=&#x27;nccl&#x27;)group = dist.new_group()# åˆ›å»º buffernvl_buffer_size = 256 * 1024 * 1024 # 256 MBbuffer = deep_ep.Buffer( group=group, num_nvl_bytes=nvl_buffer_size)# MoE forwardx = torch.randn(4096, 7168, dtype=torch.bfloat16, device=&#x27;cuda&#x27;)topk_idx = routing(x) # [4096, 8]# Layout è®¡ç®—num_tokens_per_rank, _, num_tokens_per_expert, is_token_in_rank, _ = \\ buffer.get_dispatch_layout(topk_idx, num_experts=64)# Dispatch# Configå‚æ•°: (num_sms, nvl_send_chunk, nvl_recv_chunk, rdma_send_chunk, rdma_recv_chunk)config = deep_ep.Config(20, 6, 256, 6, 128) # 8 ranksç¤ºä¾‹recv_x, recv_x_scales, handle, event = buffer.dispatch( x=x, topk_idx=topk_idx, topk_weights=topk_weights, num_tokens_per_rank=num_tokens_per_rank, is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert, config=config)# Expert è®¡ç®—with event: # ç­‰å¾… dispatch å®Œæˆ expert_output = expert_forward(recv_x)# Combineoutput, combine_event = buffer.combine( x=expert_output, handle=handle, topk_weights=topk_weights, config=config) 6.2 èŠ‚ç‚¹é—´é€šä¿¡123456789101112131415161718192021222324252627282930# åˆ›å»º buffer (åŒ…å« RDMA)rdma_buffer_size = 1024 * 1024 * 1024 # 1 GBbuffer = deep_ep.Buffer( group=group, num_nvl_bytes=nvl_buffer_size, num_rdma_bytes=rdma_buffer_size)# è·å– RDMA layoutnum_tokens_per_rdma_rank = ... # é¢å¤–çš„ RDMA level ç»Ÿè®¡# Dispatch (internode)recv_x, recv_x_scales, handle, event = buffer.internode_dispatch( x=x, topk_idx=topk_idx, topk_weights=topk_weights, num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank, is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert, config=config)# Combine (internode)output, combine_event = buffer.internode_combine( x=expert_output, handle=handle, topk_weights=topk_weights, config=config) 6.3 ä½å»¶è¿Ÿæ¨¡å¼ (æ¨ç†)12345678910111213141516171819202122232425262728293031323334353637383940414243444546# åˆ›å»ºä½å»¶è¿Ÿ bufferbuffer = deep_ep.Buffer( group=group, num_rdma_bytes=rdma_buffer_size, low_latency_mode=True, num_qps_per_rank=num_experts # æ¯ä¸ª expert ä¸€ä¸ª QP)# æ¸…ç† buffer (é¦–æ¬¡æˆ– batch æ”¹å˜æ—¶)buffer.clean_low_latency_buffer( num_max_dispatch_tokens_per_rank=512, hidden=7168, num_experts=64)# Dispatch with hookrecv_x, recv_x_scales, handle, event, recv_hook = \\ buffer.low_latency_dispatch( x=x, topk_idx=topk_idx, num_max_dispatch_tokens_per_rank=512, num_experts=64, use_fp8=True, return_recv_hook=True )# è°ƒç”¨ hook ç¡®ä¿æ•°æ®åˆ°è¾¾if recv_hook: recv_hook()# Expert è®¡ç®—expert_output = expert_forward(recv_x)# Combine with hookoutput, combine_event, recv_hook = buffer.low_latency_combine( x=expert_output, topk_idx=topk_idx, topk_weights=topk_weights, handle=handle, num_max_dispatch_tokens_per_rank=512, num_experts=64, return_recv_hook=True)if recv_hook: recv_hook() 7. é…ç½®å’Œè°ƒä¼˜7.1 å…³é”®é…ç½®å‚æ•°1234567891011class Config: num_sms: int # ä½¿ç”¨çš„ SM æ•°é‡ (é€šå¸¸ 20) num_max_nvl_chunked_send_tokens: int # NVLink å‘é€ç«¯ chunk å¤§å° num_max_nvl_chunked_recv_tokens: int # NVLink æ¥æ”¶ç«¯ chunk å¤§å° num_max_rdma_chunked_send_tokens: int # RDMA å‘é€ç«¯ chunk å¤§å° num_max_rdma_chunked_recv_tokens: int # RDMA æ¥æ”¶ç«¯ chunk å¤§å°# ç¤ºä¾‹ï¼š# Intranode (8 ranks): Config(20, 6, 256, 6, 128)# Internode (16 ranks): Config(20, 36, 288, 20, 128)# Internode (64 ranks): Config(20, 32, 288, 8, 128) 7.2 ç¯å¢ƒå˜é‡NVSHMEM é…ç½®: 12345678910111213# IBGDA ç›¸å…³export NVSHMEM_IB_ENABLE_IBGDA=1export NVSHMEM_IBGDA_NUM_RC_PER_PE=24 # æ¯ä¸ª PE çš„ QP æ•°# QP æ·±åº¦ (å¿…é¡» &gt; åœ¨é€” WR æ•°)export NVSHMEM_QP_DEPTH=1024# ç¦ç”¨ P2P (ä½å»¶è¿Ÿæ¨¡å¼å¯èƒ½éœ€è¦)export NVSHMEM_DISABLE_P2P=0/1# å†…å­˜ç›¸å…³export NVSHMEM_CUMEM_GRANULARITY=536870912 # 512 MBexport NVSHMEM_MAX_TEAMS=7 ç¼–è¯‘é€‰é¡¹: 123456789101112# SM æ¶æ„export TORCH_CUDA_ARCH_LIST=&quot;9.0&quot; # H100/H800export TORCH_CUDA_ARCH_LIST=&quot;8.0&quot; # A100# ç¦ç”¨ SM90 ç‰¹æ€§ (Ampere)export DISABLE_SM90_FEATURES=1# ç¦ç”¨æ¿€è¿› PTX æŒ‡ä»¤export DISABLE_AGGRESSIVE_PTX_INSTRS=1# topk_idx ä½æ•°export TOPK_IDX_BITS=64 # or 32 7.3 æ€§èƒ½è°ƒä¼˜å»ºè®®Training (é«˜åå): ä½¿ç”¨ internode kernels (è·¨èŠ‚ç‚¹) æˆ– intranode kernels (èŠ‚ç‚¹å†…) num_sms &#x3D; 20-40 chunk sizes æ ¹æ® rank æ•°é‡è°ƒæ•´ (å‚è€ƒ get_dispatch_config&#x2F;get_combine_config) buffer å¤§å°é€šè¿‡ Config.get_nvl_buffer_size_hint() å’Œ get_rdma_buffer_size_hint() è®¡ç®— Inference Prefill: åŒ training é…ç½® å¯ç”¨ FP8 é‡åŒ–å¯å‡å°‘å¸¦å®½éœ€æ±‚ Inference Decode (ä½å»¶è¿Ÿ): ä½¿ç”¨ low_latency kernels num_qps_per_rank &#x3D; num_local_experts (æ¯ä¸ª expert ä¸€ä¸ª QP) num_max_dispatch_tokens_per_rank æ ¹æ®æœ€å¤§ batch è®¾ç½® NVSHMEM_QP_DEPTH &gt;&#x3D; (num_max_dispatch_tokens_per_rank + 1) * 2 è€ƒè™‘å¯ç”¨ hook-based é‡å ä»¥æé«˜åå 8. ä¸ DeepSeek-V3 çš„å…³ç³»DeepEP æ˜¯ä¸º DeepSeek-V3 æ¶æ„è®¾è®¡çš„é€šä¿¡åº“ï¼Œé’ˆå¯¹å…¶ç‰¹å®šéœ€æ±‚ä¼˜åŒ–: 8.1 DeepSeek-V3 MoE é…ç½® æ€» experts: 256 Active experts: top-8 Group-limited gating: top-4 groups Hidden dimension: 7168 EP parallelism: é€šå¸¸ 64-256 ranks 8.2 å…³é”®ä¼˜åŒ–å¯¹åº” Group-limited gating é™åˆ¶æ¯ä¸ª token åªèƒ½é€‰æ‹©ç‰¹å®š groups çš„ experts DeepEP çš„ asymmetric bandwidth forwarding ä¼˜åŒ–è¿™ä¸ªæ¨¡å¼ é«˜ç»´åº¦ (7168) å¸¦å®½å¯†é›†å‹ DeepEP çš„å‘é‡åŒ–å’Œ TMA åŠ é€Ÿ å¤§è§„æ¨¡å¹¶è¡Œ (256 ranks) éœ€è¦é«˜æ•ˆçš„ RDMA å’Œå¤šçº§æ‹“æ‰‘ DeepEP çš„ RDMA + NVLink æ··åˆæ¶æ„ 9. æ€»ç»“DeepEP æ˜¯ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ MoE é€šä¿¡åº“ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹: ä¸‰ç§é€šä¿¡æ¨¡å¼: intranode (NVLink), internode (RDMA+NVLink), low-latency (çº¯ RDMA) é«˜æ€§èƒ½: Intranode: ~155 GB&#x2F;s (æ¥è¿‘ç¡¬ä»¶å³°å€¼) Internode: ~43-58 GB&#x2F;s (RDMA å¸¦å®½é™åˆ¶) Low-latency: &lt;200 us (256 ranks) çµæ´»æ€§: æ”¯æŒ FP8&#x2F;BF16 å¯é…ç½® SM å’Œ channel æ•°é‡ æ”¯æŒé€šä¿¡è®¡ç®—é‡å  é²æ£’æ€§: è¶…æ—¶æ£€æµ‹å’Œå®¹é”™ åŠ¨æ€ rank å±è”½ å®Œå–„çš„é”™è¯¯æ£€æŸ¥ å¯æ‰©å±•æ€§: æ”¯æŒæ•°ç™¾ä¸ª ranks å¤šçº§æ‹“æ‰‘ (NVLink + RDMA) é«˜æ•ˆçš„å…ƒæ•°æ®äº¤æ¢ DeepEP æ˜¯å¤§è§„æ¨¡ MoE è®­ç»ƒå’Œæ¨ç†çš„å…³é”®åŸºç¡€è®¾æ–½ç»„ä»¶ï¼Œå……åˆ†åˆ©ç”¨ç°ä»£ GPU å’Œç½‘ç»œç¡¬ä»¶çš„èƒ½åŠ›ã€‚","categories":[{"name":"ç³»ç»Ÿæ¶æ„åˆ†æ","slug":"ç³»ç»Ÿæ¶æ„åˆ†æ","permalink":"https://nash635.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/"}],"tags":[{"name":"DeepEP","slug":"DeepEP","permalink":"https://nash635.github.io/tags/DeepEP/"},{"name":"MoE","slug":"MoE","permalink":"https://nash635.github.io/tags/MoE/"},{"name":"Expert Parallelism","slug":"Expert-Parallelism","permalink":"https://nash635.github.io/tags/Expert-Parallelism/"},{"name":"é€šä¿¡åº“","slug":"é€šä¿¡åº“","permalink":"https://nash635.github.io/tags/%E9%80%9A%E4%BF%A1%E5%BA%93/"},{"name":"All-to-All","slug":"All-to-All","permalink":"https://nash635.github.io/tags/All-to-All/"}]},{"title":"Hello World","slug":"Hello-World","date":"2025-07-23T17:18:25.000Z","updated":"2025-11-10T08:32:58.192Z","comments":true,"path":"2025/07/24/Hello-World/","permalink":"https://nash635.github.io/2025/07/24/Hello-World/","excerpt":"","text":"ä½ å¥½ï¼Œä¸–ç•Œï¼æ¬¢è¿æ¥åˆ°æˆ‘çš„ä¸ªäººåšå®¢ï¼ğŸ‰ è¿™æ˜¯æˆ‘ä½¿ç”¨ Hexo é™æ€åšå®¢ç”Ÿæˆå™¨å’Œç¾ä¸½çš„ Stellar ä¸»é¢˜æ­å»ºçš„å…¨æ–°åšå®¢ã€‚ å…³äºè¿™ä¸ªåšå®¢åœ¨è¿™ä¸ªåšå®¢ä¸­ï¼Œæˆ‘å°†åˆ†äº«ï¼š ğŸ“š æŠ€æœ¯å­¦ä¹ ç¬”è®° ğŸ’» ç¼–ç¨‹ç»éªŒæ€»ç»“ ğŸŒ± ç”Ÿæ´»æ„Ÿæ‚Ÿ ğŸ¯ é¡¹ç›®è®°å½• å…³äº Stellar ä¸»é¢˜Stellar æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ Hexo ä¸»é¢˜ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š ğŸ¨ ç®€æ´ç¾è§‚çš„è®¾è®¡ ğŸ“± å®Œç¾çš„ç§»åŠ¨ç«¯é€‚é… âš¡ å¿«é€Ÿçš„åŠ è½½é€Ÿåº¦ ğŸ”§ ä¸°å¯Œçš„è‡ªå®šä¹‰é€‰é¡¹ ğŸ¯ ä¼˜ç§€çš„ SEO æ”¯æŒ å¼€å§‹æ¢ç´¢æ„Ÿè°¢æ‚¨è®¿é—®æˆ‘çš„åšå®¢ï¼å¸Œæœ›æ‚¨èƒ½åœ¨è¿™é‡Œæ‰¾åˆ°æœ‰ç”¨çš„å†…å®¹ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•å»ºè®®æˆ–æƒ³æ³•ï¼Œæ¬¢è¿éšæ—¶è”ç³»æˆ‘ã€‚ ç¥æ‚¨é˜…è¯»æ„‰å¿«ï¼ ğŸ˜Š","categories":[{"name":"å…¶ä»–","slug":"other","permalink":"https://nash635.github.io/categories/other/"}],"tags":[{"name":"hello","slug":"hello","permalink":"https://nash635.github.io/tags/hello/"},{"name":"world","slug":"world","permalink":"https://nash635.github.io/tags/world/"},{"name":"stellar","slug":"stellar","permalink":"https://nash635.github.io/tags/stellar/"}]}],"categories":[{"name":"ç³»ç»Ÿæ¶æ„åˆ†æ","slug":"ç³»ç»Ÿæ¶æ„åˆ†æ","permalink":"https://nash635.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/"},{"name":"å…¶ä»–","slug":"other","permalink":"https://nash635.github.io/categories/other/"}],"tags":[{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://nash635.github.io/tags/NVIDIA/"},{"name":"Transformer Engine","slug":"Transformer-Engine","permalink":"https://nash635.github.io/tags/Transformer-Engine/"},{"name":"FP8","slug":"FP8","permalink":"https://nash635.github.io/tags/FP8/"},{"name":"æ··åˆç²¾åº¦","slug":"æ··åˆç²¾åº¦","permalink":"https://nash635.github.io/tags/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6/"},{"name":"GPUåŠ é€Ÿ","slug":"GPUåŠ é€Ÿ","permalink":"https://nash635.github.io/tags/GPU%E5%8A%A0%E9%80%9F/"},{"name":"Megatron","slug":"Megatron","permalink":"https://nash635.github.io/tags/Megatron/"},{"name":"å¤§è¯­è¨€æ¨¡å‹","slug":"å¤§è¯­è¨€æ¨¡å‹","permalink":"https://nash635.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"åˆ†å¸ƒå¼è®­ç»ƒ","slug":"åˆ†å¸ƒå¼è®­ç»ƒ","permalink":"https://nash635.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"name":"Transformer","slug":"Transformer","permalink":"https://nash635.github.io/tags/Transformer/"},{"name":"HCCL","slug":"HCCL","permalink":"https://nash635.github.io/tags/HCCL/"},{"name":"é›†åˆé€šä¿¡","slug":"é›†åˆé€šä¿¡","permalink":"https://nash635.github.io/tags/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1/"},{"name":"åä¸º","slug":"åä¸º","permalink":"https://nash635.github.io/tags/%E5%8D%8E%E4%B8%BA/"},{"name":"æ˜‡è…¾","slug":"æ˜‡è…¾","permalink":"https://nash635.github.io/tags/%E6%98%87%E8%85%BE/"},{"name":"AllReduce","slug":"AllReduce","permalink":"https://nash635.github.io/tags/AllReduce/"},{"name":"DeepEP","slug":"DeepEP","permalink":"https://nash635.github.io/tags/DeepEP/"},{"name":"MoE","slug":"MoE","permalink":"https://nash635.github.io/tags/MoE/"},{"name":"Expert Parallelism","slug":"Expert-Parallelism","permalink":"https://nash635.github.io/tags/Expert-Parallelism/"},{"name":"é€šä¿¡åº“","slug":"é€šä¿¡åº“","permalink":"https://nash635.github.io/tags/%E9%80%9A%E4%BF%A1%E5%BA%93/"},{"name":"All-to-All","slug":"All-to-All","permalink":"https://nash635.github.io/tags/All-to-All/"},{"name":"hello","slug":"hello","permalink":"https://nash635.github.io/tags/hello/"},{"name":"world","slug":"world","permalink":"https://nash635.github.io/tags/world/"},{"name":"stellar","slug":"stellar","permalink":"https://nash635.github.io/tags/stellar/"}]}