<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Transformer Engine 架构设计分析</title>
      <link href="/2025/11/17/TransformerEngine_DeepDive/"/>
      <url>/2025/11/17/TransformerEngine_DeepDive/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer-Engine-架构设计分析文档"><a href="#Transformer-Engine-架构设计分析文档" class="headerlink" title="Transformer Engine 架构设计分析文档"></a>Transformer Engine 架构设计分析文档</h1><h2 id="1-项目概述"><a href="#1-项目概述" class="headerlink" title="1. 项目概述"></a>1. 项目概述</h2><h3 id="1-1-项目简介"><a href="#1-1-项目简介" class="headerlink" title="1.1 项目简介"></a>1.1 项目简介</h3><p><strong>Transformer Engine (TE)</strong> 是 NVIDIA 开发的高性能 Transformer 模型加速库，专门用于在 NVIDIA GPU（Hopper、Ada、Blackwell 架构）上加速 Transformer 模型的训练和推理。核心特性是支持 8 位浮点数（FP8）精度，在保持模型精度的同时显著降低内存占用和提升性能。</p><h3 id="1-2-核心特性"><a href="#1-2-核心特性" class="headerlink" title="1.2 核心特性"></a>1.2 核心特性</h3><ul><li><strong>FP8&#x2F;FP4 混合精度训练</strong>：支持 E4M3、E5M2、NVFP4 等多种低精度格式</li><li><strong>框架无关</strong>：提供 C++ API 以及 PyTorch、JAX 的 Python 绑定</li><li><strong>高度优化</strong>：融合算子、优化的 GEMM 内核、cuDNN 集成</li><li><strong>CPU Offload</strong>：支持激活值卸载到 CPU，降低 GPU 内存占用</li><li><strong>易用性</strong>：类似 automatic mixed precision 的 API 设计</li><li><strong>分布式训练支持</strong>：内置 MPI、NVSHMEM 支持</li></ul><h3 id="1-3-技术栈"><a href="#1-3-技术栈" class="headerlink" title="1.3 技术栈"></a>1.3 技术栈</h3><ul><li><strong>核心语言</strong>：C++、CUDA</li><li><strong>Python 绑定</strong>：Pybind11</li><li><strong>深度学习框架</strong>：PyTorch 2.1+、JAX</li><li><strong>构建系统</strong>：CMake、setuptools</li><li><strong>依赖库</strong>：<ul><li><strong>CUDA 12.1+</strong>：GPU 编程基础</li><li><strong>cuBLASLt</strong>：NVIDIA 高性能线性代数库（Lightweight 版本），提供 FP8 GEMM 支持</li><li><strong>cuDNN</strong>：深度神经网络加速库，提供融合注意力实现</li><li><strong>cutlass</strong>：CUDA 模板化线性代数库，用于自定义高性能 kernel</li><li><strong>NCCL</strong>（可选）：多 GPU 通信</li><li><strong>NVSHMEM</strong>（可选）：对称内存访问，低延迟通信</li></ul></li></ul><hr><h2 id="2-系统架构设计"><a href="#2-系统架构设计" class="headerlink" title="2. 系统架构设计"></a>2. 系统架构设计</h2><h3 id="2-1-整体架构层次"><a href="#2-1-整体架构层次" class="headerlink" title="2.1 整体架构层次"></a>2.1 整体架构层次</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;用户层 (User Layer)&quot;</span><br><span class="line">        A[PyTorch/JAX 应用代码]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;框架适配层 (Framework Adapter Layer)&quot;</span><br><span class="line">        B1[transformer_engine.pytorch]</span><br><span class="line">        B2[transformer_engine.jax]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Python API 层 (Python API Layer)&quot;</span><br><span class="line">        C1[Module API&lt;br/&gt;Linear, LayerNorm, Attention]</span><br><span class="line">        C2[Quantization API&lt;br/&gt;fp8_autocast, recipes]</span><br><span class="line">        C3[Distributed API&lt;br/&gt;checkpoint, communication]</span><br><span class="line">        C4[Tensor API&lt;br/&gt;Float8Tensor, Quantizer]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;核心计算层 (Core Compute Layer)&quot;</span><br><span class="line">        D[transformer_engine.common&lt;br/&gt;Framework-Agnostic C++ Library]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;算子层 (Operator Layer)&quot;</span><br><span class="line">        E1[GEMM Operators&lt;br/&gt;cuBLAS, cutlass]</span><br><span class="line">        E2[Normalization&lt;br/&gt;LayerNorm, RMSNorm]</span><br><span class="line">        E3[Attention&lt;br/&gt;Fused Attention, FlashAttention]</span><br><span class="line">        E4[Activation&lt;br/&gt;GELU, SwiGLU]</span><br><span class="line">        E5[Communication&lt;br/&gt;NCCL, NVSHMEM]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;硬件层 (Hardware Layer)&quot;</span><br><span class="line">        F[NVIDIA GPU&lt;br/&gt;Hopper/Ada/Blackwell + Tensor Cores]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    A --&gt; B1</span><br><span class="line">    A --&gt; B2</span><br><span class="line">    B1 --&gt; C1</span><br><span class="line">    B1 --&gt; C2</span><br><span class="line">    B1 --&gt; C3</span><br><span class="line">    B1 --&gt; C4</span><br><span class="line">    B2 --&gt; C1</span><br><span class="line">    B2 --&gt; C2</span><br><span class="line">    C1 --&gt; D</span><br><span class="line">    C2 --&gt; D</span><br><span class="line">    C3 --&gt; D</span><br><span class="line">    C4 --&gt; D</span><br><span class="line">    D --&gt; E1</span><br><span class="line">    D --&gt; E2</span><br><span class="line">    D --&gt; E3</span><br><span class="line">    D --&gt; E4</span><br><span class="line">    D --&gt; E5</span><br><span class="line">    E1 --&gt; F</span><br><span class="line">    E2 --&gt; F</span><br><span class="line">    E3 --&gt; F</span><br><span class="line">    E4 --&gt; F</span><br><span class="line">    E5 --&gt; F</span><br></pre></td></tr></table></figure><h3 id="2-2-模块架构"><a href="#2-2-模块架构" class="headerlink" title="2.2 模块架构"></a>2.2 模块架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph &quot;transformer_engine&quot;</span><br><span class="line">        A[__init__.py]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;common (C++核心)&quot;</span><br><span class="line">            B1[transformer_engine.cpp]</span><br><span class="line">            B2[gemm/]</span><br><span class="line">            B3[normalization/]</span><br><span class="line">            B4[fused_attn/]</span><br><span class="line">            B5[activation/]</span><br><span class="line">            B6[recipe/]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;pytorch (PyTorch绑定)&quot;</span><br><span class="line">            C1[module/]</span><br><span class="line">            C2[attention/]</span><br><span class="line">            C3[quantization.py]</span><br><span class="line">            C4[distributed.py]</span><br><span class="line">            C5[tensor/]</span><br><span class="line">            C6[ops/]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;jax (JAX绑定)&quot;</span><br><span class="line">            D1[flax/]</span><br><span class="line">            D2[attention.py]</span><br><span class="line">            D3[dense.py]</span><br><span class="line">            D4[layernorm.py]</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    A --&gt; B1</span><br><span class="line">    A --&gt; C1</span><br><span class="line">    A --&gt; D1</span><br><span class="line">    B1 --&gt; B2</span><br><span class="line">    B1 --&gt; B3</span><br><span class="line">    B1 --&gt; B4</span><br><span class="line">    B1 --&gt; B5</span><br><span class="line">    C1 --&gt; C2</span><br><span class="line">    C1 --&gt; C3</span><br><span class="line">    C1 --&gt; C4</span><br><span class="line">    C1 --&gt; C5</span><br></pre></td></tr></table></figure><hr><h2 id="3-核心组件详细设计"><a href="#3-核心组件详细设计" class="headerlink" title="3. 核心组件详细设计"></a>3. 核心组件详细设计</h2><h3 id="3-1-Common-Layer-核心-C-库"><a href="#3-1-Common-Layer-核心-C-库" class="headerlink" title="3.1 Common Layer (核心 C++ 库)"></a>3.1 Common Layer (核心 C++ 库)</h3><h4 id="3-1-1-职责"><a href="#3-1-1-职责" class="headerlink" title="3.1.1 职责"></a>3.1.1 职责</h4><ul><li>实现框架无关的核心计算逻辑</li><li>管理 FP8&#x2F;FP4 量化和缩放因子</li><li>提供高性能 CUDA 内核</li><li>类型转换和内存管理</li></ul><h4 id="3-1-2-关键模块"><a href="#3-1-2-关键模块" class="headerlink" title="3.1.2 关键模块"></a>3.1.2 关键模块</h4><p><strong>transformer_engine.cpp</strong></p><ul><li>张量类型定义和验证</li><li>DType 枚举（kFloat32, kFloat16, kBFloat16, kFloat8E4M3, kFloat8E5M2, kFloat4E2M1）</li><li>缩放模式管理（NVTE_DELAYED_TENSOR_SCALING, NVTE_MXFP8_1D_SCALING, NVTE_BLOCK_SCALING_1D&#x2F;2D, NVTE_NVFP4_1D_SCALING）</li><li>张量合法性检查</li></ul><p><strong>gemm&#x2F; (矩阵乘法)</strong></p><ul><li><code>cublaslt_gemm.cu</code>：基于 cuBLASLt 的高性能 GEMM 实现<ul><li>支持 FP8&#x2F;FP16&#x2F;BF16 混合精度</li><li>Fast Accumulator、Epilogue Fusion、自动调优</li><li>详见 11.4 节 cuBLASLt 详细对比</li></ul></li><li><code>cutlass_grouped_gemm.cu</code>：基于 cutlass 的分组 GEMM（用于 MoE）</li></ul><p><strong>normalization&#x2F; (归一化)</strong></p><ul><li>LayerNorm、RMSNorm 的 FP8 实现</li><li>融合的 bias 和 dropout 操作</li><li>Zero-centered gamma 支持</li></ul><p><strong>fused_attn&#x2F; (融合注意力)</strong></p><ul><li>基于 cuDNN 的融合注意力实现</li><li>支持多种 mask 类型（causal, padding, arbitrary）</li><li>滑动窗口注意力（Sliding Window Attention）</li><li>FlashAttention 集成</li></ul><p><strong>activation&#x2F; (激活函数)</strong></p><ul><li>GELU, ReLU, SwiGLU, GEGLU 等</li><li>支持 FP8 输入输出</li><li>融合实现以减少内存访问</li></ul><h3 id="3-2-PyTorch-Adapter-Layer"><a href="#3-2-PyTorch-Adapter-Layer" class="headerlink" title="3.2 PyTorch Adapter Layer"></a>3.2 PyTorch Adapter Layer</h3><h4 id="3-2-1-Module-子系统"><a href="#3-2-1-Module-子系统" class="headerlink" title="3.2.1 Module 子系统"></a>3.2.1 Module 子系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">    class TransformerEngineBaseModule &#123;</span><br><span class="line">        +fp8: bool</span><br><span class="line">        +fp8_calibration: bool</span><br><span class="line">        +fp8_parameters: bool</span><br><span class="line">        +forward()</span><br><span class="line">        +_get_fp8_params()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class Linear &#123;</span><br><span class="line">        +in_features: int</span><br><span class="line">        +out_features: int</span><br><span class="line">        +weight: Parameter</span><br><span class="line">        +bias: Optional[Parameter]</span><br><span class="line">        +forward(inp: Tensor)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class LayerNormLinear &#123;</span><br><span class="line">        +normalization: str</span><br><span class="line">        +forward(inp: Tensor)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class LayerNormMLP &#123;</span><br><span class="line">        +activation: str</span><br><span class="line">        +ffn_hidden_size: int</span><br><span class="line">        +forward(inp: Tensor)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class GroupedLinear &#123;</span><br><span class="line">        +num_gemms: int</span><br><span class="line">        +forward(inp: Tensor)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    TransformerEngineBaseModule &lt;|-- Linear</span><br><span class="line">    TransformerEngineBaseModule &lt;|-- LayerNormLinear</span><br><span class="line">    TransformerEngineBaseModule &lt;|-- LayerNormMLP</span><br><span class="line">    TransformerEngineBaseModule &lt;|-- GroupedLinear</span><br></pre></td></tr></table></figure><p><strong>核心模块</strong>：</p><ol><li><strong>Linear</strong>：基础线性层，支持 FP8 权重和激活</li><li><strong>LayerNormLinear</strong>：融合 LayerNorm + Linear</li><li><strong>LayerNormMLP</strong>：融合 LayerNorm + MLP（两层线性层）</li><li><strong>GroupedLinear</strong>：分组线性层（用于 MoE 等场景）</li><li><strong>LayerNorm&#x2F;RMSNorm</strong>：归一化层</li></ol><h4 id="3-2-2-Attention-子系统"><a href="#3-2-2-Attention-子系统" class="headerlink" title="3.2.2 Attention 子系统"></a>3.2.2 Attention 子系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[MultiheadAttention] --&gt; B[DotProductAttention]</span><br><span class="line">    A --&gt; C[RotaryPositionEmbedding]</span><br><span class="line">    B --&gt; D[Fused Attention Backend]</span><br><span class="line">    B --&gt; E[Unfused Attention]</span><br><span class="line">    D --&gt; F[cuDNN Fused Attn]</span><br><span class="line">    D --&gt; G[FlashAttention]</span><br></pre></td></tr></table></figure><p><strong>关键特性</strong>：</p><ul><li>Multi-Query Attention (MQA)</li><li>Grouped Query Attention (GQA)</li><li>多种 mask 类型支持</li><li>RoPE（Rotary Position Embedding）</li><li>Sliding Window Attention</li><li>InferenceParams for KV cache</li><li>FP8 DPA (Dot Product Attention) 支持</li><li>FP8 MHA (Multi-Head Attention) 端到端优化</li></ul><h4 id="3-2-3-Quantization-子系统"><a href="#3-2-3-Quantization-子系统" class="headerlink" title="3.2.3 Quantization 子系统"></a>3.2.3 Quantization 子系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[fp8_autocast Context Manager] --&gt; B&#123;Recipe Type&#125;</span><br><span class="line">    B --&gt;|DelayedScaling| C[Delayed Scaling&lt;br/&gt;延迟缩放因子更新]</span><br><span class="line">    B --&gt;|Float8CurrentScaling| D[Current Scaling&lt;br/&gt;当前批次缩放]</span><br><span class="line">    B --&gt;|MXFP8BlockScaling| E[MXFP8 Block Scaling&lt;br/&gt;1D 块级缩放]</span><br><span class="line">    B --&gt;|Float8BlockScaling| F[FP8 Block Scaling&lt;br/&gt;1D/2D 块级缩放]</span><br><span class="line">    B --&gt;|NVFP4BlockScaling| G[NVFP4 Block Scaling&lt;br/&gt;4-bit 量化]</span><br><span class="line">    </span><br><span class="line">    C --&gt; H[FP8MetaManager]</span><br><span class="line">    D --&gt; H</span><br><span class="line">    E --&gt; H</span><br><span class="line">    F --&gt; H</span><br><span class="line">    G --&gt; H</span><br><span class="line">    </span><br><span class="line">    H --&gt; I[Forward/Backward Pass&lt;br/&gt;with FP8]</span><br></pre></td></tr></table></figure><p><strong>Recipe 系统</strong>：</p><ul><li><strong>DelayedScaling</strong>：使用历史 amax 统计更新缩放因子</li><li><strong>Float8CurrentScaling</strong>：基于当前批次的 amax</li><li><strong>MXFP8BlockScaling</strong>：Microscaling FP8（适用于 Blackwell）</li><li><strong>Float8BlockScaling</strong>：块级量化（1D&#x2F;2D）</li><li><strong>NVFP4BlockScaling</strong>：4-bit 量化（Blackwell 专用）</li></ul><h4 id="3-2-4-Tensor-子系统"><a href="#3-2-4-Tensor-子系统" class="headerlink" title="3.2.4 Tensor 子系统"></a>3.2.4 Tensor 子系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">    class QuantizedTensor &#123;</span><br><span class="line">        +data: Tensor</span><br><span class="line">        +scale_inv: Tensor</span><br><span class="line">        +quantizer: Quantizer</span><br><span class="line">        +dtype: DType</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class Float8Tensor &#123;</span><br><span class="line">        +Float8Quantizer</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class MXFP8Tensor &#123;</span><br><span class="line">        +MXFP8Quantizer</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class Float8BlockwiseQTensor &#123;</span><br><span class="line">        +Float8BlockQuantizer</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class NVFP4Tensor &#123;</span><br><span class="line">        +NVFP4Quantizer</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    QuantizedTensor &lt;|-- Float8Tensor</span><br><span class="line">    QuantizedTensor &lt;|-- MXFP8Tensor</span><br><span class="line">    QuantizedTensor &lt;|-- Float8BlockwiseQTensor</span><br><span class="line">    QuantizedTensor &lt;|-- NVFP4Tensor</span><br></pre></td></tr></table></figure><h3 id="3-3-JAX-Adapter-Layer"><a href="#3-3-JAX-Adapter-Layer" class="headerlink" title="3.3 JAX Adapter Layer"></a>3.3 JAX Adapter Layer</h3><h4 id="3-3-1-Flax-集成"><a href="#3-3-1-Flax-集成" class="headerlink" title="3.3.1 Flax 集成"></a>3.3.1 Flax 集成</h4><ul><li>提供 Flax 兼容的 Module</li><li>JAX JIT 编译支持</li><li>XLA FFI（Foreign Function Interface）集成</li></ul><h4 id="3-3-2-核心模块"><a href="#3-3-2-核心模块" class="headerlink" title="3.3.2 核心模块"></a>3.3.2 核心模块</h4><ul><li><code>Dense</code>：线性层</li><li><code>LayerNorm</code>：归一化层</li><li><code>DotProductAttention</code>：注意力机制</li><li><code>LayerNormMLP</code>：融合 MLP</li></ul><hr><h2 id="4-数据流与调用关系"><a href="#4-数据流与调用关系" class="headerlink" title="4. 数据流与调用关系"></a>4. 数据流与调用关系</h2><blockquote><p><strong>时序图符号说明</strong>：</p><ul><li><code>A-&gt;&gt;B</code>（实线箭头）：A 调用 B 的方法或发送消息</li><li><code>B--&gt;&gt;A</code>（虚线箭头）：B 返回结果给 A（函数返回值或响应）</li><li><code>activate/deactivate</code>：表示组件处于活跃状态的生命周期</li><li>箭头上的文字：说明具体的调用或返回内容</li></ul></blockquote><h3 id="4-1-前向传播数据流"><a href="#4-1-前向传播数据流" class="headerlink" title="4.1 前向传播数据流"></a>4.1 前向传播数据流</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant User as 用户代码</span><br><span class="line">    participant Autocast as fp8_autocast</span><br><span class="line">    participant Module as TE Module&lt;br/&gt;(e.g., Linear)</span><br><span class="line">    participant Quantizer as Quantizer</span><br><span class="line">    participant CPP as C++ Backend</span><br><span class="line">    participant CUDA as CUDA Kernel</span><br><span class="line">    </span><br><span class="line">    User-&gt;&gt;Autocast: with fp8_autocast(recipe)</span><br><span class="line">    activate Autocast</span><br><span class="line">    Autocast-&gt;&gt;Autocast: 设置 FP8 Recipe</span><br><span class="line">    User-&gt;&gt;Module: forward(input)</span><br><span class="line">    activate Module</span><br><span class="line">    </span><br><span class="line">    Module-&gt;&gt;Quantizer: 量化输入 (FP32/BF16 → FP8)</span><br><span class="line">    Quantizer-&gt;&gt;CPP: nvte_cast_to_fp8()</span><br><span class="line">    CPP-&gt;&gt;CUDA: Cast Kernel</span><br><span class="line">    CUDA--&gt;&gt;CPP: FP8 Tensor</span><br><span class="line">    CPP--&gt;&gt;Quantizer: FP8 Tensor</span><br><span class="line">    Quantizer--&gt;&gt;Module: FP8 Input</span><br><span class="line">    </span><br><span class="line">    Module-&gt;&gt;CPP: nvte_fp8_gemm(input, weight)</span><br><span class="line">    CPP-&gt;&gt;CUDA: cuBLASLt FP8 GEMM</span><br><span class="line">    CUDA--&gt;&gt;CPP: FP8 Output</span><br><span class="line">    CPP--&gt;&gt;Module: FP8 Output</span><br><span class="line">    </span><br><span class="line">    Module-&gt;&gt;Quantizer: 反量化输出 (FP8 → FP32/BF16)</span><br><span class="line">    Quantizer-&gt;&gt;CPP: nvte_cast_from_fp8()</span><br><span class="line">    CPP-&gt;&gt;CUDA: Cast Kernel</span><br><span class="line">    CUDA--&gt;&gt;CPP: High Precision Tensor</span><br><span class="line">    CPP--&gt;&gt;Quantizer: Output</span><br><span class="line">    Quantizer--&gt;&gt;Module: Output</span><br><span class="line">    </span><br><span class="line">    Module--&gt;&gt;User: Output</span><br><span class="line">    deactivate Module</span><br><span class="line">    deactivate Autocast</span><br></pre></td></tr></table></figure><h3 id="4-2-反向传播与缩放因子更新"><a href="#4-2-反向传播与缩放因子更新" class="headerlink" title="4.2 反向传播与缩放因子更新"></a>4.2 反向传播与缩放因子更新</h3><blockquote><p><strong>特别说明</strong>：反向传播中的箭头方向与 forward 不同</p><ul><li><strong>实线箭头</strong>：既可以表示”调用 backward()”，也可以表示”传递梯度”</li><li>在 PyTorch Autograd 中，<code>backward()</code> 不是简单的返回值，而是<strong>链式调用</strong>机制</li><li>每个模块计算完梯度后，会<strong>主动调用</strong>前一个节点的 <code>backward()</code>，因此用实线</li></ul></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant Loss as Loss.backward()</span><br><span class="line">    participant Module as TE Module</span><br><span class="line">    participant Autograd as PyTorch Autograd</span><br><span class="line">    participant CPP as C++ Backend</span><br><span class="line">    participant Recipe as FP8 Recipe</span><br><span class="line">    </span><br><span class="line">    Loss-&gt;&gt;Autograd: 触发反向传播</span><br><span class="line">    Autograd-&gt;&gt;Module: backward()</span><br><span class="line">    activate Module</span><br><span class="line">    </span><br><span class="line">    Module-&gt;&gt;CPP: nvte_fp8_gemm_bwd()</span><br><span class="line">    CPP-&gt;&gt;CPP: 计算梯度 (FP8)</span><br><span class="line">    CPP--&gt;&gt;Module: 梯度（返回值）</span><br><span class="line">    </span><br><span class="line">    Note over Module: 计算完成后触发前一层</span><br><span class="line">    Module-&gt;&gt;Autograd: 调用前一层 backward()</span><br><span class="line">    Note right of Module: 这是主动调用，&lt;br/&gt;不是被动返回</span><br><span class="line">    </span><br><span class="line">    Module-&gt;&gt;Recipe: 收集 amax 统计</span><br><span class="line">    Recipe-&gt;&gt;Recipe: 更新缩放因子历史</span><br><span class="line">    </span><br><span class="line">    alt DelayedScaling</span><br><span class="line">        Recipe-&gt;&gt;Recipe: amax_history.append(current_amax)</span><br><span class="line">        Recipe-&gt;&gt;Recipe: scale = max(amax_history) * margin</span><br><span class="line">    else Float8CurrentScaling</span><br><span class="line">        Recipe-&gt;&gt;Recipe: scale = current_amax * margin</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Recipe--&gt;&gt;Module: 更新的缩放因子</span><br><span class="line">    Module-&gt;&gt;Autograd: 梯度传播（调用前一层）</span><br><span class="line">    deactivate Module</span><br></pre></td></tr></table></figure><p><strong>关键概念</strong>：</p><ul><li><strong>Forward Pass</strong>：<code>A → B → C</code>，每层返回输出（虚线返回）</li><li><strong>Backward Pass</strong>：<code>C.backward() → B.backward() → A.backward()</code>，链式调用（实线调用）</li><li>Backward 不是简单的 return，而是触发前一层的计算（Autograd 的核心机制）</li></ul><h3 id="4-3-TransformerLayer-完整调用链"><a href="#4-3-TransformerLayer-完整调用链" class="headerlink" title="4.3 TransformerLayer 完整调用链"></a>4.3 TransformerLayer 完整调用链</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[TransformerLayer.forward] --&gt; B&#123;Parallel Attn-MLP?&#125;</span><br><span class="line">    </span><br><span class="line">    B --&gt;|No| C[Self Attention]</span><br><span class="line">    C --&gt; D[Residual + Dropout]</span><br><span class="line">    D --&gt; E[LayerNorm]</span><br><span class="line">    E --&gt; F&#123;Layer Type?&#125;</span><br><span class="line">    F --&gt;|Encoder| G[MLP]</span><br><span class="line">    F --&gt;|Decoder| H[&quot;Cross Attention&lt;br/&gt;(需要Encoder输出)&quot;]</span><br><span class="line">    H --&gt; I[Residual + Dropout]</span><br><span class="line">    I --&gt; G</span><br><span class="line">    G --&gt; J[Residual + Dropout]</span><br><span class="line">    J --&gt; K[Output]</span><br><span class="line">    </span><br><span class="line">    B --&gt;|Yes| L[&quot;输入: hidden_states&quot;]</span><br><span class="line">    L --&gt; M[Self Attention Branch]</span><br><span class="line">    L --&gt; N[MLP Branch]</span><br><span class="line">    M --&gt; O[&quot;直接相加&lt;br/&gt;(不含LayerNorm)&lt;br/&gt;+ Residual&quot;]</span><br><span class="line">    N --&gt; O</span><br><span class="line">    O --&gt; K</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Self Attention Detail&quot;</span><br><span class="line">        C --&gt; C1[MultiheadAttention]</span><br><span class="line">        C1 --&gt; C2[&quot;QKV Projection&lt;br/&gt;(LayerNormLinear)&quot;]</span><br><span class="line">        C2 --&gt; C3[DotProductAttention]</span><br><span class="line">        C3 --&gt; C4[&quot;Output Projection&lt;br/&gt;(Linear)&quot;]</span><br><span class="line">        C4 --&gt; D</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;MLP Detail&quot;</span><br><span class="line">        G --&gt; G1[LayerNormMLP]</span><br><span class="line">        G1 --&gt; G2[FC1 + Activation]</span><br><span class="line">        G2 --&gt; G3[FC2]</span><br><span class="line">        G3 --&gt; J</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><p><strong>关键数据流说明</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ========== 代码对应（transformer.py 756-840 行） ==========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模式1: 标准顺序模式 (parallel_attention_mlp=False)</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 1. Self Attention 完整流程</span></span><br><span class="line">attention_output = <span class="variable language_">self</span>.self_attention(hidden_states)</span><br><span class="line">    <span class="comment"># MultiheadAttention 内部流程 (multi_head_attention.py 773-1002 行)：</span></span><br><span class="line">    <span class="comment"># hidden_states → LayerNormLinear (含LayerNorm) → QKV分离</span></span><br><span class="line">    <span class="comment"># → DotProductAttention (Q*K^T, softmax, *V)</span></span><br><span class="line">    <span class="comment"># → Output Projection Linear → attention_output</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 第一次残差连接 + Dropout</span></span><br><span class="line">hidden_states = bias_dropout_add(attention_output, residual)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Decoder 模式额外的 Cross Attention</span></span><br><span class="line"><span class="keyword">if</span> layer_type == <span class="string">&quot;decoder&quot;</span>:</span><br><span class="line">    <span class="comment"># Query 来自 Self Attention 输出，Key/Value 来自 Encoder 输出</span></span><br><span class="line">    cross_attn_output = <span class="variable language_">self</span>.inter_attention(</span><br><span class="line">        hidden_states,  <span class="comment"># Query</span></span><br><span class="line">        encoder_output=encoder_output  <span class="comment"># Key/Value</span></span><br><span class="line">    )</span><br><span class="line">    hidden_states = bias_dropout_add(cross_attn_output, hidden_states)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. MLP 流程</span></span><br><span class="line">mlp_output = <span class="variable language_">self</span>.layernorm_mlp(hidden_states)</span><br><span class="line">    <span class="comment"># 内部流程：LayerNorm → FC1 → Activation → FC2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 第二次残差连接 + Dropout</span></span><br><span class="line">output = bias_dropout_add(mlp_output, hidden_states)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模式2: 并行注意力-MLP模式 (parallel_attention_mlp=True)</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 注意力和MLP并行计算，然后直接相加</span></span><br><span class="line">self_attention_outputs = <span class="variable language_">self</span>.self_attention(hidden_states)</span><br><span class="line">mlp_outputs = <span class="variable language_">self</span>.layernorm_mlp(hidden_states)  <span class="comment"># 同时计算</span></span><br><span class="line">output = bias_dropout_add(</span><br><span class="line">    self_attention_outputs + mlp_outputs,  <span class="comment"># 直接相加</span></span><br><span class="line">    hidden_states  <span class="comment"># 残差</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：很多架构图会将 Self Attention 画成一个整体模块，从顶部引出输出线。这种表示方法虽然简洁，但容易让人误解 Output Projection Linear 没有对外输出。实际上，<strong>Output Projection 的输出就是整个 Self Attention 模块的输出</strong>。</p><hr><h3 id="4-4-架构图常见误解与代码对照"><a href="#4-4-架构图常见误解与代码对照" class="headerlink" title="4.4 架构图常见误解与代码对照"></a>4.4 架构图常见误解与代码对照</h3><p>通过代码 review，以下是容易误解的几个要点：</p><table><thead><tr><th>架构图表示</th><th>常见误解</th><th>实际代码实现</th><th>代码位置</th></tr></thead><tbody><tr><td><strong>QKV Projection (LayerNormLinear)</strong></td><td>LayerNorm 在投影之前单独存在</td><td>LayerNormLinear 内部<strong>已包含</strong> LayerNorm<br/>是融合实现</td><td><code>multi_head_attention.py:773</code><br/><code>self.qkv(hidden_states)</code></td></tr><tr><td><strong>Output Projection 无输出连接</strong></td><td>只有顶部的连接，底部没输出</td><td>Output Projection 的输出<strong>就是</strong> Self Attention 模块的输出<br/>直接连到 Residual + Dropout</td><td><code>multi_head_attention.py:1002-1018</code><br/><code>return (attention_output, ...)</code></td></tr><tr><td><strong>并行模式的”合并”</strong></td><td>两个分支各自做residual再合并</td><td>两个分支输出<strong>直接相加</strong><br/>只做一次 residual</td><td><code>transformer.py:836-840</code><br/><code>bias_dropout_add(attn + mlp, residual)</code></td></tr><tr><td><strong>Decoder Cross Attention</strong></td><td>图中只显示单输入</td><td>需要<strong>两个输入</strong>：<br/>• Query: Self Attn 输出<br/>• Key&#x2F;Value: Encoder 输出</td><td><code>transformer.py:789-819</code><br/><code>inter_attention(hidden_states, encoder_output)</code></td></tr><tr><td><strong>LayerNorm 位置</strong></td><td>在每个模块外部</td><td>Pre-LN：在模块<strong>内部</strong><br/>Post-LN：在模块外部<br/>TE 默认使用 Pre-LN</td><td><code>multi_head_attention.py:input_layernorm=True</code></td></tr></tbody></table><p><strong>关键发现</strong>：</p><ol><li><strong>融合算子</strong>：<code>LayerNormLinear</code> 和 <code>LayerNormMLP</code> 都是融合实现，内部包含 LayerNorm</li><li><strong>Pre-Layer Norm</strong>：TE 默认使用 Pre-LN 架构（LayerNorm 在 Attention&#x2F;MLP 之前）</li><li><strong>残差连接时机</strong>：每个主要模块（Attention、MLP）之后都有残差连接</li><li><strong>并行模式</strong>：是真正的并行（同时计算），不是串行后伪装的并行</li></ol><hr><h2 id="5-关键技术实现"><a href="#5-关键技术实现" class="headerlink" title="5. 关键技术实现"></a>5. 关键技术实现</h2><h3 id="5-1-FP8-量化策略"><a href="#5-1-FP8-量化策略" class="headerlink" title="5.1 FP8 量化策略"></a>5.1 FP8 量化策略</h3><h4 id="5-1-1-延迟缩放（Delayed-Scaling）"><a href="#5-1-1-延迟缩放（Delayed-Scaling）" class="headerlink" title="5.1.1 延迟缩放（Delayed Scaling）"></a>5.1.1 延迟缩放（Delayed Scaling）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DelayedScaling</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, margin=<span class="number">0</span>, interval=<span class="number">1</span>, fp8_format=E4M3</span>):</span><br><span class="line">        <span class="variable language_">self</span>.amax_history = deque(maxlen=interval)</span><br><span class="line">        <span class="variable language_">self</span>.margin = margin</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_scale</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        amax = tensor.<span class="built_in">abs</span>().<span class="built_in">max</span>()</span><br><span class="line">        <span class="variable language_">self</span>.amax_history.append(amax)</span><br><span class="line">        scale = <span class="built_in">max</span>(<span class="variable language_">self</span>.amax_history) * (<span class="number">1</span> + <span class="variable language_">self</span>.margin) / fp8_max</span><br><span class="line">        <span class="keyword">return</span> scale</span><br></pre></td></tr></table></figure><h4 id="5-1-2-块级缩放（Block-Scaling）"><a href="#5-1-2-块级缩放（Block-Scaling）" class="headerlink" title="5.1.2 块级缩放（Block Scaling）"></a>5.1.2 块级缩放（Block Scaling）</h4><ul><li>将张量分成多个块，每个块独立计算缩放因子</li><li>减少量化误差，提高精度</li><li>适用于权重矩阵和激活</li></ul><h3 id="5-2-融合算子优化"><a href="#5-2-融合算子优化" class="headerlink" title="5.2 融合算子优化"></a>5.2 融合算子优化</h3><h4 id="5-2-1-LayerNorm-Linear-融合"><a href="#5-2-1-LayerNorm-Linear-融合" class="headerlink" title="5.2.1 LayerNorm + Linear 融合"></a>5.2.1 LayerNorm + Linear 融合</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">输入: [seq_len, batch, hidden_dim]</span><br><span class="line">↓</span><br><span class="line">LayerNorm (融合 bias + dropout)</span><br><span class="line">↓</span><br><span class="line">FP8 Cast</span><br><span class="line">↓</span><br><span class="line">FP8 GEMM (weight 预先量化)</span><br><span class="line">↓</span><br><span class="line">FP8 Cast Back</span><br><span class="line">↓</span><br><span class="line">输出: [seq_len, batch, out_dim]</span><br></pre></td></tr></table></figure><p><strong>优势</strong>：</p><ul><li>减少内存访问次数</li><li>降低量化&#x2F;反量化开销</li><li>提高 GPU 利用率</li></ul><h4 id="5-2-2-Fused-Attention"><a href="#5-2-2-Fused-Attention" class="headerlink" title="5.2.2 Fused Attention"></a>5.2.2 Fused Attention</h4><ul><li>使用 cuDNN 融合的注意力实现</li><li>支持 FlashAttention-2 后端</li><li>自动选择最优实现</li></ul><h3 id="5-3-分布式训练支持"><a href="#5-3-分布式训练支持" class="headerlink" title="5.3 分布式训练支持"></a>5.3 分布式训练支持</h3><h4 id="5-3-1-User-Buffer-UB"><a href="#5-3-1-User-Buffer-UB" class="headerlink" title="5.3.1 User Buffer (UB)"></a>5.3.1 User Buffer (UB)</h4><ul><li>重叠通信和计算</li><li>支持 FP8 通信</li><li>基于 NVSHMEM 的低延迟通信</li></ul><h4 id="5-3-2-Checkpoint"><a href="#5-3-2-Checkpoint" class="headerlink" title="5.3.2 Checkpoint"></a>5.3.2 Checkpoint</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分布式 checkpoint</span></span><br><span class="line"><span class="keyword">from</span> transformer_engine.pytorch <span class="keyword">import</span> checkpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line">checkpoint(model, optimizer, save_dir, dist_group)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载</span></span><br><span class="line">checkpoint.load(model, optimizer, load_dir, dist_group)</span><br></pre></td></tr></table></figure><hr><h2 id="6-性能优化策略"><a href="#6-性能优化策略" class="headerlink" title="6. 性能优化策略"></a>6. 性能优化策略</h2><h3 id="6-1-内存优化"><a href="#6-1-内存优化" class="headerlink" title="6.1 内存优化"></a>6.1 内存优化</h3><ol><li><strong>FP8 量化</strong>：减少 50% 内存占用</li><li><strong>Gradient Checkpointing</strong>：重计算中间激活</li><li><strong>CPU Offloading</strong>：将激活值卸载到 CPU 内存<ul><li>使用 <code>get_cpu_offload_context()</code> 管理卸载</li><li>支持异步数据传输，减少性能影响</li><li>适用于超大模型训练</li></ul></li></ol><h3 id="6-2-计算优化"><a href="#6-2-计算优化" class="headerlink" title="6.2 计算优化"></a>6.2 计算优化</h3><ol><li><strong>Tensor Core 利用</strong>：FP8 Tensor Core 吞吐量是 FP16 的 2 倍</li><li><strong>融合算子</strong>：减少 kernel launch 开销</li><li><strong>cuDNN 集成</strong>：使用 cuDNN 优化的注意力实现</li></ol><h3 id="6-3-通信优化"><a href="#6-3-通信优化" class="headerlink" title="6.3 通信优化"></a>6.3 通信优化</h3><ol><li><strong>FP8 All-Reduce</strong>：减少通信数据量</li><li><strong>通信计算重叠</strong>：User Buffer 机制</li><li><strong>NVSHMEM</strong>：低延迟点对点通信</li></ol><hr><h2 id="7-扩展性与依赖"><a href="#7-扩展性与依赖" class="headerlink" title="7. 扩展性与依赖"></a>7. 扩展性与依赖</h2><h3 id="7-1-自定义-Recipe"><a href="#7-1-自定义-Recipe" class="headerlink" title="7.1 自定义 Recipe"></a>7.1 自定义 Recipe</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformer_engine.common.recipe <span class="keyword">import</span> CustomRecipe</span><br><span class="line"></span><br><span class="line">custom_recipe = CustomRecipe(</span><br><span class="line">    margin=<span class="number">0.1</span>, fp8_format=Format.HYBRID,</span><br><span class="line">    amax_history_len=<span class="number">10</span>, amax_compute_algo=<span class="string">&#x27;max&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="7-2-自定义算子"><a href="#7-2-自定义算子" class="headerlink" title="7.2 自定义算子"></a>7.2 自定义算子</h3><p>通过 C++ API 扩展新算子，使用 Pybind11 暴露到 Python</p><h3 id="7-3-框架支持"><a href="#7-3-框架支持" class="headerlink" title="7.3 框架支持"></a>7.3 框架支持</h3><p>PyTorch（完整支持）、JAX&#x2F;Flax、其他框架（通过 C++ API）</p><h3 id="7-4-依赖关系"><a href="#7-4-依赖关系" class="headerlink" title="7.4 依赖关系"></a>7.4 依赖关系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[TransformerEngine] --&gt; B[CUDA 12.1+]</span><br><span class="line">    A --&gt; C[cuDNN]</span><br><span class="line">    A --&gt; D[cuBLASLt]</span><br><span class="line">    A --&gt; E[PyTorch 2.1+ / JAX]</span><br><span class="line">    A --&gt; F[cutlass]</span><br><span class="line">    A -.-&gt; G[NCCL/MPI/NVSHMEM&lt;br/&gt;可选]</span><br></pre></td></tr></table></figure><hr><h2 id="8-使用示例"><a href="#8-使用示例" class="headerlink" title="8. 使用示例"></a>8. 使用示例</h2><h3 id="PyTorch-基础示例"><a href="#PyTorch-基础示例" class="headerlink" title="PyTorch 基础示例"></a>PyTorch 基础示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformer_engine.pytorch <span class="keyword">as</span> te</span><br><span class="line"><span class="keyword">from</span> transformer_engine.common <span class="keyword">import</span> recipe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = te.Linear(<span class="number">768</span>, <span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 FP8 Recipe</span></span><br><span class="line">fp8_recipe = recipe.DelayedScaling(</span><br><span class="line">    margin=<span class="number">0</span>,</span><br><span class="line">    fp8_format=recipe.Format.HYBRID</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播（启用 FP8）</span></span><br><span class="line"><span class="keyword">with</span> te.fp8_autocast(enabled=<span class="literal">True</span>, recipe=fp8_recipe):</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><h3 id="TransformerLayer-示例"><a href="#TransformerLayer-示例" class="headerlink" title="TransformerLayer 示例"></a>TransformerLayer 示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">layer = te.TransformerLayer(</span><br><span class="line">    hidden_size=<span class="number">1024</span>,</span><br><span class="line">    ffn_hidden_size=<span class="number">4096</span>,</span><br><span class="line">    num_attention_heads=<span class="number">16</span>,</span><br><span class="line">    num_gqa_groups=<span class="number">8</span>,  <span class="comment"># Grouped Query Attention</span></span><br><span class="line">    layernorm_epsilon=<span class="number">1e-5</span>,</span><br><span class="line">    hidden_dropout=<span class="number">0.1</span>,</span><br><span class="line">    attention_dropout=<span class="number">0.1</span>,</span><br><span class="line">    self_attn_mask_type=<span class="string">&#x27;causal&#x27;</span>,</span><br><span class="line">    normalization=<span class="string">&#x27;RMSNorm&#x27;</span>,</span><br><span class="line">    activation=<span class="string">&#x27;swiglu&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> te.fp8_autocast(enabled=<span class="literal">True</span>, recipe=fp8_recipe):</span><br><span class="line">    output = layer(hidden_states, attention_mask)</span><br></pre></td></tr></table></figure><h3 id="CPU-Offload-示例"><a href="#CPU-Offload-示例" class="headerlink" title="CPU Offload 示例"></a>CPU Offload 示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformer_engine.pytorch <span class="keyword">import</span> get_cpu_offload_context</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CPU Offload 上下文</span></span><br><span class="line">cpu_offload_ctx, sync_fn = get_cpu_offload_context(</span><br><span class="line">    enabled=<span class="literal">True</span>,</span><br><span class="line">    num_layers=<span class="number">24</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环中使用</span></span><br><span class="line"><span class="keyword">with</span> cpu_offload_ctx:</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = criterion(output, target)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 同步等待异步传输完成</span></span><br><span class="line">sync_fn()</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><hr><h2 id="9-架构设计优势"><a href="#9-架构设计优势" class="headerlink" title="9. 架构设计优势"></a>9. 架构设计优势</h2><ol><li><strong>模块化</strong>：清晰分层（User API → Adapter → Core → Hardware），松耦合易扩展</li><li><strong>高性能</strong>：零拷贝设计、融合算子、硬件感知优化</li><li><strong>易用性</strong>：Pythonic API、自动管理缩放因子、灵活的 Recipe 系统</li><li><strong>工业级</strong>：完备测试、齐全文档、持续集成</li></ol><hr><h2 id="10-技术挑战与解决方案"><a href="#10-技术挑战与解决方案" class="headerlink" title="10. 技术挑战与解决方案"></a>10. 技术挑战与解决方案</h2><h3 id="10-1-数值稳定性"><a href="#10-1-数值稳定性" class="headerlink" title="10.1 数值稳定性"></a>10.1 数值稳定性</h3><p><strong>挑战</strong>：FP8 动态范围小，容易溢出&#x2F;下溢<br><strong>解决方案</strong>：</p><ul><li>Delayed Scaling 策略</li><li>Amax 历史追踪</li><li>Margin 参数调整</li></ul><h3 id="10-2-性能瓶颈"><a href="#10-2-性能瓶颈" class="headerlink" title="10.2 性能瓶颈"></a>10.2 性能瓶颈</h3><p><strong>挑战</strong>：量化&#x2F;反量化开销<br><strong>解决方案</strong>：</p><ul><li>融合算子</li><li>预量化权重</li><li>端到端 FP8 流程</li></ul><h3 id="10-3-框架兼容性"><a href="#10-3-框架兼容性" class="headerlink" title="10.3 框架兼容性"></a>10.3 框架兼容性</h3><p><strong>挑战</strong>：不同框架的 autograd 机制不同<br><strong>解决方案</strong>：</p><ul><li>框架无关的 C++ 核心</li><li>适配层抽象差异</li><li>Custom Autograd Functions</li></ul><h3 id="10-4-cuBLASLt-vs-cuBLAS-选择"><a href="#10-4-cuBLASLt-vs-cuBLAS-选择" class="headerlink" title="10.4 cuBLASLt vs cuBLAS 选择"></a>10.4 cuBLASLt vs cuBLAS 选择</h3><p><strong>挑战</strong>：为什么使用 cuBLASLt 而不是传统 cuBLAS？<br><strong>原因分析</strong>：</p><ul><li><strong>FP8 支持</strong>：cuBLAS 不支持 FP8 数据类型，cuBLASLt 从 CUDA 11.8+ 开始原生支持</li><li><strong>灵活性</strong>：cuBLASLt 的描述符（Descriptor）API 允许精细控制每个操作细节</li><li><strong>性能优化</strong>：<ul><li>Fast Accumulator（快速累加器）：针对 Hopper 架构的分块累加优化</li><li>Epilogue Fusion：将 bias、激活函数等后处理融合到 GEMM kernel 中</li><li>自动调优：根据矩阵大小和硬件特性自动选择最优算法</li></ul></li><li><strong>缩放因子集成</strong>：直接支持 FP8 的 scale 和 scale_inv 参数，无需额外的 kernel launch</li></ul><p><strong>代码对比</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cuBLAS (传统 API，不支持 FP8)</span></span><br><span class="line"><span class="built_in">cublasSgemm</span>(handle, CUBLAS_OP_N, CUBLAS_OP_N, </span><br><span class="line">            m, n, k, &amp;alpha, A, lda, B, ldb, &amp;beta, C, ldc);</span><br><span class="line"></span><br><span class="line"><span class="comment">// cuBLASLt (描述符 API，支持 FP8)</span></span><br><span class="line"><span class="built_in">cublasLtMatmul</span>(handle, matmulDesc, &amp;alpha,</span><br><span class="line">               A, Adesc,  <span class="comment">// 可指定 FP8 类型和缩放因子</span></span><br><span class="line">               B, Bdesc, </span><br><span class="line">               &amp;beta, C, Cdesc, C, Cdesc,</span><br><span class="line">               &amp;algo, workspace, workspaceSize, stream);</span><br></pre></td></tr></table></figure><hr><h2 id="11-未来发展方向"><a href="#11-未来发展方向" class="headerlink" title="11. 未来发展方向"></a>11. 未来发展方向</h2><h3 id="12-1-新硬件支持"><a href="#12-1-新硬件支持" class="headerlink" title="12.1 新硬件支持"></a>12.1 新硬件支持</h3><ul><li><strong>Blackwell 架构优化</strong>：MXFP8、NVFP4</li><li><strong>多 GPU 架构</strong>：更好的多卡支持</li></ul><h3 id="12-2-新功能"><a href="#12-2-新功能" class="headerlink" title="12.2 新功能"></a>12.2 新功能</h3><ul><li><strong>更多融合算子</strong>：Softmax, Dropout 等</li><li><strong>量化感知训练（QAT）</strong></li><li><strong>混合精度策略优化</strong></li></ul><h3 id="12-3-生态系统集成"><a href="#12-3-生态系统集成" class="headerlink" title="12.3 生态系统集成"></a>12.3 生态系统集成</h3><ul><li><strong>Megatron-LM 集成</strong></li><li><strong>HuggingFace Transformers 支持</strong></li><li><strong>ONNX 导出优化</strong></li></ul><hr><h2 id="12-总结"><a href="#12-总结" class="headerlink" title="12. 总结"></a>12. 总结</h2><p>Transformer Engine 是 NVIDIA 开发的<strong>高度模块化、性能优先、易于使用</strong>的 Transformer 加速库。其核心架构设计为大规模 Transformer 模型的高效训练和推理提供了坚实基础，特点包括：分层清晰、框架无关、高性能 FP8 量化、易扩展的 Recipe 系统，以及工业级的测试和文档体系。</p><hr><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><strong>关键文件</strong>：</p><ul><li>C++ 核心：<code>transformer_engine/common/</code> (transformer_engine.cpp, gemm&#x2F;, fused_attn&#x2F;, normalization&#x2F;)</li><li>PyTorch：<code>transformer_engine/pytorch/</code> (module&#x2F;, attention&#x2F;, quantization.py)</li><li>JAX：<code>transformer_engine/jax/</code> (flax&#x2F;, attention.py)</li></ul><p><strong>资源链接</strong>：</p><ul><li>官方文档：<a href="https://docs.nvidia.com/deeplearning/transformer-engine/">https://docs.nvidia.com/deeplearning/transformer-engine/</a></li><li>GitHub：<a href="https://github.com/NVIDIA/TransformerEngine">https://github.com/NVIDIA/TransformerEngine</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 系统架构分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NVIDIA </tag>
            
            <tag> Transformer Engine </tag>
            
            <tag> FP8 </tag>
            
            <tag> 混合精度 </tag>
            
            <tag> GPU加速 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Megatron-LM 架构深度分析</title>
      <link href="/2025/11/17/Megatron_DeepDive/"/>
      <url>/2025/11/17/Megatron_DeepDive/</url>
      
        <content type="html"><![CDATA[<h1 id="Megatron-LM-代码库架构分析报告"><a href="#Megatron-LM-代码库架构分析报告" class="headerlink" title="Megatron-LM 代码库架构分析报告"></a>Megatron-LM 代码库架构分析报告</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><a href="#1-%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0">项目概述</a></li><li><a href="#2-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84">整体架构</a></li><li><a href="#3-%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90">核心模块分析</a></li><li><a href="#4-%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5%E8%AF%A6%E8%A7%A3">并行策略详解</a></li><li><a href="#5-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0">模型实现</a></li><li><a href="#6-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B">训练流程</a></li><li><a href="#7-%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%89%B9%E6%80%A7">关键技术特性</a></li><li><a href="#8-%E4%BB%A3%E7%A0%81%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84">代码组织结构</a></li></ol><hr><h2 id="1-项目概述"><a href="#1-项目概述" class="headerlink" title="1. 项目概述"></a>1. 项目概述</h2><h3 id="1-1-项目定位"><a href="#1-1-项目定位" class="headerlink" title="1.1 项目定位"></a>1.1 项目定位</h3><p>Megatron-LM 是 NVIDIA 开发的用于大规模 Transformer 模型训练的 GPU 优化库，包含两个核心部分：</p><ul><li><strong>Megatron-LM</strong>: 参考实现，包含完整的训练脚本和工具</li><li><strong>Megatron Core</strong>: 可组合的生产级库，提供模块化的构建块</li></ul><h3 id="1-2-主要特点"><a href="#1-2-主要特点" class="headerlink" title="1.2 主要特点"></a>1.2 主要特点</h3><ul><li>GPU 优化的 Transformer 实现</li><li>多种并行策略（TP、PP、DP、EP、CP）</li><li>支持多种模型架构（GPT、LLaMA、Mixtral、Mamba、DeepSeek-V3 等）</li><li>FP8、FP16、BF16、FP4 混合精度训练</li><li>分布式优化器和检查点</li><li>MoE（Mixture of Experts）支持，包括 Shared Experts</li><li>MLA（Multi-Latent Attention）高效注意力机制</li><li>动态推理引擎（Dynamic Inference Engine）</li><li>容错训练（NVRx 集成）</li><li>HyperCommGrid N维通信网格管理</li><li>推理引擎和模型导出（TensorRT-LLM）</li></ul><h3 id="1-3-生态系统"><a href="#1-3-生态系统" class="headerlink" title="1.3 生态系统"></a>1.3 生态系统</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Dependencies 依赖库&quot;</span><br><span class="line">        TE[Transformer Engine&lt;br/&gt;FP8优化内核]</span><br><span class="line">        Energon[Megatron Energon&lt;br/&gt;多模态数据加载器]</span><br><span class="line">        NVRx[NVRx&lt;br/&gt;容错训练]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Core 核心库&quot;</span><br><span class="line">        MCore[Megatron Core&lt;br/&gt;核心构建块]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Applications 应用层&quot;</span><br><span class="line">        MLM[Megatron-LM&lt;br/&gt;参考实现]</span><br><span class="line">        Bridge[Megatron Bridge&lt;br/&gt;HF互操作]</span><br><span class="line">        NeMo[NeMo Framework&lt;br/&gt;企业框架]</span><br><span class="line">        NeMoRL[NeMo RL&lt;br/&gt;RLHF训练]</span><br><span class="line">        ModelOpt[TensorRT ModelOpt&lt;br/&gt;模型优化]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    TE --&gt; MCore</span><br><span class="line">    Energon --&gt; MCore</span><br><span class="line">    NVRx --&gt; MCore</span><br><span class="line">    </span><br><span class="line">    MCore --&gt; MLM</span><br><span class="line">    MCore --&gt; Bridge</span><br><span class="line">    MCore --&gt; NeMo</span><br><span class="line">    MCore --&gt; NeMoRL</span><br><span class="line">    MCore --&gt; ModelOpt</span><br><span class="line">    </span><br><span class="line">    style MCore fill:#4CAF50</span><br><span class="line">    style MLM fill:#2196F3</span><br></pre></td></tr></table></figure><hr><h2 id="2-整体架构"><a href="#2-整体架构" class="headerlink" title="2. 整体架构"></a>2. 整体架构</h2><h3 id="2-1-项目结构"><a href="#2-1-项目结构" class="headerlink" title="2.1 项目结构"></a>2.1 项目结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Megatron-LM/</span><br><span class="line">├── megatron/                          # 核心代码目录</span><br><span class="line">│   ├── core/                          # Megatron Core 生产库</span><br><span class="line">│   │   ├── models/                    # 模型实现</span><br><span class="line">│   │   │   ├── gpt/                   # GPT 模型</span><br><span class="line">│   │   │   ├── bert/                  # BERT 模型</span><br><span class="line">│   │   │   ├── T5/                    # T5 模型</span><br><span class="line">│   │   │   ├── mamba/                 # Mamba（SSM）模型</span><br><span class="line">│   │   │   ├── multimodal/            # 多模态模型</span><br><span class="line">│   │   │   ├── retro/                 # RETRO 模型</span><br><span class="line">│   │   │   └── vision/                # 视觉模型</span><br><span class="line">│   │   ├── transformer/               # Transformer 构建块</span><br><span class="line">│   │   │   ├── transformer_layer.py   # Transformer 层</span><br><span class="line">│   │   │   ├── transformer_block.py   # Transformer 块</span><br><span class="line">│   │   │   ├── transformer_config.py  # 配置类</span><br><span class="line">│   │   │   ├── attention.py           # 注意力机制</span><br><span class="line">│   │   │   └── mlp.py                 # MLP 层</span><br><span class="line">│   │   ├── tensor_parallel/           # 张量并行</span><br><span class="line">│   │   ├── pipeline_parallel/         # 流水线并行</span><br><span class="line">│   │   ├── distributed/               # 分布式训练（FSDP、DDP）</span><br><span class="line">│   │   ├── optimizer/                 # 优化器</span><br><span class="line">│   │   ├── datasets/                  # 数据集加载器</span><br><span class="line">│   │   ├── inference/                 # 推理引擎</span><br><span class="line">│   │   ├── export/                    # 模型导出</span><br><span class="line">│   │   ├── quantization/              # 量化</span><br><span class="line">│   │   ├── fusions/                   # 融合内核</span><br><span class="line">│   │   └── dist_checkpointing/        # 分布式检查点</span><br><span class="line">│   ├── training/                      # 训练循环和工具</span><br><span class="line">│   ├── legacy/                        # 遗留组件</span><br><span class="line">│   └── post_training/                 # 后训练（RLHF 等）</span><br><span class="line">├── examples/                          # 示例脚本</span><br><span class="line">│   ├── gpt3/                          # GPT-3 示例</span><br><span class="line">│   ├── llama/                         # LLaMA 示例</span><br><span class="line">│   ├── mixtral/                       # Mixtral 示例</span><br><span class="line">│   ├── multimodal/                    # 多模态示例</span><br><span class="line">│   └── post_training/                 # 后训练示例</span><br><span class="line">├── tools/                             # 工具脚本</span><br><span class="line">│   ├── preprocess_data.py            # 数据预处理</span><br><span class="line">│   ├── checkpoint/                    # 检查点转换工具</span><br><span class="line">│   └── run_text_generation_server.py # 推理服务器</span><br><span class="line">└── tests/                             # 测试套件</span><br><span class="line">    ├── unit_tests/                    # 单元测试</span><br><span class="line">    └── functional_tests/              # 功能测试</span><br></pre></td></tr></table></figure><h3 id="2-2-系统架构图"><a href="#2-2-系统架构图" class="headerlink" title="2.2 系统架构图"></a>2.2 系统架构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;User Interface 用户界面&quot;</span><br><span class="line">        PretrainScripts[预训练脚本&lt;br/&gt;pretrain_gpt.py&lt;br/&gt;pretrain_llama.py]</span><br><span class="line">        Examples[示例代码&lt;br/&gt;examples/]</span><br><span class="line">        Tools[工具集&lt;br/&gt;tools/]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Training Layer 训练层&quot;</span><br><span class="line">        TrainingLoop[训练循环&lt;br/&gt;training/training.py]</span><br><span class="line">        DataLoader[数据加载&lt;br/&gt;datasets/]</span><br><span class="line">        Checkpoint[检查点管理&lt;br/&gt;checkpointing.py]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Megatron Core 核心层&quot;</span><br><span class="line">        subgraph &quot;Models 模型层&quot;</span><br><span class="line">            GPT[GPTModel]</span><br><span class="line">            BERT[BERTModel]</span><br><span class="line">            T5[T5Model]</span><br><span class="line">            Multimodal[MultimodalModel]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Transformer Components 组件层&quot;</span><br><span class="line">            TransformerBlock[TransformerBlock]</span><br><span class="line">            TransformerLayer[TransformerLayer]</span><br><span class="line">            Attention[Attention]</span><br><span class="line">            MLP[MLP/MoE]</span><br><span class="line">            Embedding[Embedding]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Parallelism 并行层&quot;</span><br><span class="line">            TensorParallel[Tensor Parallel&lt;br/&gt;张量并行]</span><br><span class="line">            PipelineParallel[Pipeline Parallel&lt;br/&gt;流水线并行]</span><br><span class="line">            DataParallel[Data Parallel&lt;br/&gt;数据并行]</span><br><span class="line">            ExpertParallel[Expert Parallel&lt;br/&gt;专家并行]</span><br><span class="line">            ContextParallel[Context Parallel&lt;br/&gt;上下文并行]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Optimization 优化层&quot;</span><br><span class="line">            Optimizer[分布式优化器&lt;br/&gt;DistributedOptimizer]</span><br><span class="line">            MixedPrecision[混合精度&lt;br/&gt;FP8/FP16/BF16]</span><br><span class="line">            GradAccum[梯度累积]</span><br><span class="line">            ParamScheduler[参数调度器]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Inference &amp; Export 推理导出层&quot;</span><br><span class="line">            InferenceEngine[推理引擎&lt;br/&gt;Dynamic Inference]</span><br><span class="line">            ExportModule[模型导出&lt;br/&gt;TensorRT-LLM/ONNX]</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Infrastructure 基础设施层&quot;</span><br><span class="line">        ParallelState[并行状态管理&lt;br/&gt;parallel_state.py]</span><br><span class="line">        ProcessGroups[进程组&lt;br/&gt;process_groups_config.py]</span><br><span class="line">        Memory[内存管理&lt;br/&gt;GlobalMemoryBuffer]</span><br><span class="line">        Timers[性能计时器&lt;br/&gt;timers.py]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;External Dependencies 外部依赖&quot;</span><br><span class="line">        PyTorch[PyTorch]</span><br><span class="line">        TE[Transformer Engine]</span><br><span class="line">        NCCL[NCCL]</span><br><span class="line">        Apex[Apex&lt;br/&gt;可选]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    PretrainScripts --&gt; TrainingLoop</span><br><span class="line">    Examples --&gt; TrainingLoop</span><br><span class="line">    Tools --&gt; Checkpoint</span><br><span class="line">    </span><br><span class="line">    TrainingLoop --&gt; GPT</span><br><span class="line">    TrainingLoop --&gt; BERT</span><br><span class="line">    TrainingLoop --&gt; T5</span><br><span class="line">    TrainingLoop --&gt; Multimodal</span><br><span class="line">    TrainingLoop --&gt; DataLoader</span><br><span class="line">    TrainingLoop --&gt; Checkpoint</span><br><span class="line">    </span><br><span class="line">    GPT --&gt; TransformerBlock</span><br><span class="line">    BERT --&gt; TransformerBlock</span><br><span class="line">    T5 --&gt; TransformerBlock</span><br><span class="line">    Multimodal --&gt; TransformerBlock</span><br><span class="line">    </span><br><span class="line">    TransformerBlock --&gt; TransformerLayer</span><br><span class="line">    TransformerLayer --&gt; Attention</span><br><span class="line">    TransformerLayer --&gt; MLP</span><br><span class="line">    TransformerBlock --&gt; Embedding</span><br><span class="line">    </span><br><span class="line">    TransformerLayer --&gt; TensorParallel</span><br><span class="line">    TransformerBlock --&gt; PipelineParallel</span><br><span class="line">    TrainingLoop --&gt; DataParallel</span><br><span class="line">    MLP --&gt; ExpertParallel</span><br><span class="line">    Attention --&gt; ContextParallel</span><br><span class="line">    </span><br><span class="line">    TrainingLoop --&gt; Optimizer</span><br><span class="line">    Optimizer --&gt; MixedPrecision</span><br><span class="line">    Optimizer --&gt; GradAccum</span><br><span class="line">    TrainingLoop --&gt; ParamScheduler</span><br><span class="line">    </span><br><span class="line">    TensorParallel --&gt; ParallelState</span><br><span class="line">    PipelineParallel --&gt; ParallelState</span><br><span class="line">    DataParallel --&gt; ParallelState</span><br><span class="line">    ExpertParallel --&gt; ParallelState</span><br><span class="line">    ParallelState --&gt; ProcessGroups</span><br><span class="line">    </span><br><span class="line">    ParallelState --&gt; Memory</span><br><span class="line">    TrainingLoop --&gt; Timers</span><br><span class="line">    </span><br><span class="line">    TransformerLayer --&gt; TE</span><br><span class="line">    ParallelState --&gt; NCCL</span><br><span class="line">    Optimizer --&gt; PyTorch</span><br><span class="line">    MixedPrecision --&gt; Apex</span><br><span class="line">    </span><br><span class="line">    style GPT fill:#FF6B6B</span><br><span class="line">    style TransformerBlock fill:#4ECDC4</span><br><span class="line">    style TensorParallel fill:#95E1D3</span><br><span class="line">    style PipelineParallel fill:#95E1D3</span><br><span class="line">    style Optimizer fill:#FFD93D</span><br></pre></td></tr></table></figure><hr><h2 id="3-核心模块分析"><a href="#3-核心模块分析" class="headerlink" title="3. 核心模块分析"></a>3. 核心模块分析</h2><h3 id="3-1-Transformer-组件"><a href="#3-1-Transformer-组件" class="headerlink" title="3.1 Transformer 组件"></a>3.1 Transformer 组件</h3><h4 id="3-1-1-TransformerConfig"><a href="#3-1-1-TransformerConfig" class="headerlink" title="3.1.1 TransformerConfig"></a>3.1.1 TransformerConfig</h4><p>配置类，管理所有 Transformer 相关的配置参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerConfig</span>(<span class="title class_ inherited__">ModelParallelConfig</span>):</span><br><span class="line">    <span class="comment"># 模型架构</span></span><br><span class="line">    num_layers: <span class="built_in">int</span>                        <span class="comment"># Transformer 层数</span></span><br><span class="line">    hidden_size: <span class="built_in">int</span>                       <span class="comment"># 隐藏层大小</span></span><br><span class="line">    num_attention_heads: <span class="built_in">int</span>               <span class="comment"># 注意力头数</span></span><br><span class="line">    ffn_hidden_size: <span class="type">Optional</span>[<span class="built_in">int</span>]         <span class="comment"># FFN 隐藏层大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 并行配置</span></span><br><span class="line">    tensor_model_parallel_size: <span class="built_in">int</span> = <span class="number">1</span>    <span class="comment"># 张量并行大小</span></span><br><span class="line">    pipeline_model_parallel_size: <span class="built_in">int</span> = <span class="number">1</span>  <span class="comment"># 流水线并行大小</span></span><br><span class="line">    expert_model_parallel_size: <span class="built_in">int</span> = <span class="number">1</span>    <span class="comment"># 专家并行大小</span></span><br><span class="line">    context_parallel_size: <span class="built_in">int</span> = <span class="number">1</span>         <span class="comment"># 上下文并行大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 精度配置</span></span><br><span class="line">    fp16: <span class="built_in">bool</span> = <span class="literal">False</span>                     <span class="comment"># FP16 训练</span></span><br><span class="line">    bf16: <span class="built_in">bool</span> = <span class="literal">False</span>                     <span class="comment"># BF16 训练</span></span><br><span class="line">    fp8: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>              <span class="comment"># FP8 训练</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MoE 配置</span></span><br><span class="line">    num_moe_experts: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>  <span class="comment"># MoE 专家数</span></span><br><span class="line">    moe_router_topk: <span class="built_in">int</span> = <span class="number">2</span>               <span class="comment"># TopK 路由</span></span><br></pre></td></tr></table></figure><h4 id="3-1-2-TransformerLayer"><a href="#3-1-2-TransformerLayer" class="headerlink" title="3.1.2 TransformerLayer"></a>3.1.2 TransformerLayer</h4><p>基础 Transformer 层实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Input[输入 Hidden States]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Self-Attention Block&quot;</span><br><span class="line">        SelfAttn[Self-Attention]</span><br><span class="line">        Dropout1[Dropout]</span><br><span class="line">        Residual1[Residual Connection]</span><br><span class="line">        PostAttnLN[Post-Attention LayerNorm]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;MLP/MoE Block&quot;</span><br><span class="line">        MLPorMoE&#123;MLP or MoE?&#125;</span><br><span class="line">        MLP[MLP]</span><br><span class="line">        MoE[MoE Router + Experts]</span><br><span class="line">        Dropout2[Dropout]</span><br><span class="line">        Residual2[Residual Connection]</span><br><span class="line">        PostMLPLN[Post-MLP LayerNorm]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Output[输出 Hidden States]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; SelfAttn</span><br><span class="line">    SelfAttn --&gt; Dropout1</span><br><span class="line">    Dropout1 --&gt; Residual1</span><br><span class="line">    Input --&gt; Residual1</span><br><span class="line">    Residual1 --&gt; PostAttnLN</span><br><span class="line">    </span><br><span class="line">    PostAttnLN --&gt; MLPorMoE</span><br><span class="line">    MLPorMoE --&gt;|标准| MLP</span><br><span class="line">    MLPorMoE --&gt;|MoE| MoE</span><br><span class="line">    MLP --&gt; Dropout2</span><br><span class="line">    MoE --&gt; Dropout2</span><br><span class="line">    Dropout2 --&gt; Residual2</span><br><span class="line">    PostAttnLN --&gt; Residual2</span><br><span class="line">    Residual2 --&gt; PostMLPLN</span><br><span class="line">    </span><br><span class="line">    PostMLPLN --&gt; Output</span><br><span class="line">    </span><br><span class="line">    style SelfAttn fill:#FFB6C1</span><br><span class="line">    style MLP fill:#98D8C8</span><br><span class="line">    style MoE fill:#F7DC6F</span><br></pre></td></tr></table></figure><h4 id="3-1-3-Attention-机制"><a href="#3-1-3-Attention-机制" class="headerlink" title="3.1.3 Attention 机制"></a>3.1.3 Attention 机制</h4><p>支持多种注意力实现：</p><ul><li><strong>标准 Multi-Head Attention (MHA)</strong></li><li><strong>Grouped Query Attention (GQA)</strong></li><li><strong>Multi-Query Attention (MQA)</strong></li><li><strong>Flash Attention</strong>（通过 Transformer Engine）</li><li><strong>Context Parallel Attention</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关键参数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>:</span><br><span class="line">    num_attention_heads: <span class="built_in">int</span>           <span class="comment"># 注意力头数</span></span><br><span class="line">    num_query_groups: <span class="built_in">int</span>              <span class="comment"># 查询组数（GQA）</span></span><br><span class="line">    kv_channels: <span class="built_in">int</span>                   <span class="comment"># K/V 通道数</span></span><br><span class="line">    attention_dropout: <span class="built_in">float</span>           <span class="comment"># Dropout 概率</span></span><br><span class="line">    attn_mask_type: AttnMaskType       <span class="comment"># 掩码类型</span></span><br><span class="line">    qkv_format: <span class="built_in">str</span>                    <span class="comment"># QKV 格式（sbhd/bshd等）</span></span><br></pre></td></tr></table></figure><h3 id="3-2-模型实现"><a href="#3-2-模型实现" class="headerlink" title="3.2 模型实现"></a>3.2 模型实现</h3><h4 id="3-2-1-GPTModel-架构"><a href="#3-2-1-GPTModel-架构" class="headerlink" title="3.2.1 GPTModel 架构"></a>3.2.1 GPTModel 架构</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;GPTModel&quot;</span><br><span class="line">        subgraph &quot;Pre-Process 预处理阶段&quot;</span><br><span class="line">            TokenEmbed[Token Embedding&lt;br/&gt;词嵌入层]</span><br><span class="line">            PosEmbed[Position Embedding&lt;br/&gt;位置编码]</span><br><span class="line">            EmbedDropout[Embedding Dropout]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Encoder 编码器&quot;</span><br><span class="line">            TransBlock[TransformerBlock&lt;br/&gt;N层Transformer]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Post-Process 后处理阶段&quot;</span><br><span class="line">            FinalLN[Final LayerNorm]</span><br><span class="line">            OutputLayer[Output Layer&lt;br/&gt;输出投影层]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Optional 可选组件&quot;</span><br><span class="line">            MTP[Multi-Token Prediction&lt;br/&gt;多标记预测]</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Input[Input Token IDs] --&gt; TokenEmbed</span><br><span class="line">    Input --&gt; PosEmbed</span><br><span class="line">    TokenEmbed --&gt; EmbedDropout</span><br><span class="line">    PosEmbed --&gt; EmbedDropout</span><br><span class="line">    EmbedDropout --&gt; TransBlock</span><br><span class="line">    TransBlock --&gt; FinalLN</span><br><span class="line">    FinalLN --&gt; OutputLayer</span><br><span class="line">    OutputLayer --&gt; Logits[Logits]</span><br><span class="line">    </span><br><span class="line">    FinalLN -.可选.-&gt; MTP</span><br><span class="line">    MTP -.-&gt; MTPLogits[MTP Logits]</span><br><span class="line">    </span><br><span class="line">    style TransBlock fill:#4CAF50</span><br><span class="line">    style MTP fill:#FF9800</span><br></pre></td></tr></table></figure><h4 id="3-2-2-支持的模型类型"><a href="#3-2-2-支持的模型类型" class="headerlink" title="3.2.2 支持的模型类型"></a>3.2.2 支持的模型类型</h4><table><thead><tr><th>模型类型</th><th>实现位置</th><th>特性</th></tr></thead><tbody><tr><td>GPT</td><td><code>megatron/core/models/gpt/</code></td><td>自回归语言模型，因果注意力</td></tr><tr><td>BERT</td><td><code>megatron/core/models/bert/</code></td><td>双向编码器，MLM训练</td></tr><tr><td>T5</td><td><code>megatron/core/models/T5/</code></td><td>编码器-解码器架构</td></tr><tr><td>Mamba</td><td><code>megatron/core/models/mamba/</code></td><td>状态空间模型（SSM）</td></tr><tr><td>Multimodal</td><td><code>megatron/core/models/multimodal/</code></td><td>多模态模型（LLaVA、MiMo、NVLM）</td></tr><tr><td>RETRO</td><td><code>megatron/core/models/retro/</code></td><td>检索增强模型</td></tr><tr><td>Vision</td><td><code>megatron/core/models/vision/</code></td><td>视觉模型（CLIP、RADIO、ViT）</td></tr><tr><td>MiMo</td><td><code>megatron/core/models/mimo/</code></td><td>多图像多输出视频VLM</td></tr></tbody></table><hr><h2 id="4-并行策略详解"><a href="#4-并行策略详解" class="headerlink" title="4. 并行策略详解"></a>4. 并行策略详解</h2><h3 id="4-1-并行策略概览"><a href="#4-1-并行策略概览" class="headerlink" title="4.1 并行策略概览"></a>4.1 并行策略概览</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph &quot;并行维度&quot;</span><br><span class="line">        DP[Data Parallel&lt;br/&gt;数据并行&lt;br/&gt;复制模型]</span><br><span class="line">        TP[Tensor Parallel&lt;br/&gt;张量并行&lt;br/&gt;切分层内张量]</span><br><span class="line">        PP[Pipeline Parallel&lt;br/&gt;流水线并行&lt;br/&gt;切分层间]</span><br><span class="line">        EP[Expert Parallel&lt;br/&gt;专家并行&lt;br/&gt;切分MoE专家]</span><br><span class="line">        CP[Context Parallel&lt;br/&gt;上下文并行&lt;br/&gt;切分序列]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Model[完整模型] --&gt; DP</span><br><span class="line">    Model --&gt; TP</span><br><span class="line">    Model --&gt; PP</span><br><span class="line">    Model --&gt; EP</span><br><span class="line">    Model --&gt; CP</span><br><span class="line">    </span><br><span class="line">    DP --&gt;|组合| Hybrid[混合并行策略]</span><br><span class="line">    TP --&gt;|组合| Hybrid</span><br><span class="line">    PP --&gt;|组合| Hybrid</span><br><span class="line">    EP --&gt;|组合| Hybrid</span><br><span class="line">    CP --&gt;|组合| Hybrid</span><br><span class="line">    </span><br><span class="line">    style DP fill:#FFE66D</span><br><span class="line">    style TP fill:#4ECDC4</span><br><span class="line">    style PP fill:#FF6B6B</span><br><span class="line">    style EP fill:#95E1D3</span><br><span class="line">    style CP fill:#C7CEEA</span><br></pre></td></tr></table></figure><h3 id="4-2-Tensor-Parallel（张量并行）"><a href="#4-2-Tensor-Parallel（张量并行）" class="headerlink" title="4.2 Tensor Parallel（张量并行）"></a>4.2 Tensor Parallel（张量并行）</h3><p><strong>原理</strong>: 在层内切分张量（权重矩阵和激活），不同 GPU 计算不同部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Column Parallel 列并行&quot;</span><br><span class="line">        Input1[Input&lt;br/&gt;Shape: [s, b, h]]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;GPU 0&quot;</span><br><span class="line">            Weight1_0[W1[:, 0:h/2]]</span><br><span class="line">            Output1_0[Output[:, :, 0:h/2]]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;GPU 1&quot;</span><br><span class="line">            Weight1_1[W1[:, h/2:h]]</span><br><span class="line">            Output1_1[Output[:, :, h/2:h]]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        AllGather1[All-Gather&lt;br/&gt;合并输出]</span><br><span class="line">        ConcatOutput1[Concatenated Output]</span><br><span class="line">        </span><br><span class="line">        Input1 --&gt; Weight1_0</span><br><span class="line">        Input1 --&gt; Weight1_1</span><br><span class="line">        Weight1_0 --&gt; Output1_0</span><br><span class="line">        Weight1_1 --&gt; Output1_1</span><br><span class="line">        Output1_0 --&gt; AllGather1</span><br><span class="line">        Output1_1 --&gt; AllGather1</span><br><span class="line">        AllGather1 --&gt; ConcatOutput1</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Row Parallel 行并行&quot;</span><br><span class="line">        Input2[Input&lt;br/&gt;Shape: [s, b, h]]</span><br><span class="line">        Split[Split 输入]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;GPU 0 &quot;</span><br><span class="line">            Input2_0[Input[:, :, 0:h/2]]</span><br><span class="line">            Weight2_0[W2[0:h/2, :]]</span><br><span class="line">            Output2_0[Partial Output]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;GPU 1 &quot;</span><br><span class="line">            Input2_1[Input[:, :, h/2:h]]</span><br><span class="line">            Weight2_1[W2[h/2:h, :]]</span><br><span class="line">            Output2_1[Partial Output]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        AllReduce2[All-Reduce&lt;br/&gt;求和]</span><br><span class="line">        FinalOutput2[Final Output]</span><br><span class="line">        </span><br><span class="line">        Input2 --&gt; Split</span><br><span class="line">        Split --&gt; Input2_0</span><br><span class="line">        Split --&gt; Input2_1</span><br><span class="line">        Input2_0 --&gt; Weight2_0</span><br><span class="line">        Input2_1 --&gt; Weight2_1</span><br><span class="line">        Weight2_0 --&gt; Output2_0</span><br><span class="line">        Weight2_1 --&gt; Output2_1</span><br><span class="line">        Output2_0 --&gt; AllReduce2</span><br><span class="line">        Output2_1 --&gt; AllReduce2</span><br><span class="line">        AllReduce2 --&gt; FinalOutput2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    style Weight1_0 fill:#FFB6C1</span><br><span class="line">    style Weight1_1 fill:#FFB6C1</span><br><span class="line">    style Weight2_0 fill:#98D8C8</span><br><span class="line">    style Weight2_1 fill:#98D8C8</span><br></pre></td></tr></table></figure><p><strong>核心组件</strong>:</p><ul><li><code>ColumnParallelLinear</code>: 列并行线性层</li><li><code>RowParallelLinear</code>: 行并行线性层</li><li><code>VocabParallelEmbedding</code>: 词表并行嵌入层</li></ul><p><strong>通信模式</strong>:</p><ul><li>All-Gather: 收集所有 GPU 的输出</li><li>All-Reduce: 对所有 GPU 的输出求和</li></ul><h3 id="4-3-Pipeline-Parallel（流水线并行）"><a href="#4-3-Pipeline-Parallel（流水线并行）" class="headerlink" title="4.3 Pipeline Parallel（流水线并行）"></a>4.3 Pipeline Parallel（流水线并行）</h3><p><strong>原理</strong>: 将模型按层切分到不同 GPU，采用流水线调度策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">gantt</span><br><span class="line">    title 1F1B Pipeline Schedule</span><br><span class="line">    dateFormat X</span><br><span class="line">    axisFormat %s</span><br><span class="line">    </span><br><span class="line">    section GPU-0</span><br><span class="line">    F1 : 0, 1</span><br><span class="line">    F2 : 1, 2</span><br><span class="line">    F3 : 2, 3</span><br><span class="line">    F4 : 3, 4</span><br><span class="line">    B1 : 4, 5</span><br><span class="line">    B2 : 5, 6</span><br><span class="line">    B3 : 6, 7</span><br><span class="line">    B4 : 7, 8</span><br><span class="line">    </span><br><span class="line">    section GPU-1</span><br><span class="line">    Idle : 0, 1</span><br><span class="line">    F1  : 1, 2</span><br><span class="line">    F2  : 2, 3</span><br><span class="line">    F3  : 3, 4</span><br><span class="line">    B1  : 4, 5</span><br><span class="line">    B2  : 5, 6</span><br><span class="line">    B3  : 6, 7</span><br><span class="line">    B4  : 7, 8</span><br><span class="line">    </span><br><span class="line">    section GPU-2</span><br><span class="line">    Idle : 0, 2</span><br><span class="line">    F1   : 2, 3</span><br><span class="line">    F2   : 3, 4</span><br><span class="line">    F3   : 4, 5</span><br><span class="line">    B1   : 5, 6</span><br><span class="line">    B2   : 6, 7</span><br><span class="line">    B3   : 7, 8</span><br><span class="line">    </span><br><span class="line">    section GPU-3</span><br><span class="line">    Idle : 0, 3</span><br><span class="line">    F1   : 3, 4</span><br><span class="line">    F2   : 4, 5</span><br><span class="line">    F3   : 5, 6</span><br><span class="line">    F4   : 6, 7</span><br><span class="line">    B1   : 7, 8</span><br></pre></td></tr></table></figure><p><strong>说明</strong>:</p><ul><li><strong>F1-F4</strong>: 前向传播（Forward pass）批次1-4</li><li><strong>B1-B4</strong>: 反向传播（Backward pass）批次1-4</li><li><strong>Idle</strong>: 空闲等待</li><li><strong>GPU-0</strong>: 处理 Layer 0-7</li><li><strong>GPU-1</strong>: 处理 Layer 8-15</li><li><strong>GPU-2</strong>: 处理 Layer 16-23</li><li><strong>GPU-3</strong>: 处理 Layer 24-31</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**调度策略**:</span><br><span class="line">- **1F1B (1-Forward-1-Backward)**: 交替执行前向和反向传播</span><br><span class="line">- **Interleaved 1F1B**: 虚拟流水线并行，减少气泡</span><br><span class="line">- **Combined 1F1B**: 结合数据并行和流水线并行</span><br><span class="line"></span><br><span class="line">**核心文件**:</span><br><span class="line">- `pipeline_parallel/schedules.py`: 调度算法实现</span><br><span class="line">- `pipeline_parallel/p2p_communication.py`: 点对点通信</span><br><span class="line"></span><br><span class="line">### 4.4 Data Parallel（数据并行）</span><br><span class="line"></span><br><span class="line">**原理**: 复制模型到多个 GPU，每个 GPU 处理不同的数据批次</span><br><span class="line"></span><br><span class="line">**实现方式**:</span><br><span class="line">1. **DDP (DistributedDataParallel)**: PyTorch 原生 DDP</span><br><span class="line">2. **FSDP (Fully Sharded Data Parallel)**: 全分片数据并行</span><br><span class="line">3. **ZeRO**: 分片优化器状态</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># DDP 配置</span><br><span class="line">ddp_config = DistributedDataParallelConfig(</span><br><span class="line">    grad_reduce_in_fp32=True,                # FP32 梯度规约</span><br><span class="line">    overlap_grad_reduce=True,                 # 重叠梯度通信</span><br><span class="line">    use_distributed_optimizer=True,           # 分布式优化器</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="4-5-Expert-Parallel（专家并行）"><a href="#4-5-Expert-Parallel（专家并行）" class="headerlink" title="4.5 Expert Parallel（专家并行）"></a>4.5 Expert Parallel（专家并行）</h3><p><strong>原理</strong>: 在 MoE 模型中，将专家分布到不同 GPU</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Input[输入 Tokens]</span><br><span class="line">    Router[Router 路由器]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;GPU 0&quot;</span><br><span class="line">        Expert0[Expert 0]</span><br><span class="line">        Expert1[Expert 1]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;GPU 1&quot;</span><br><span class="line">        Expert2[Expert 2]</span><br><span class="line">        Expert3[Expert 3]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;GPU 2&quot;</span><br><span class="line">        Expert4[Expert 4]</span><br><span class="line">        Expert5[Expert 5]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    AllToAll1[All-to-All&lt;br/&gt;Token 分发]</span><br><span class="line">    AllToAll2[All-to-All&lt;br/&gt;结果收集]</span><br><span class="line">    Output[输出]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; Router</span><br><span class="line">    Router --&gt; AllToAll1</span><br><span class="line">    AllToAll1 --&gt; Expert0</span><br><span class="line">    AllToAll1 --&gt; Expert1</span><br><span class="line">    AllToAll1 --&gt; Expert2</span><br><span class="line">    AllToAll1 --&gt; Expert3</span><br><span class="line">    AllToAll1 --&gt; Expert4</span><br><span class="line">    AllToAll1 --&gt; Expert5</span><br><span class="line">    </span><br><span class="line">    Expert0 --&gt; AllToAll2</span><br><span class="line">    Expert1 --&gt; AllToAll2</span><br><span class="line">    Expert2 --&gt; AllToAll2</span><br><span class="line">    Expert3 --&gt; AllToAll2</span><br><span class="line">    Expert4 --&gt; AllToAll2</span><br><span class="line">    Expert5 --&gt; AllToAll2</span><br><span class="line">    </span><br><span class="line">    AllToAll2 --&gt; Output</span><br><span class="line">    </span><br><span class="line">    style Router fill:#F39C12</span><br><span class="line">    style Expert0 fill:#3498DB</span><br><span class="line">    style Expert1 fill:#3498DB</span><br><span class="line">    style Expert2 fill:#E74C3C</span><br><span class="line">    style Expert3 fill:#E74C3C</span><br><span class="line">    style Expert4 fill:#2ECC71</span><br><span class="line">    style Expert5 fill:#2ECC71</span><br></pre></td></tr></table></figure><p><strong>关键特性</strong>:</p><ul><li>TopK 路由选择</li><li>负载均衡损失</li><li>Token Dropping</li><li>专家容量因子</li></ul><h3 id="4-6-Context-Parallel（上下文并行）"><a href="#4-6-Context-Parallel（上下文并行）" class="headerlink" title="4.6 Context Parallel（上下文并行）"></a>4.6 Context Parallel（上下文并行）</h3><p><strong>原理</strong>: 在序列维度上切分长序列，适用于超长上下文</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 序列切分示例</span></span><br><span class="line"><span class="comment"># 原始序列长度: 128K tokens</span></span><br><span class="line"><span class="comment"># Context Parallel Size: 4</span></span><br><span class="line"><span class="comment"># 每个 GPU 处理: 32K tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU 0: tokens[0:32K]</span></span><br><span class="line"><span class="comment"># GPU 1: tokens[32K:64K]</span></span><br><span class="line"><span class="comment"># GPU 2: tokens[64K:96K]</span></span><br><span class="line"><span class="comment"># GPU 3: tokens[96K:128K]</span></span><br></pre></td></tr></table></figure><p><strong>通信需求</strong>:</p><ul><li>All-Gather: 在注意力计算时收集 K&#x2F;V</li><li>Ring Attention: 环形注意力机制</li></ul><h3 id="4-7-并行策略选择指南"><a href="#4-7-并行策略选择指南" class="headerlink" title="4.7 并行策略选择指南"></a>4.7 并行策略选择指南</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    Start[开始选择并行策略]</span><br><span class="line">    </span><br><span class="line">    Q1&#123;模型能否放入&lt;br/&gt;单个GPU?&#125;</span><br><span class="line">    Q2&#123;主要瓶颈是?&#125;</span><br><span class="line">    Q3&#123;是否使用MoE?&#125;</span><br><span class="line">    Q4&#123;序列长度是否&lt;br/&gt;超过32K?&#125;</span><br><span class="line">    Q5&#123;GPU数量?&#125;</span><br><span class="line">    </span><br><span class="line">    DP[数据并行&lt;br/&gt;Data Parallel]</span><br><span class="line">    TP_DP[张量并行 + 数据并行&lt;br/&gt;TP + DP]</span><br><span class="line">    PP_TP_DP[流水线 + 张量 + 数据&lt;br/&gt;PP + TP + DP]</span><br><span class="line">    EP_ADDED[添加专家并行&lt;br/&gt;+ EP]</span><br><span class="line">    CP_ADDED[添加上下文并行&lt;br/&gt;+ CP]</span><br><span class="line">    </span><br><span class="line">    Start --&gt; Q1</span><br><span class="line">    Q1 --&gt;|是| DP</span><br><span class="line">    Q1 --&gt;|否| Q2</span><br><span class="line">    </span><br><span class="line">    Q2 --&gt;|层内参数量| Q5</span><br><span class="line">    Q2 --&gt;|层数过多| PP_TP_DP</span><br><span class="line">    </span><br><span class="line">    Q5 --&gt;|2-8| TP_DP</span><br><span class="line">    Q5 --&gt;|&gt;8| PP_TP_DP</span><br><span class="line">    </span><br><span class="line">    TP_DP --&gt; Q3</span><br><span class="line">    PP_TP_DP --&gt; Q3</span><br><span class="line">    </span><br><span class="line">    Q3 --&gt;|是| EP_ADDED</span><br><span class="line">    Q3 --&gt;|否| Q4</span><br><span class="line">    EP_ADDED --&gt; Q4</span><br><span class="line">    </span><br><span class="line">    Q4 --&gt;|是| CP_ADDED</span><br><span class="line">    Q4 --&gt;|否| End[完成配置]</span><br><span class="line">    CP_ADDED --&gt; End</span><br><span class="line">    </span><br><span class="line">    style DP fill:#90EE90</span><br><span class="line">    style TP_DP fill:#87CEEB</span><br><span class="line">    style PP_TP_DP fill:#FFB6C1</span><br><span class="line">    style EP_ADDED fill:#F0E68C</span><br><span class="line">    style CP_ADDED fill:#DDA0DD</span><br></pre></td></tr></table></figure><p><strong>经验法则</strong>:</p><table><thead><tr><th>模型大小</th><th>GPU 数量</th><th>推荐策略</th></tr></thead><tbody><tr><td>&lt; 1B</td><td>1-8</td><td>DP only</td></tr><tr><td>1B - 13B</td><td>8-64</td><td>TP&#x3D;2-4, DP&#x3D;rest</td></tr><tr><td>13B - 70B</td><td>64-256</td><td>TP&#x3D;4-8, PP&#x3D;2-4, DP&#x3D;rest</td></tr><tr><td>70B - 175B</td><td>256-1024</td><td>TP&#x3D;8, PP&#x3D;4-8, DP&#x3D;rest</td></tr><tr><td>&gt; 175B</td><td>&gt; 1024</td><td>TP&#x3D;8, PP&#x3D;16+, DP&#x3D;rest</td></tr></tbody></table><hr><h2 id="5-模型实现"><a href="#5-模型实现" class="headerlink" title="5. 模型实现"></a>5. 模型实现</h2><h3 id="5-1-模型层次结构"><a href="#5-1-模型层次结构" class="headerlink" title="5.1 模型层次结构"></a>5.1 模型层次结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">    class MegatronModule &#123;</span><br><span class="line">        +config: TransformerConfig</span><br><span class="line">        +shared_embedding_or_output_weight()</span><br><span class="line">        +initialize_embedding_or_output_weight()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class LanguageModule &#123;</span><br><span class="line">        +state_dict_for_save_checkpoint()</span><br><span class="line">        +load_state_dict()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class GPTModel &#123;</span><br><span class="line">        +embedding: LanguageModelEmbedding</span><br><span class="line">        +decoder: TransformerBlock</span><br><span class="line">        +output_layer: Linear</span><br><span class="line">        +forward()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class TransformerBlock &#123;</span><br><span class="line">        +num_layers: int</span><br><span class="line">        +layers: ModuleList</span><br><span class="line">        +final_layernorm: LayerNorm</span><br><span class="line">        +forward()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class TransformerLayer &#123;</span><br><span class="line">        +self_attention: Attention</span><br><span class="line">        +mlp: MLP</span><br><span class="line">        +input_layernorm: LayerNorm</span><br><span class="line">        +pre_mlp_layernorm: LayerNorm</span><br><span class="line">        +forward()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class Attention &#123;</span><br><span class="line">        +linear_qkv: ColumnParallelLinear</span><br><span class="line">        +core_attention: DotProductAttention</span><br><span class="line">        +linear_proj: RowParallelLinear</span><br><span class="line">        +forward()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class MLP &#123;</span><br><span class="line">        +linear_fc1: ColumnParallelLinear</span><br><span class="line">        +activation_func: Activation</span><br><span class="line">        +linear_fc2: RowParallelLinear</span><br><span class="line">        +forward()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    MegatronModule &lt;|-- LanguageModule</span><br><span class="line">    LanguageModule &lt;|-- GPTModel</span><br><span class="line">    MegatronModule &lt;|-- TransformerBlock</span><br><span class="line">    MegatronModule &lt;|-- TransformerLayer</span><br><span class="line">    MegatronModule &lt;|-- Attention</span><br><span class="line">    MegatronModule &lt;|-- MLP</span><br><span class="line">    </span><br><span class="line">    GPTModel *-- TransformerBlock</span><br><span class="line">    TransformerBlock *-- TransformerLayer</span><br><span class="line">    TransformerLayer *-- Attention</span><br><span class="line">    TransformerLayer *-- MLP</span><br></pre></td></tr></table></figure><h3 id="5-2-MoE（Mixture-of-Experts）实现"><a href="#5-2-MoE（Mixture-of-Experts）实现" class="headerlink" title="5.2 MoE（Mixture of Experts）实现"></a>5.2 MoE（Mixture of Experts）实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Input[输入 Hidden States]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;MoE Layer&quot;</span><br><span class="line">        SharedCheck&#123;是否有 Shared Experts?&#125;</span><br><span class="line">        SharedExperts[Shared Experts 共享专家&lt;br/&gt;所有token都计算]</span><br><span class="line">        </span><br><span class="line">        Router[TopK Router 路由器&lt;br/&gt;计算专家分数 + Aux Loss]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Token Dispatching&quot;</span><br><span class="line">            Dispatcher&#123;Token Dispatcher&lt;br/&gt;分发策略&#125;</span><br><span class="line">            AllGather[AllGather&lt;br/&gt;收集所有token]</span><br><span class="line">            AllToAll[AllToAll&lt;br/&gt;token重排列]</span><br><span class="line">            Flex[Flex Dispatcher&lt;br/&gt;统一TP+EP通信]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;Routed Experts 路由专家&quot;</span><br><span class="line">            E1[Expert 1]</span><br><span class="line">            E2[Expert 2]</span><br><span class="line">            EN[Expert N]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        Combine[Token Combine 合并&lt;br/&gt;加权求和专家输出]</span><br><span class="line">        MixOutput[混合输出&lt;br/&gt;Shared + Routed]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Output[输出 Hidden States]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; SharedCheck</span><br><span class="line">    SharedCheck --&gt;|有| SharedExperts</span><br><span class="line">    SharedCheck --&gt;|无| Router</span><br><span class="line">    SharedExperts --&gt; Router</span><br><span class="line">    </span><br><span class="line">    Router --&gt;|routing_map + probs| Dispatcher</span><br><span class="line">    </span><br><span class="line">    Dispatcher --&gt;|All-Gather| AllGather</span><br><span class="line">    Dispatcher --&gt;|All-to-All| AllToAll</span><br><span class="line">    Dispatcher --&gt;|统一通信| Flex</span><br><span class="line">    </span><br><span class="line">    AllGather --&gt; E1</span><br><span class="line">    AllGather --&gt; E2</span><br><span class="line">    AllGather --&gt; EN</span><br><span class="line">    </span><br><span class="line">    AllToAll --&gt; E1</span><br><span class="line">    AllToAll --&gt; E2</span><br><span class="line">    AllToAll --&gt; EN</span><br><span class="line">    </span><br><span class="line">    Flex --&gt; E1</span><br><span class="line">    Flex --&gt; E2</span><br><span class="line">    Flex --&gt; EN</span><br><span class="line">    </span><br><span class="line">    E1 --&gt; Combine</span><br><span class="line">    E2 --&gt; Combine</span><br><span class="line">    EN --&gt; Combine</span><br><span class="line">    </span><br><span class="line">    Combine --&gt; MixOutput</span><br><span class="line">    SharedExperts -.-&gt; MixOutput</span><br><span class="line">    </span><br><span class="line">    MixOutput --&gt; Output</span><br><span class="line">    </span><br><span class="line">    style Router fill:#FFD93D</span><br><span class="line">    style SharedExperts fill:#FF6B6B</span><br><span class="line">    style E1 fill:#98D8C8</span><br><span class="line">    style E2 fill:#98D8C8</span><br><span class="line">    style EN fill:#98D8C8</span><br><span class="line">    style Combine fill:#F7DC6F</span><br></pre></td></tr></table></figure><p><strong>MoE 关键参数</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MoE 配置</span></span><br><span class="line">moe_config = TransformerConfig(</span><br><span class="line">    num_moe_experts=<span class="number">64</span>,                    <span class="comment"># 专家总数</span></span><br><span class="line">    moe_router_topk=<span class="number">2</span>,                     <span class="comment"># 每个token选择的专家数</span></span><br><span class="line">    moe_aux_loss_coeff=<span class="number">0.01</span>,              <span class="comment"># 辅助损失系数</span></span><br><span class="line">    moe_token_dispatcher_type=<span class="string">&#x27;alltoall&#x27;</span>,  <span class="comment"># Token分发类型</span></span><br><span class="line">    expert_model_parallel_size=<span class="number">8</span>,          <span class="comment"># 专家并行度</span></span><br><span class="line">    moe_router_load_balancing_type=<span class="string">&#x27;aux_loss&#x27;</span>,  <span class="comment"># 负载均衡类型</span></span><br><span class="line">    moe_router_dtype=<span class="string">&#x27;fp32&#x27;</span>,               <span class="comment"># 路由器精度（推荐fp32）</span></span><br><span class="line">    moe_grouped_gemm=<span class="literal">True</span>,                 <span class="comment"># 分组GEMM优化</span></span><br><span class="line">    <span class="comment"># Shared Experts 配置（如 DeepSeek-V3）</span></span><br><span class="line">    moe_shared_expert_intermediate_size=<span class="literal">None</span>,  <span class="comment"># 共享专家FFN大小</span></span><br><span class="line">    moe_shared_expert_overlap=<span class="literal">True</span>,        <span class="comment"># 共享专家计算重叠</span></span><br><span class="line">    <span class="comment"># 负载均衡策略</span></span><br><span class="line">    moe_aux_loss_free=<span class="literal">False</span>,               <span class="comment"># 无辅助损失策略</span></span><br><span class="line">    moe_expert_capacity_factor=<span class="number">1.0</span>,        <span class="comment"># 专家容量因子</span></span><br><span class="line">    moe_pad_expert_input_to_capacity=<span class="literal">False</span>,  <span class="comment"># 填充到容量</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>DeepSeek-V3 特性</strong>:</p><ul><li><strong>Node-limited routing</strong>: 节点限制路由</li><li><strong>Device-limited routing</strong>: 设备限制路由</li><li><strong>Aux-loss-free</strong>: 无辅助损失负载均衡</li><li><strong>Shared Experts</strong>: 所有token都经过的共享专家层</li><li><strong>Fine-grained parallelism</strong>: 细粒度并行优化</li></ul><h3 id="5-3-Multi-Token-Prediction-MTP"><a href="#5-3-Multi-Token-Prediction-MTP" class="headerlink" title="5.3 Multi-Token Prediction (MTP)"></a>5.3 Multi-Token Prediction (MTP)</h3><p><strong>概念</strong>: 在训练时同时预测多个未来 token，提升训练效率</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    Input[输入序列&lt;br/&gt;t1, t2, ..., tn]</span><br><span class="line">    Encoder[主编码器&lt;br/&gt;TransformerBlock]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;MTP Block&quot;</span><br><span class="line">        MTP_Layer1[MTP Layer 1]</span><br><span class="line">        MTP_Layer2[MTP Layer 2]</span><br><span class="line">        MTP_LayerK[MTP Layer K]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Pred_t1[预测 t+1]</span><br><span class="line">    Pred_t2[预测 t+2]</span><br><span class="line">    Pred_tk[预测 t+k]</span><br><span class="line">    </span><br><span class="line">    Loss[综合损失]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; Encoder</span><br><span class="line">    Encoder --&gt; MTP_Layer1</span><br><span class="line">    MTP_Layer1 --&gt; MTP_Layer2</span><br><span class="line">    MTP_Layer2 --&gt; MTP_LayerK</span><br><span class="line">    </span><br><span class="line">    Encoder --&gt; Pred_t1</span><br><span class="line">    MTP_Layer1 --&gt; Pred_t2</span><br><span class="line">    MTP_LayerK --&gt; Pred_tk</span><br><span class="line">    </span><br><span class="line">    Pred_t1 --&gt; Loss</span><br><span class="line">    Pred_t2 --&gt; Loss</span><br><span class="line">    Pred_tk --&gt; Loss</span><br><span class="line">    </span><br><span class="line">    style Encoder fill:#4CAF50</span><br><span class="line">    style MTP_Layer1 fill:#FF9800</span><br><span class="line">    style MTP_Layer2 fill:#FF9800</span><br><span class="line">    style MTP_LayerK fill:#FF9800</span><br></pre></td></tr></table></figure><hr><h2 id="6-训练流程"><a href="#6-训练流程" class="headerlink" title="6. 训练流程"></a>6. 训练流程</h2><h3 id="6-1-训练主循环"><a href="#6-1-训练主循环" class="headerlink" title="6.1 训练主循环"></a>6.1 训练主循环</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    Start([开始训练])</span><br><span class="line">    Init[初始化&lt;br/&gt;initialize_megatron]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;初始化阶段&quot;</span><br><span class="line">        ParseArgs[解析命令行参数]</span><br><span class="line">        InitDist[初始化分布式环境&lt;br/&gt;parallel_state]</span><br><span class="line">        BuildModel[构建模型&lt;br/&gt;model_provider]</span><br><span class="line">        BuildOptim[构建优化器&lt;br/&gt;get_megatron_optimizer]</span><br><span class="line">        BuildData[构建数据加载器&lt;br/&gt;build_train_valid_test_datasets]</span><br><span class="line">        LoadCkpt[加载检查点&lt;br/&gt;load_checkpoint]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;训练循环&quot;</span><br><span class="line">        EpochLoop&#123;遍历 Epoch&#125;</span><br><span class="line">        BatchLoop&#123;遍历 Batch&#125;</span><br><span class="line">        </span><br><span class="line">        GetBatch[获取数据批次]</span><br><span class="line">        ForwardBackward[前向+反向传播&lt;br/&gt;forward_backward_func]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;前向反向传播&quot;</span><br><span class="line">            Forward[前向传播]</span><br><span class="line">            ComputeLoss[计算损失]</span><br><span class="line">            Backward[反向传播]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        ReduceGrad[梯度规约&lt;br/&gt;All-Reduce]</span><br><span class="line">        ClipGrad[梯度裁剪]</span><br><span class="line">        UpdateParams[更新参数&lt;br/&gt;optimizer.step]</span><br><span class="line">        UpdateLR[更新学习率]</span><br><span class="line">        LogMetrics[记录指标]</span><br><span class="line">        </span><br><span class="line">        CheckSave&#123;是否保存?&#125;</span><br><span class="line">        SaveCkpt[保存检查点]</span><br><span class="line">        CheckEval&#123;是否评估?&#125;</span><br><span class="line">        Evaluate[评估模型]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    End([训练结束])</span><br><span class="line">    </span><br><span class="line">    Start --&gt; Init</span><br><span class="line">    Init --&gt; ParseArgs</span><br><span class="line">    ParseArgs --&gt; InitDist</span><br><span class="line">    InitDist --&gt; BuildModel</span><br><span class="line">    BuildModel --&gt; BuildOptim</span><br><span class="line">    BuildOptim --&gt; BuildData</span><br><span class="line">    BuildData --&gt; LoadCkpt</span><br><span class="line">    </span><br><span class="line">    LoadCkpt --&gt; EpochLoop</span><br><span class="line">    EpochLoop --&gt;|继续| BatchLoop</span><br><span class="line">    EpochLoop --&gt;|完成| End</span><br><span class="line">    </span><br><span class="line">    BatchLoop --&gt;|继续| GetBatch</span><br><span class="line">    BatchLoop --&gt;|完成| EpochLoop</span><br><span class="line">    </span><br><span class="line">    GetBatch --&gt; ForwardBackward</span><br><span class="line">    ForwardBackward --&gt; Forward</span><br><span class="line">    Forward --&gt; ComputeLoss</span><br><span class="line">    ComputeLoss --&gt; Backward</span><br><span class="line">    </span><br><span class="line">    Backward --&gt; ReduceGrad</span><br><span class="line">    ReduceGrad --&gt; ClipGrad</span><br><span class="line">    ClipGrad --&gt; UpdateParams</span><br><span class="line">    UpdateParams --&gt; UpdateLR</span><br><span class="line">    UpdateLR --&gt; LogMetrics</span><br><span class="line">    </span><br><span class="line">    LogMetrics --&gt; CheckSave</span><br><span class="line">    CheckSave --&gt;|是| SaveCkpt</span><br><span class="line">    CheckSave --&gt;|否| CheckEval</span><br><span class="line">    SaveCkpt --&gt; CheckEval</span><br><span class="line">    </span><br><span class="line">    CheckEval --&gt;|是| Evaluate</span><br><span class="line">    CheckEval --&gt;|否| BatchLoop</span><br><span class="line">    Evaluate --&gt; BatchLoop</span><br><span class="line">    </span><br><span class="line">    style Init fill:#90EE90</span><br><span class="line">    style ForwardBackward fill:#FFB6C1</span><br><span class="line">    style UpdateParams fill:#87CEEB</span><br></pre></td></tr></table></figure><h3 id="6-2-混合精度训练流程"><a href="#6-2-混合精度训练流程" class="headerlink" title="6.2 混合精度训练流程"></a>6.2 混合精度训练流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Input[FP32/BF16 输入]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;前向传播&quot;</span><br><span class="line">        Cast1[转换为 FP16/BF16/FP8]</span><br><span class="line">        Compute1[模型计算&lt;br/&gt;低精度]</span><br><span class="line">        Loss[计算损失&lt;br/&gt;FP32]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;反向传播&quot;</span><br><span class="line">        ScaleLoss[损失缩放&lt;br/&gt;Loss Scaling]</span><br><span class="line">        Backward[反向传播&lt;br/&gt;低精度梯度]</span><br><span class="line">        Unscale[反缩放梯度]</span><br><span class="line">        CheckOverflow&#123;检查溢出?&#125;</span><br><span class="line">        Cast2[转换为 FP32]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;优化器&quot;</span><br><span class="line">        ClipGrad[梯度裁剪&lt;br/&gt;FP32]</span><br><span class="line">        Update[参数更新&lt;br/&gt;FP32]</span><br><span class="line">        UpdateScale[更新Loss Scale]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Output[更新后的参数]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; Cast1</span><br><span class="line">    Cast1 --&gt; Compute1</span><br><span class="line">    Compute1 --&gt; Loss</span><br><span class="line">    </span><br><span class="line">    Loss --&gt; ScaleLoss</span><br><span class="line">    ScaleLoss --&gt; Backward</span><br><span class="line">    Backward --&gt; Unscale</span><br><span class="line">    Unscale --&gt; CheckOverflow</span><br><span class="line">    </span><br><span class="line">    CheckOverflow --&gt;|溢出| UpdateScale</span><br><span class="line">    CheckOverflow --&gt;|正常| Cast2</span><br><span class="line">    UpdateScale --&gt; SkipStep[跳过更新]</span><br><span class="line">    </span><br><span class="line">    Cast2 --&gt; ClipGrad</span><br><span class="line">    ClipGrad --&gt; Update</span><br><span class="line">    Update --&gt; Output</span><br><span class="line">    </span><br><span class="line">    style Compute1 fill:#FFE66D</span><br><span class="line">    style Backward fill:#FF6B6B</span><br><span class="line">    style Update fill:#4ECDC4</span><br></pre></td></tr></table></figure><hr><h2 id="7-关键技术特性"><a href="#7-关键技术特性" class="headerlink" title="7. 关键技术特性"></a>7. 关键技术特性</h2><h3 id="7-1-FP8-训练"><a href="#7-1-FP8-训练" class="headerlink" title="7.1 FP8 训练"></a>7.1 FP8 训练</h3><p><strong>Transformer Engine 集成</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FP8 配置</span></span><br><span class="line">config = TransformerConfig(</span><br><span class="line">    fp8=<span class="string">&#x27;hybrid&#x27;</span>,                    <span class="comment"># FP8 模式: hybrid/e4m3/e5m2</span></span><br><span class="line">    fp8_margin=<span class="number">0</span>,                    <span class="comment"># FP8 边界</span></span><br><span class="line">    fp8_interval=<span class="number">1</span>,                  <span class="comment"># 缩放因子更新间隔</span></span><br><span class="line">    fp8_amax_history_len=<span class="number">1024</span>,       <span class="comment"># 历史最大值长度</span></span><br><span class="line">    fp8_amax_compute_algo=<span class="string">&#x27;max&#x27;</span>,     <span class="comment"># 计算算法</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>精度对比</strong>:</p><table><thead><tr><th>精度类型</th><th>指数位</th><th>尾数位</th><th>动态范围</th><th>适用场景</th></tr></thead><tbody><tr><td>FP32</td><td>8</td><td>23</td><td>10^38</td><td>基准</td></tr><tr><td>FP16</td><td>5</td><td>10</td><td>10^4</td><td>通用训练</td></tr><tr><td>BF16</td><td>8</td><td>7</td><td>10^38</td><td>稳定训练</td></tr><tr><td>FP8 E4M3</td><td>4</td><td>3</td><td>10^2</td><td>前向传播</td></tr><tr><td>FP8 E5M2</td><td>5</td><td>2</td><td>10^4</td><td>反向传播</td></tr></tbody></table><h3 id="7-2-序列并行（Sequence-Parallel）"><a href="#7-2-序列并行（Sequence-Parallel）" class="headerlink" title="7.2 序列并行（Sequence Parallel）"></a>7.2 序列并行（Sequence Parallel）</h3><p><strong>与张量并行结合</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph &quot;标准 Tensor Parallel&quot;</span><br><span class="line">        Input1[Input&lt;br/&gt;复制到所有GPU]</span><br><span class="line">        TP1[TP Computation]</span><br><span class="line">        Output1[Output&lt;br/&gt;All-Reduce]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Sequence Parallel&quot;</span><br><span class="line">        Input2[Input&lt;br/&gt;切分序列维度]</span><br><span class="line">        TP2[TP Computation&lt;br/&gt;每个GPU处理部分序列]</span><br><span class="line">        Output2[Output&lt;br/&gt;All-Gather]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Input1 --&gt; TP1</span><br><span class="line">    TP1 --&gt; Output1</span><br><span class="line">    </span><br><span class="line">    Input2 --&gt; TP2</span><br><span class="line">    TP2 --&gt; Output2</span><br><span class="line">    </span><br><span class="line">    style Input2 fill:#90EE90</span><br><span class="line">    style TP2 fill:#FFB6C1</span><br></pre></td></tr></table></figure><p><strong>优势</strong>:</p><ul><li>减少激活内存占用</li><li>降低通信量</li><li>更好的扩展性</li></ul><h3 id="7-3-重计算（Activation-Recomputation）"><a href="#7-3-重计算（Activation-Recomputation）" class="headerlink" title="7.3 重计算（Activation Recomputation）"></a>7.3 重计算（Activation Recomputation）</h3><p><strong>策略</strong>:</p><ol><li><strong>Full Recomputation</strong>: 重计算所有激活</li><li><strong>Selective Recomputation</strong>: 仅重计算部分层</li><li><strong>Partial Recomputation</strong>: 重计算注意力层</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置重计算</span></span><br><span class="line">config = TransformerConfig(</span><br><span class="line">    recompute_granularity=<span class="string">&#x27;selective&#x27;</span>,  <span class="comment"># full/selective/partial</span></span><br><span class="line">    recompute_method=<span class="string">&#x27;uniform&#x27;</span>,         <span class="comment"># uniform/block</span></span><br><span class="line">    recompute_num_layers=<span class="number">1</span>,            <span class="comment"># 重计算层数</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="7-4-分布式优化器"><a href="#7-4-分布式优化器" class="headerlink" title="7.4 分布式优化器"></a>7.4 分布式优化器</h3><p><strong>特性</strong>:</p><ul><li>分片优化器状态（类似 ZeRO-1）</li><li>重叠通信和计算</li><li>支持梯度累积</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分布式优化器配置</span></span><br><span class="line">optimizer_config = OptimizerConfig(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    lr=<span class="number">1e-4</span>,</span><br><span class="line">    weight_decay=<span class="number">0.1</span>,</span><br><span class="line">    adam_beta1=<span class="number">0.9</span>,</span><br><span class="line">    adam_beta2=<span class="number">0.999</span>,</span><br><span class="line">    use_distributed_optimizer=<span class="literal">True</span>,</span><br><span class="line">    overlap_grad_reduce=<span class="literal">True</span>,</span><br><span class="line">    overlap_param_gather=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="7-5-Flash-Attention"><a href="#7-5-Flash-Attention" class="headerlink" title="7.5 Flash Attention"></a>7.5 Flash Attention</h3><p>通过 Transformer Engine 集成：</p><p><strong>优势</strong>:</p><ul><li>降低 HBM 访问</li><li>O(N) 内存复杂度</li><li>2-4倍加速</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用 Flash Attention</span></span><br><span class="line">config = TransformerConfig(</span><br><span class="line">    attention_backend=<span class="string">&#x27;flash&#x27;</span>,  <span class="comment"># transformer_engine/torch</span></span><br><span class="line">    attention_dropout=<span class="number">0.0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="7-6-Multi-Latent-Attention-MLA"><a href="#7-6-Multi-Latent-Attention-MLA" class="headerlink" title="7.6 Multi-Latent Attention (MLA)"></a>7.6 Multi-Latent Attention (MLA)</h3><p><strong>概念</strong>: DeepSeek-V3 引入的高效注意力机制，通过潜在压缩降低KV Cache内存</p><p><strong>架构特点</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    Input[输入 Hidden States]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;MLA Layer&quot;</span><br><span class="line">        QProj[Q Projection]</span><br><span class="line">        QDown[Q Down Projection&lt;br/&gt;降维到潜在空间]</span><br><span class="line">        QUp[Q Up Projection&lt;br/&gt;升维]</span><br><span class="line">        </span><br><span class="line">        KVDown[KV Down Projection&lt;br/&gt;共享压缩]</span><br><span class="line">        KVUp[KV Up Projection&lt;br/&gt;分离K和V]</span><br><span class="line">        </span><br><span class="line">        RoPE[Rotary Position&lt;br/&gt;Embedding]</span><br><span class="line">        Attn[Attention&lt;br/&gt;Computation]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Output[输出]</span><br><span class="line">    </span><br><span class="line">    Input --&gt; QProj</span><br><span class="line">    QProj --&gt; QDown</span><br><span class="line">    QDown --&gt; QUp</span><br><span class="line">    QUp --&gt; RoPE</span><br><span class="line">    </span><br><span class="line">    Input --&gt; KVDown</span><br><span class="line">    KVDown --&gt; KVUp</span><br><span class="line">    KVUp --&gt; RoPE</span><br><span class="line">    </span><br><span class="line">    RoPE --&gt; Attn</span><br><span class="line">    Attn --&gt; Output</span><br><span class="line">    </span><br><span class="line">    style KVDown fill:#FFB6C1</span><br><span class="line">    style QDown fill:#98D8C8</span><br></pre></td></tr></table></figure><p><strong>关键优势</strong>:</p><ul><li><strong>内存效率</strong>: KV Cache 内存降低 75%+</li><li><strong>计算效率</strong>: 减少注意力计算复杂度</li><li><strong>长序列支持</strong>: 支持更长的上下文窗口</li></ul><p><strong>配置示例</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MLA 配置</span></span><br><span class="line">mla_config = MLATransformerConfig(</span><br><span class="line">    <span class="comment"># 基础配置</span></span><br><span class="line">    hidden_size=<span class="number">5120</span>,</span><br><span class="line">    num_attention_heads=<span class="number">128</span>,</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MLA 特定配置</span></span><br><span class="line">    q_lora_rank=<span class="number">1536</span>,              <span class="comment"># Q的LoRA秩</span></span><br><span class="line">    kv_lora_rank=<span class="number">512</span>,              <span class="comment"># KV的LoRA秩</span></span><br><span class="line">    qk_rope_head_dim=<span class="number">64</span>,           <span class="comment"># RoPE维度</span></span><br><span class="line">    v_head_dim=<span class="number">128</span>,                <span class="comment"># V头维度</span></span><br><span class="line">    qk_nope_head_dim=<span class="number">128</span>,          <span class="comment"># 非RoPE部分维度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化选项</span></span><br><span class="line">    use_fused_rope=<span class="literal">True</span>,           <span class="comment"># 融合RoPE算子</span></span><br><span class="line">    cache_kv_in_compressed_form=<span class="literal">True</span>,  <span class="comment"># 缓存压缩形式的KV</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="7-7-HyperCommGrid"><a href="#7-7-HyperCommGrid" class="headerlink" title="7.7 HyperCommGrid"></a>7.7 HyperCommGrid</h3><p><strong>概念</strong>: N维通信网格，灵活管理多种并行策略的进程组</p><p><strong>特点</strong>:</p><ul><li>支持任意维度的并行组合</li><li>动态创建进程组</li><li>避免重复创建相同维度组合</li></ul><p><strong>使用示例</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> megatron.core.hyper_comm_grid <span class="keyword">import</span> HyperCommGrid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建4维并行网格: DP x TP x PP x EP</span></span><br><span class="line">grid = HyperCommGrid(</span><br><span class="line">    dim_names=[<span class="string">&#x27;dp&#x27;</span>, <span class="string">&#x27;tp&#x27;</span>, <span class="string">&#x27;pp&#x27;</span>, <span class="string">&#x27;ep&#x27;</span>],</span><br><span class="line">    dim_sizes=[<span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">    world_size=<span class="number">256</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建特定维度的进程组</span></span><br><span class="line">grid.create_pg([<span class="string">&#x27;tp&#x27;</span>, <span class="string">&#x27;pp&#x27;</span>])  <span class="comment"># TP+PP 组合</span></span><br><span class="line">grid.create_pg([<span class="string">&#x27;dp&#x27;</span>, <span class="string">&#x27;ep&#x27;</span>])  <span class="comment"># DP+EP 组合</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取进程组</span></span><br><span class="line">tp_pp_group = grid.get_pg([<span class="string">&#x27;tp&#x27;</span>, <span class="string">&#x27;pp&#x27;</span>])</span><br></pre></td></tr></table></figure><h3 id="7-8-动态推理引擎"><a href="#7-8-动态推理引擎" class="headerlink" title="7.8 动态推理引擎"></a>7.8 动态推理引擎</h3><p><strong>特性</strong>:</p><ul><li><strong>In-flight Batching</strong>: 动态批处理，提升吞吐量</li><li><strong>Chunked KV Cache</strong>: 分块KV缓存管理</li><li><strong>Multi-batch CUDA Graphs</strong>: 多批次CUDA图优化</li><li><strong>Async Support</strong>: 异步推理支持</li></ul><p><strong>推理引擎架构</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Client[推理客户端]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;推理引擎&quot;</span><br><span class="line">        Scheduler[调度器&lt;br/&gt;Scheduler]</span><br><span class="line">        Coordinator[协调器&lt;br/&gt;DP Coordinator]</span><br><span class="line">        </span><br><span class="line">        subgraph &quot;执行层&quot;</span><br><span class="line">            Engine1[推理引擎 GPU-0]</span><br><span class="line">            Engine2[推理引擎 GPU-1]</span><br><span class="line">            EngineN[推理引擎 GPU-N]</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        KVCache[KV Cache 管理器]</span><br><span class="line">        BatchManager[批处理管理器]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Client --&gt; Scheduler</span><br><span class="line">    Scheduler --&gt; Coordinator</span><br><span class="line">    Coordinator --&gt; Engine1</span><br><span class="line">    Coordinator --&gt; Engine2</span><br><span class="line">    Coordinator --&gt; EngineN</span><br><span class="line">    </span><br><span class="line">    Engine1 --&gt; KVCache</span><br><span class="line">    Engine2 --&gt; KVCache</span><br><span class="line">    EngineN --&gt; KVCache</span><br><span class="line">    </span><br><span class="line">    Scheduler --&gt; BatchManager</span><br><span class="line">    </span><br><span class="line">    style Scheduler fill:#4CAF50</span><br><span class="line">    style KVCache fill:#FF9800</span><br></pre></td></tr></table></figure><p><strong>使用示例</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> megatron.core.inference <span class="keyword">import</span> DynamicInferenceEngine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建推理引擎</span></span><br><span class="line">engine = DynamicInferenceEngine(</span><br><span class="line">    model=model,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_batch_size=<span class="number">64</span>,</span><br><span class="line">    max_sequence_length=<span class="number">8192</span>,</span><br><span class="line">    enable_cuda_graph=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步推理</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">    response = <span class="keyword">await</span> engine.generate_async(</span><br><span class="line">        prompts=[<span class="string">&quot;Hello, how are you?&quot;</span>],</span><br><span class="line">        max_new_tokens=<span class="number">100</span>,</span><br><span class="line">        temperature=<span class="number">0.7</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure><h3 id="7-9-容错训练（NVRx）"><a href="#7-9-容错训练（NVRx）" class="headerlink" title="7.9 容错训练（NVRx）"></a>7.9 容错训练（NVRx）</h3><p><strong>NVIDIA Resiliency Extension 集成</strong>:</p><p><strong>功能</strong>:</p><ul><li><strong>Straggler Detection</strong>: 掉队检测</li><li><strong>Fault Detection</strong>: 故障检测</li><li><strong>Hang Detection</strong>: 挂起检测</li><li><strong>Automatic Recovery</strong>: 自动恢复</li></ul><p><strong>配置</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用容错训练</span></span><br><span class="line">--enable-ft-pipeline</span><br><span class="line">--ft-timeout <span class="number">300</span></span><br><span class="line">--straggler-detector-enabled</span><br><span class="line">--straggler-detector-window-size <span class="number">10</span></span><br></pre></td></tr></table></figure><h3 id="7-10-CUDA-Graphs"><a href="#7-10-CUDA-Graphs" class="headerlink" title="7.10 CUDA Graphs"></a>7.10 CUDA Graphs</h3><p><strong>优化内核启动开销</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CUDA Graphs 配置</span></span><br><span class="line"><span class="keyword">if</span> args.use_cuda_graph:</span><br><span class="line">    cuda_graph = FullCudaGraphWrapper(</span><br><span class="line">        model=model,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        data_loader=train_data_iterator,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><strong>MoE CUDA Graphs 优化</strong>:</p><ul><li>支持动态专家选择</li><li>优化Token分发路径</li><li>减少内核启动开销</li></ul><hr><h2 id="8-代码组织结构"><a href="#8-代码组织结构" class="headerlink" title="8. 代码组织结构"></a>8. 代码组织结构</h2><h3 id="8-1-关键文件清单"><a href="#8-1-关键文件清单" class="headerlink" title="8.1 关键文件清单"></a>8.1 关键文件清单</h3><h4 id="训练入口"><a href="#训练入口" class="headerlink" title="训练入口"></a>训练入口</h4><table><thead><tr><th>文件</th><th>功能</th></tr></thead><tbody><tr><td><code>pretrain_gpt.py</code></td><td>GPT 模型预训练入口</td></tr><tr><td><code>pretrain_bert.py</code></td><td>BERT 模型预训练入口</td></tr><tr><td><code>pretrain_t5.py</code></td><td>T5 模型预训练入口</td></tr><tr><td><code>pretrain_vlm.py</code></td><td>多模态模型预训练入口</td></tr></tbody></table><h4 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h4><table><thead><tr><th>目录&#x2F;文件</th><th>功能</th></tr></thead><tbody><tr><td><code>megatron/core/transformer/</code></td><td>Transformer 核心组件</td></tr><tr><td><code>megatron/core/models/</code></td><td>模型实现</td></tr><tr><td><code>megatron/core/parallel_state.py</code></td><td>并行状态管理</td></tr><tr><td><code>megatron/core/hyper_comm_grid.py</code></td><td>N维通信网格管理</td></tr><tr><td><code>megatron/core/tensor_parallel/</code></td><td>张量并行实现</td></tr><tr><td><code>megatron/core/pipeline_parallel/</code></td><td>流水线并行实现</td></tr><tr><td><code>megatron/core/optimizer/</code></td><td>优化器实现</td></tr><tr><td><code>megatron/core/datasets/</code></td><td>数据集加载器</td></tr><tr><td><code>megatron/core/inference/</code></td><td>推理引擎</td></tr><tr><td><code>megatron/core/transformer/moe/</code></td><td>MoE 实现</td></tr><tr><td><code>megatron/core/transformer/multi_latent_attention.py</code></td><td>MLA 实现</td></tr><tr><td><code>megatron/training/training.py</code></td><td>训练主循环</td></tr><tr><td><code>megatron/training/checkpointing.py</code></td><td>检查点管理</td></tr></tbody></table><h4 id="工具脚本"><a href="#工具脚本" class="headerlink" title="工具脚本"></a>工具脚本</h4><table><thead><tr><th>文件</th><th>功能</th></tr></thead><tbody><tr><td><code>tools/preprocess_data.py</code></td><td>数据预处理</td></tr><tr><td><code>tools/checkpoint/</code></td><td>检查点转换工具</td></tr><tr><td><code>tools/run_text_generation_server.py</code></td><td>推理服务器</td></tr></tbody></table><h3 id="8-2-配置管理"><a href="#8-2-配置管理" class="headerlink" title="8.2 配置管理"></a>8.2 配置管理</h3><p><strong>层次化配置</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    GlobalConfig[全局配置&lt;br/&gt;megatron/training/arguments.py]</span><br><span class="line">    </span><br><span class="line">    ModelConfig[模型配置&lt;br/&gt;TransformerConfig]</span><br><span class="line">    ParallelConfig[并行配置&lt;br/&gt;ModelParallelConfig]</span><br><span class="line">    OptimizerConfig[优化器配置&lt;br/&gt;OptimizerConfig]</span><br><span class="line">    DataConfig[数据配置&lt;br/&gt;DataConfig]</span><br><span class="line">    </span><br><span class="line">    GlobalConfig --&gt; ModelConfig</span><br><span class="line">    GlobalConfig --&gt; ParallelConfig</span><br><span class="line">    GlobalConfig --&gt; OptimizerConfig</span><br><span class="line">    GlobalConfig --&gt; DataConfig</span><br><span class="line">    </span><br><span class="line">    LayerSpec[层规范&lt;br/&gt;ModuleSpec]</span><br><span class="line">    ModelConfig --&gt; LayerSpec</span><br><span class="line">    </span><br><span class="line">    ProcessGroups[进程组&lt;br/&gt;ProcessGroupCollection]</span><br><span class="line">    ParallelConfig --&gt; ProcessGroups</span><br></pre></td></tr></table></figure><h3 id="8-3-测试体系"><a href="#8-3-测试体系" class="headerlink" title="8.3 测试体系"></a>8.3 测试体系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tests/</span><br><span class="line">├── unit_tests/                      # 单元测试</span><br><span class="line">│   ├── data/                        # 数据加载测试</span><br><span class="line">│   ├── dist_checkpointing/          # 检查点测试</span><br><span class="line">│   ├── distributed/                 # 分布式测试</span><br><span class="line">│   ├── inference/                   # 推理测试</span><br><span class="line">│   ├── models/                      # 模型测试</span><br><span class="line">│   ├── pipeline_parallel/           # 流水线并行测试</span><br><span class="line">│   ├── tensor_parallel/             # 张量并行测试</span><br><span class="line">│   └── transformer/                 # Transformer测试</span><br><span class="line">└── functional_tests/                # 功能测试</span><br><span class="line">    ├── test_scripts/                # 测试脚本</span><br><span class="line">    └── test_results/                # 测试结果</span><br></pre></td></tr></table></figure><hr><h2 id="9-性能优化技巧"><a href="#9-性能优化技巧" class="headerlink" title="9. 性能优化技巧"></a>9. 性能优化技巧</h2><h3 id="9-1-内存优化"><a href="#9-1-内存优化" class="headerlink" title="9.1 内存优化"></a>9.1 内存优化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph &quot;内存优化策略&quot;</span><br><span class="line">        A[激活重计算&lt;br/&gt;Activation Recomputation]</span><br><span class="line">        B[梯度累积&lt;br/&gt;Gradient Accumulation]</span><br><span class="line">        C[CPU Offloading&lt;br/&gt;参数/优化器状态]</span><br><span class="line">        D[序列并行&lt;br/&gt;Sequence Parallel]</span><br><span class="line">        E[混合精度&lt;br/&gt;Mixed Precision]</span><br><span class="line">        F[Flash Attention&lt;br/&gt;高效注意力]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Memory[内存使用]</span><br><span class="line">    </span><br><span class="line">    A --&gt; Memory</span><br><span class="line">    B --&gt; Memory</span><br><span class="line">    C --&gt; Memory</span><br><span class="line">    D --&gt; Memory</span><br><span class="line">    E --&gt; Memory</span><br><span class="line">    F --&gt; Memory</span><br><span class="line">    </span><br><span class="line">    style Memory fill:#FF6B6B</span><br></pre></td></tr></table></figure><h3 id="9-2-通信优化"><a href="#9-2-通信优化" class="headerlink" title="9.2 通信优化"></a>9.2 通信优化</h3><ol><li><p><strong>重叠通信和计算</strong></p><ul><li>梯度规约与反向传播重叠</li><li>参数收集与前向传播重叠</li></ul></li><li><p><strong>通信融合</strong></p><ul><li>多个小通信操作融合为一个大操作</li><li>减少通信次数</li></ul></li><li><p><strong>通信压缩</strong></p><ul><li>FP16&#x2F;BF16 梯度通信</li><li>梯度压缩算法</li></ul></li></ol><h3 id="9-3-计算优化"><a href="#9-3-计算优化" class="headerlink" title="9.3 计算优化"></a>9.3 计算优化</h3><ol><li><p><strong>内核融合</strong></p><ul><li>LayerNorm + Dropout</li><li>Bias + GELU</li><li>Softmax + Mask</li></ul></li><li><p><strong>高效算子</strong></p><ul><li>Flash Attention</li><li>Fused Adam</li><li>Fused LayerNorm</li></ul></li><li><p><strong>CUDA Graphs</strong></p><ul><li>减少内核启动开销</li></ul></li></ol><hr><h2 id="10-最佳实践"><a href="#10-最佳实践" class="headerlink" title="10. 最佳实践"></a>10. 最佳实践</h2><h3 id="10-1-启动配置示例"><a href="#10-1-启动配置示例" class="headerlink" title="10.1 启动配置示例"></a>10.1 启动配置示例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 关键参数配置示例</span></span><br><span class="line"></span><br><span class="line">torchrun \</span><br><span class="line">    --nproc_per_node 8 \</span><br><span class="line">    --nnodes 8 \</span><br><span class="line">    pretrain_gpt.py \</span><br><span class="line">    --tensor-model-parallel-size 4 \</span><br><span class="line">    --pipeline-model-parallel-size 2 \</span><br><span class="line">    --num-layers 40 \</span><br><span class="line">    --hidden-size 5120 \</span><br><span class="line">    --num-attention-heads 40 \</span><br><span class="line">    --seq-length 4096 \</span><br><span class="line">    --micro-batch-size 1 \</span><br><span class="line">    --global-batch-size 256 \</span><br><span class="line">    --lr 1.5e-4 \</span><br><span class="line">    --min-lr 1.5e-5 \</span><br><span class="line">    --lr-decay-style cosine \</span><br><span class="line">    --weight-decay 0.1 \</span><br><span class="line">    --clip-grad 1.0 \</span><br><span class="line">    --fp16 \</span><br><span class="line">    --use-distributed-optimizer \</span><br><span class="line">    --overlap-grad-reduce \</span><br><span class="line">    --overlap-param-gather</span><br></pre></td></tr></table></figure><p><strong>关键参数说明</strong>:</p><ul><li><code>tensor-model-parallel-size</code>: 张量并行度</li><li><code>pipeline-model-parallel-size</code>: 流水线并行度</li><li><code>global-batch-size</code>: 全局批次大小 &#x3D; micro-batch-size × DP × 梯度累积步数</li><li><code>overlap-grad-reduce</code>: 重叠梯度通信与计算</li><li><code>use-distributed-optimizer</code>: 启用分布式优化器</li></ul><h3 id="10-2-调试技巧"><a href="#10-2-调试技巧" class="headerlink" title="10.2 调试技巧"></a>10.2 调试技巧</h3><ol><li><p><strong>启用详细日志</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> NCCL_DEBUG=INFO</span><br><span class="line"><span class="built_in">export</span> TORCH_DISTRIBUTED_DEBUG=DETAIL</span><br></pre></td></tr></table></figure></li><li><p><strong>检查张量形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> megatron.core <span class="keyword">import</span> mpu</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;TP rank: <span class="subst">&#123;mpu.get_tensor_model_parallel_rank()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor shape: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>验证梯度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查梯度是否为 NaN</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> param.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> torch.isnan(param.grad).<span class="built_in">any</span>():</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;NaN gradient in <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><hr><h2 id="11-总结"><a href="#11-总结" class="headerlink" title="11. 总结"></a>11. 总结</h2><h3 id="11-1-核心优势"><a href="#11-1-核心优势" class="headerlink" title="11.1 核心优势"></a>11.1 核心优势</h3><ol><li><strong>高性能</strong>: GPU 优化的内核和通信策略</li><li><strong>可扩展</strong>: 支持千卡级别的大规模训练</li><li><strong>灵活性</strong>: 模块化设计，易于定制</li><li><strong>生态丰富</strong>: 与多个框架和工具集成</li></ol><h3 id="11-2-适用场景"><a href="#11-2-适用场景" class="headerlink" title="11.2 适用场景"></a>11.2 适用场景</h3><ul><li>大规模预训练（1B - 1000B+ 参数）</li><li>多模态模型训练（文本、图像、视频）</li><li>细粒度 MoE 模型训练（DeepSeek-V3、Qwen3、Mixtral）</li><li>超长上下文模型（32K - 256K+ tokens）</li><li>高性能分布式推理</li><li>Blackwell 平台优化训练</li><li>跨数据中心训练（N&#x2F;S连接）</li></ul><h3 id="11-3-学习路径"><a href="#11-3-学习路径" class="headerlink" title="11.3 学习路径"></a>11.3 学习路径</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A[理解基础概念] --&gt; B[学习并行策略]</span><br><span class="line">    B --&gt; C[运行示例代码]</span><br><span class="line">    C --&gt; D[自定义模型]</span><br><span class="line">    D --&gt; E[性能优化]</span><br><span class="line">    E --&gt; F[生产部署]</span><br><span class="line">    </span><br><span class="line">    style A fill:#90EE90</span><br><span class="line">    style F fill:#FF6B6B</span><br></pre></td></tr></table></figure><h3 id="11-4-参考资源"><a href="#11-4-参考资源" class="headerlink" title="11.4 参考资源"></a>11.4 参考资源</h3><ul><li><strong>官方文档</strong>: <a href="https://docs.nvidia.com/Megatron-Core/">https://docs.nvidia.com/Megatron-Core/</a></li><li><strong>GitHub 仓库</strong>: <a href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a></li><li><strong>论文</strong>:<ul><li><a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li><li><a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li><li><a href="https://arxiv.org/abs/2205.05198">Reducing Activation Recomputation in Large Transformer Models</a></li></ul></li></ul><hr><p><strong>报告结束</strong></p><p><em>此报告基于 Megatron-LM v0.14.0 代码库分析生成</em></p>]]></content>
      
      
      <categories>
          
          <category> 系统架构分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Megatron </tag>
            
            <tag> NVIDIA </tag>
            
            <tag> 大语言模型 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HCCL 集合通信库设计分析</title>
      <link href="/2025/11/17/HCCL_DeepDive/"/>
      <url>/2025/11/17/HCCL_DeepDive/</url>
      
        <content type="html"><![CDATA[<h1 id="HCCL-Huawei-Collective-Communication-Library-设计文档"><a href="#HCCL-Huawei-Collective-Communication-Library-设计文档" class="headerlink" title="HCCL (Huawei Collective Communication Library) 设计文档"></a>HCCL (Huawei Collective Communication Library) 设计文档</h1><h2 id="1-项目概述"><a href="#1-项目概述" class="headerlink" title="1. 项目概述"></a>1. 项目概述</h2><h3 id="1-1-项目简介"><a href="#1-1-项目简介" class="headerlink" title="1.1 项目简介"></a>1.1 项目简介</h3><p>HCCL（Huawei Collective Communication Library，华为集合通信库）是基于昇腾AI处理器的高性能集合通信库，为单机多卡及多机多卡环境提供高效的数据并行和模型并行集合通信方案。</p><p>开源代码库：<a href="https://gitee.com/ascend/cann-hccl">https://gitee.com/ascend/cann-hccl</a></p><p><strong>版本信息：</strong> 配套CANN软件版本发行</p><p><strong>许可证：</strong> CANN Open Software License Agreement Version 1.0</p><h3 id="1-2-核心特性"><a href="#1-2-核心特性" class="headerlink" title="1.2 核心特性"></a>1.2 核心特性</h3><ul><li>✅ <strong>高性能通信算法</strong>：支持9种拓扑算法（Mesh、Ring、RHD、PairWise、Star、NHR、NB、AHC、Pipeline）</li><li>✅ <strong>灵活的通信模式</strong>：支持单机多卡和多机多卡场景</li><li>✅ <strong>智能算法选择</strong>：根据通信域信息和数据量自动选择最优算法</li><li>✅ <strong>分层网络优化</strong>：支持Server内和Server间分级通信</li><li>✅ <strong>多种集合操作</strong>：AllReduce、AllGather、ReduceScatter、Broadcast等</li></ul><h3 id="1-3-系统架构"><a href="#1-3-系统架构" class="headerlink" title="1.3 系统架构"></a>1.3 系统架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;适配层&quot;</span><br><span class="line">        A[图引擎适配] --&gt; B[单算子适配]</span><br><span class="line">        B --&gt; C[通信切分优化]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;集合通信业务层&quot;</span><br><span class="line">        D[通信框架模块]</span><br><span class="line">        E[通信算法模块]</span><br><span class="line">        </span><br><span class="line">        D --&gt; D1[通信域管理]</span><br><span class="line">        D --&gt; D2[算子业务串联]</span><br><span class="line">        D --&gt; D3[算法选择]</span><br><span class="line">        D --&gt; D4[资源申请]</span><br><span class="line">        D --&gt; D5[任务下发]</span><br><span class="line">        </span><br><span class="line">        E --&gt; E1[Mesh算法]</span><br><span class="line">        E --&gt; E2[Ring算法]</span><br><span class="line">        E --&gt; E3[RHD算法]</span><br><span class="line">        E --&gt; E4[PairWise算法]</span><br><span class="line">        E --&gt; E5[Star算法]</span><br><span class="line">        E --&gt; E6[NHR算法]</span><br><span class="line">        E --&gt; E7[NB算法]</span><br><span class="line">        E --&gt; E8[AHC算法]</span><br><span class="line">        E --&gt; E9[Pipeline算法]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;集合通信平台层&quot;</span><br><span class="line">        F[资源抽象]</span><br><span class="line">        G[维测能力]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    C --&gt; D</span><br><span class="line">    D &lt;--&gt; E</span><br><span class="line">    D --&gt; F</span><br><span class="line">    F --&gt; G</span><br><span class="line">    </span><br><span class="line">    style D fill:#e1d5e7</span><br><span class="line">    style E fill:#e1d5e7</span><br><span class="line">    style D1 fill:#fff2cc</span><br><span class="line">    style D2 fill:#fff2cc</span><br><span class="line">    style D3 fill:#fff2cc</span><br><span class="line">    style D4 fill:#fff2cc</span><br><span class="line">    style D5 fill:#fff2cc</span><br></pre></td></tr></table></figure><h2 id="2-核心架构设计"><a href="#2-核心架构设计" class="headerlink" title="2. 核心架构设计"></a>2. 核心架构设计</h2><h3 id="2-1-三层架构模型"><a href="#2-1-三层架构模型" class="headerlink" title="2.1 三层架构模型"></a>2.1 三层架构模型</h3><p>HCCL采用分层设计，从上到下分为三个核心层次：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A[适配层] --&gt; B[业务层]</span><br><span class="line">    B --&gt; C[平台层]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;本源码仓实现范围&quot;</span><br><span class="line">        B1[通信框架]</span><br><span class="line">        B2[通信算法]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    B -.包含.-&gt; B1</span><br><span class="line">    B -.包含.-&gt; B2</span><br><span class="line">    </span><br><span class="line">    style B1 fill:#e1d5e7</span><br><span class="line">    style B2 fill:#e1d5e7</span><br></pre></td></tr></table></figure><h4 id="2-1-1-适配层"><a href="#2-1-1-适配层" class="headerlink" title="2.1.1 适配层"></a>2.1.1 适配层</h4><p><strong>职责：</strong></p><ul><li>图引擎与单算子的对接适配</li><li>通信操作的切分与优化</li><li>任务分发策略制定</li></ul><h4 id="2-1-2-集合通信业务层（本仓核心）"><a href="#2-1-2-集合通信业务层（本仓核心）" class="headerlink" title="2.1.2 集合通信业务层（本仓核心）"></a>2.1.2 集合通信业务层（本仓核心）</h4><p><strong>通信框架模块：</strong></p><ul><li>通信域（Communicator）生命周期管理</li><li>集合通信算子的业务流程编排</li><li>算法选择策略与调度</li><li>与平台层协作完成资源申请</li><li>任务下发与执行管理</li></ul><p><strong>通信算法模块：</strong></p><ul><li>实现9种核心集合通信算法</li><li>资源消耗计算与评估</li><li>基于通信域信息的任务编排</li><li>算法性能模型（α-β模型）实现</li></ul><h4 id="2-1-3-集合通信平台层"><a href="#2-1-3-集合通信平台层" class="headerlink" title="2.1.3 集合通信平台层"></a>2.1.3 集合通信平台层</h4><p><strong>职责：</strong></p><ul><li>NPU硬件资源抽象与管理</li><li>HCCS链路资源管理</li><li>通信日志与性能监控</li><li>错误诊断与恢复机制</li></ul><h3 id="2-2-目录结构"><a href="#2-2-目录结构" class="headerlink" title="2.2 目录结构"></a>2.2 目录结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cann-hccl/</span><br><span class="line">├── src/domain/collective_communication/</span><br><span class="line">│   ├── algorithm/          # 通信算法实现</span><br><span class="line">│   └── framework/          # 通信框架实现</span><br><span class="line">├── inc/hccl/              # 对外头文件</span><br><span class="line">│   ├── hccl.h</span><br><span class="line">│   └── hccl_types.h</span><br><span class="line">├── docs/                   # 算法原理文档</span><br><span class="line">├── test/                   # 测试代码</span><br><span class="line">├── cmake/                  # 编译配置</span><br><span class="line">└── build.sh               # 编译脚本</span><br></pre></td></tr></table></figure><h2 id="3-集合通信算法详解"><a href="#3-集合通信算法详解" class="headerlink" title="3. 集合通信算法详解"></a>3. 集合通信算法详解</h2><p>HCCL的核心竞争力在于其丰富的集合通信算法库，针对不同的网络拓扑、节点规模和数据量提供最优解决方案。</p><h3 id="3-1-性能评估模型"><a href="#3-1-性能评估模型" class="headerlink" title="3.1 性能评估模型"></a>3.1 性能评估模型</h3><p>HCCL采用 <strong>α-β模型（Hockney模型）</strong> 进行性能评估：</p><p>$$<br>T &#x3D; \alpha + n\beta + n\gamma<br>$$</p><p><strong>参数说明：</strong></p><ul><li><strong>α</strong>：节点间的固定时延（启动开销）</li><li><strong>β</strong>：每byte数据传输耗时（带宽倒数）</li><li><strong>n</strong>：通信数据大小（bytes）</li><li><strong>γ</strong>：每byte数据规约计算耗时</li><li><strong>p</strong>：通信域节点个数</li></ul><h3 id="3-2-Mesh-算法"><a href="#3-2-Mesh-算法" class="headerlink" title="3.2 Mesh 算法"></a>3.2 Mesh 算法</h3><h4 id="3-2-1-算法原理"><a href="#3-2-1-算法原理" class="headerlink" title="3.2.1 算法原理"></a>3.2.1 算法原理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    N0((Rank0)) ---|全连接| N1((Rank1))</span><br><span class="line">    N0 ---|全连接| N2((Rank2))</span><br><span class="line">    N0 ---|全连接| N3((Rank3))</span><br><span class="line">    N1 ---|全连接| N2</span><br><span class="line">    N1 ---|全连接| N3</span><br><span class="line">    N2 ---|全连接| N3</span><br><span class="line">    </span><br><span class="line">    style N0 fill:#ffcccc</span><br><span class="line">    style N1 fill:#ccffcc</span><br><span class="line">    style N2 fill:#ccccff</span><br><span class="line">    style N3 fill:#ffffcc</span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ul><li><strong>拓扑：</strong> FullMesh互联，NPU间全连接</li><li><strong>时间复杂度：</strong> O(1)</li><li><strong>适用场景：</strong> Server内通信，小规模集群</li><li><strong>优势：</strong> 一步完成通信，延迟最低</li><li><strong>劣势：</strong> 资源开销大，难以扩展到大规模</li></ul><h4 id="3-2-2-执行流程示例（以AllReduce为例）"><a href="#3-2-2-执行流程示例（以AllReduce为例）" class="headerlink" title="3.2.2 执行流程示例（以AllReduce为例）"></a>3.2.2 执行流程示例（以AllReduce为例）</h4><p><strong>说明：</strong> Mesh算法支持所有集合通信原语（AllReduce、AllGather、ReduceScatter、Broadcast、Reduce、Scatter、Gather等），此处以AllReduce为典型示例展示执行流程。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant R0 as Rank0</span><br><span class="line">    participant R1 as Rank1</span><br><span class="line">    participant R2 as Rank2</span><br><span class="line">    participant R3 as Rank3</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Phase 1: ReduceScatter (并发)</span><br><span class="line">    Note over R0,R3: 每个节点将数据切分为p份，并发发送给所有其他节点</span><br><span class="line">    R0-&gt;&gt;R1: 发送chunk_1</span><br><span class="line">    R0-&gt;&gt;R2: 发送chunk_2</span><br><span class="line">    R0-&gt;&gt;R3: 发送chunk_3</span><br><span class="line">    R1-&gt;&gt;R0: 发送chunk_0</span><br><span class="line">    R1-&gt;&gt;R2: 发送chunk_2</span><br><span class="line">    R1-&gt;&gt;R3: 发送chunk_3</span><br><span class="line">    R2-&gt;&gt;R0: 发送chunk_0</span><br><span class="line">    R2-&gt;&gt;R1: 发送chunk_1</span><br><span class="line">    R2-&gt;&gt;R3: 发送chunk_3</span><br><span class="line">    R3-&gt;&gt;R0: 发送chunk_0</span><br><span class="line">    R3-&gt;&gt;R1: 发送chunk_1</span><br><span class="line">    R3-&gt;&gt;R2: 发送chunk_2</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Phase 2: 本地Reduce</span><br><span class="line">    Note over R0: 规约chunk_0</span><br><span class="line">    Note over R1: 规约chunk_1</span><br><span class="line">    Note over R2: 规约chunk_2</span><br><span class="line">    Note over R3: 规约chunk_3</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Phase 3: AllGather (并发)</span><br><span class="line">    Note over R0,R3: 每个节点并发向所有其他节点发送自己的规约结果</span><br><span class="line">    R0-&gt;&gt;R1: 广播chunk_0*</span><br><span class="line">    R0-&gt;&gt;R2: 广播chunk_0*</span><br><span class="line">    R0-&gt;&gt;R3: 广播chunk_0*</span><br><span class="line">    R1-&gt;&gt;R0: 广播chunk_1*</span><br><span class="line">    R1-&gt;&gt;R2: 广播chunk_1*</span><br><span class="line">    R1-&gt;&gt;R3: 广播chunk_1*</span><br><span class="line">    R2-&gt;&gt;R0: 广播chunk_2*</span><br><span class="line">    R2-&gt;&gt;R1: 广播chunk_2*</span><br><span class="line">    R2-&gt;&gt;R3: 广播chunk_2*</span><br><span class="line">    R3-&gt;&gt;R0: 广播chunk_3*</span><br><span class="line">    R3-&gt;&gt;R1: 广播chunk_3*</span><br><span class="line">    R3-&gt;&gt;R2: 广播chunk_3*</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: 所有节点持有完整规约结果</span><br></pre></td></tr></table></figure><p><strong>执行流程详细描述：</strong></p><p><strong>Phase 1: ReduceScatter（并发执行）</strong></p><ul><li><strong>数据准备</strong>: 每个节点将自己的n字节数据切分为p个chunk，每个chunk大小为n&#x2F;p字节</li><li><strong>并发发送</strong>: <ul><li>Rank0保留chunk_0，将chunk_1发送给Rank1，chunk_2发送给Rank2，chunk_3发送给Rank3</li><li>Rank1保留chunk_1，将chunk_0发送给Rank0，chunk_2发送给Rank2，chunk_3发送给Rank3</li><li>Rank2保留chunk_2，将chunk_0发送给Rank0，chunk_1发送给Rank1，chunk_3发送给Rank3</li><li>Rank3保留chunk_3，将chunk_0发送给Rank0，chunk_1发送给Rank1，chunk_2发送给Rank2</li></ul></li><li><strong>通信特点</strong>: 全连接并发，所有通信同时进行，利用FullMesh拓扑的双向带宽</li></ul><p><strong>Phase 2: 本地Reduce（本地计算）</strong></p><ul><li>Rank0对接收到的所有chunk_0进行规约：chunk_0* &#x3D; chunk_0(R0) + chunk_0(R1) + chunk_0(R2) + chunk_0(R3)</li><li>Rank1对接收到的所有chunk_1进行规约：chunk_1* &#x3D; chunk_1(R0) + chunk_1(R1) + chunk_1(R2) + chunk_1(R3)</li><li>Rank2对接收到的所有chunk_2进行规约：chunk_2* &#x3D; chunk_2(R0) + chunk_2(R1) + chunk_2(R2) + chunk_2(R3)</li><li>Rank3对接收到的所有chunk_3进行规约：chunk_3* &#x3D; chunk_3(R0) + chunk_3(R1) + chunk_3(R2) + chunk_3(R3)</li><li>此时每个节点持有1&#x2F;p的完整规约结果</li></ul><p><strong>Phase 3: AllGather（并发执行）</strong></p><ul><li><strong>并发广播</strong>: <ul><li>Rank0将chunk_0*并发发送给Rank1, Rank2, Rank3</li><li>Rank1将chunk_1*并发发送给Rank0, Rank2, Rank3</li><li>Rank2将chunk_2*并发发送给Rank0, Rank1, Rank3</li><li>Rank3将chunk_3*并发发送给Rank0, Rank1, Rank2</li></ul></li><li><strong>最终状态</strong>: 所有节点持有完整的规约结果[chunk_0*, chunk_1*, chunk_2*, chunk_3*]</li><li><strong>通信特点</strong>: 全连接并发，充分利用FullMesh拓扑的所有链路</li></ul><p><strong>其他原语：</strong></p><ul><li><strong>AllGather</strong>: 直接执行Phase 3（并发收集所有节点数据）</li><li><strong>ReduceScatter</strong>: 执行Phase 1 + Phase 2（并发规约后分散）</li><li><strong>Broadcast</strong>: 根节点向所有节点并发发送完整数据</li><li><strong>Gather</strong>: 所有节点向根节点并发发送数据</li></ul><h4 id="3-2-3-性能模型"><a href="#3-2-3-性能模型" class="headerlink" title="3.2.3 性能模型"></a>3.2.3 性能模型</h4><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>Scatter</td><td>$\alpha + \frac{1}{p}n\beta$</td><td>一步完成，根节点向p个节点并发发送，每节点接收n&#x2F;p数据</td></tr><tr><td>Gather</td><td>$\alpha + \frac{1}{p}n\beta$</td><td>一步完成，p个节点向根节点并发发送，根节点接收全部数据</td></tr><tr><td>Broadcast</td><td>$2\alpha + \frac{2}{p}n\beta$</td><td>Scatter + AllGather实现（两步），每步传输部分数据</td></tr><tr><td>Reduce</td><td>$2\alpha + \frac{2}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter + Gather实现，需规约所有输入数据</td></tr><tr><td>ReduceScatter</td><td>$\alpha + \frac{1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>一步完成，并发规约并分发，每节点接收n&#x2F;p结果</td></tr><tr><td>AllGather</td><td>$\alpha + \frac{1}{p}n\beta$</td><td>一步完成，全连接并发传输，每节点发送n&#x2F;p数据</td></tr><tr><td>AllReduce</td><td>$2\alpha + \frac{2}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter + AllGather两阶段，总共两步通信</td></tr></tbody></table><h3 id="3-3-Ring-算法"><a href="#3-3-Ring-算法" class="headerlink" title="3.3 Ring 算法"></a>3.3 Ring 算法</h3><h4 id="3-3-1-算法原理"><a href="#3-3-1-算法原理" class="headerlink" title="3.3.1 算法原理"></a>3.3.1 算法原理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    R0((Rank0)) --&gt;|右手卡| R1((Rank1))</span><br><span class="line">    R1 --&gt;|右手卡| R2((Rank2))</span><br><span class="line">    R2 --&gt;|右手卡| R3((Rank3))</span><br><span class="line">    R3 --&gt;|右手卡| R0</span><br><span class="line">    </span><br><span class="line">    R0 -.-&gt;|左手卡| R3</span><br><span class="line">    R1 -.-&gt;|左手卡| R0</span><br><span class="line">    R2 -.-&gt;|左手卡| R1</span><br><span class="line">    R3 -.-&gt;|左手卡| R2</span><br><span class="line">    </span><br><span class="line">    style R0 fill:#ffcccc</span><br><span class="line">    style R1 fill:#ccffcc</span><br><span class="line">    style R2 fill:#ccccff</span><br><span class="line">    style R3 fill:#ffffcc</span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ul><li><strong>拓扑：</strong> 环形结构，每个节点只与左右邻居通信</li><li><strong>时间复杂度：</strong> O(p-1) - 线性复杂度</li><li><strong>适用场景：</strong> <ul><li>Server内和Server间通信</li><li>小规模集群或小数据量</li><li>网络拥塞场景</li><li>Pipeline不适用的场景</li></ul></li></ul><h4 id="3-3-2-执行流程示例（以AllReduce为例）"><a href="#3-3-2-执行流程示例（以AllReduce为例）" class="headerlink" title="3.3.2 执行流程示例（以AllReduce为例）"></a>3.3.2 执行流程示例（以AllReduce为例）</h4><p><strong>说明：</strong> Ring算法支持多种集合通信原语（AllReduce、AllGather、ReduceScatter、Broadcast、Reduce、Scatter、Gather等），此处以AllReduce为典型示例展示执行流程。</p><h5 id="两阶段概览"><a href="#两阶段概览" class="headerlink" title="两阶段概览"></a>两阶段概览</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;阶段1: ReduceScatter (p-1步)&quot;</span><br><span class="line">        A1[数据切分为p块] --&gt; A2[沿环传输并规约]</span><br><span class="line">        A2 --&gt; A3[每个节点持有1/p规约结果]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;阶段2: AllGather (p-1步)&quot;</span><br><span class="line">        B1[每个节点持有1/p数据] --&gt; B2[沿环传输完整数据]</span><br><span class="line">        B2 --&gt; B3[所有节点获得完整结果]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    A3 --&gt; B1</span><br></pre></td></tr></table></figure><h5 id="详细执行步骤（4节点示例）"><a href="#详细执行步骤（4节点示例）" class="headerlink" title="详细执行步骤（4节点示例）"></a>详细执行步骤（4节点示例）</h5><p><strong>阶段1: ReduceScatter（p-1&#x3D;3步完成）</strong></p><p>Ring算法的ReduceScatter阶段：每个节点在每一步都向右邻居发送一个数据块，并接收左邻居的数据块进行规约。经过p-1步后，每个节点持有一个完整规约的数据块。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant R0 as Rank0&lt;br/&gt;[A0,B0,C0,D0]</span><br><span class="line">    participant R1 as Rank1&lt;br/&gt;[A1,B1,C1,D1]</span><br><span class="line">    participant R2 as Rank2&lt;br/&gt;[A2,B2,C2,D2]</span><br><span class="line">    participant R3 as Rank3&lt;br/&gt;[A3,B3,C3,D3]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 1: 每个节点向右发送一个块</span><br><span class="line">    R0-&gt;&gt;R1: D0</span><br><span class="line">    R1-&gt;&gt;R2: D1  </span><br><span class="line">    R2-&gt;&gt;R3: D2</span><br><span class="line">    R3-&gt;&gt;R0: D3</span><br><span class="line">    Note over R0: 接收D3，持有[A0,B0,C0,D0+D3]</span><br><span class="line">    Note over R1: 接收D0，持有[A1,B1,C1,D0+D1]</span><br><span class="line">    Note over R2: 接收D1，持有[A2,B2,C2+D1,D2]</span><br><span class="line">    Note over R3: 接收D2，持有[A3+D2,B3,C3,D3]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 2: 继续向右发送刚规约的块</span><br><span class="line">    R0-&gt;&gt;R1: C0</span><br><span class="line">    R1-&gt;&gt;R2: D0+D1</span><br><span class="line">    R2-&gt;&gt;R3: C2+D1  </span><br><span class="line">    R3-&gt;&gt;R0: D2+D3</span><br><span class="line">    Note over R0: 接收D2+D3，持有[A0,B0,C0,D*]</span><br><span class="line">    Note over R1: 接收C0，持有[A1,B1,C0+C1,D*]</span><br><span class="line">    Note over R2: 接收D0+D1，持有[A2,B2,C*,D*]</span><br><span class="line">    Note over R3: 接收C2+D1，持有[A*,B3,C*,D3]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 3: 最后一轮</span><br><span class="line">    R0-&gt;&gt;R1: B0</span><br><span class="line">    R1-&gt;&gt;R2: C0+C1</span><br><span class="line">    R2-&gt;&gt;R3: B2</span><br><span class="line">    R3-&gt;&gt;R0: C*</span><br><span class="line">    Note over R0: 接收C*，持有[A0,B0,C*,D*]</span><br><span class="line">    Note over R1: 接收B0，持有[A1,B0+B1,C*,D*]</span><br><span class="line">    Note over R2: 接收C0+C1，持有[A2,B*,C*,D2]</span><br><span class="line">    Note over R3: 接收B2，持有[A3,B2+B3,C3,D*]</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 上述简化示例未完整展示。实际上ReduceScatter需要p-1&#x3D;3步，每步每个节点都在某个特定位置进行规约。最终：</p><ul><li>Rank0持有A块的完整规约结果A*</li><li>Rank1持有B块的完整规约结果B*  </li><li>Rank2持有C块的完整规约结果C*</li><li>Rank3持有D块的完整规约结果D*</li></ul><p><strong>阶段2: AllGather（3步完成）</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant R0 as Rank0</span><br><span class="line">    participant R1 as Rank1</span><br><span class="line">    participant R2 as Rank2</span><br><span class="line">    participant R3 as Rank3</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: 每个节点持有1/4规约结果</span><br><span class="line">    Note over R0: [A*,B*,-,-]</span><br><span class="line">    Note over R1: [-,B*,C*,-]</span><br><span class="line">    Note over R2: [-,-,C*,D*]</span><br><span class="line">    Note over R3: [A*,-,-,D*]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 1: 沿环传输收集</span><br><span class="line">    R0-&gt;&gt;R1: A*</span><br><span class="line">    R1-&gt;&gt;R2: B*</span><br><span class="line">    R2-&gt;&gt;R3: C*</span><br><span class="line">    R3-&gt;&gt;R0: D*</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 2: 继续传输</span><br><span class="line">    R0-&gt;&gt;R1: D*</span><br><span class="line">    R1-&gt;&gt;R2: A*</span><br><span class="line">    R2-&gt;&gt;R3: B*</span><br><span class="line">    R3-&gt;&gt;R0: C*</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 3: 最后一轮</span><br><span class="line">    R0-&gt;&gt;R1: C*</span><br><span class="line">    R1-&gt;&gt;R2: D*</span><br><span class="line">    R2-&gt;&gt;R3: A*</span><br><span class="line">    R3-&gt;&gt;R0: B*</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: 所有节点持有完整结果[A*,B*,C*,D*]</span><br></pre></td></tr></table></figure><p><strong>执行流程详细描述：</strong></p><p><strong>阶段1: ReduceScatter（p-1&#x3D;3步完成）</strong></p><p>Ring算法的核心思想：数据切分为p块，每个节点在每一步向右邻居发送一个块，从左邻居接收一个块并规约。经过p-1步后，每个节点持有一个完整规约的数据块。</p><ul><li><p><strong>初始状态</strong>: </p><ul><li>Rank0持有[A0, B0, C0, D0]</li><li>Rank1持有[A1, B1, C1, D1]</li><li>Rank2持有[A2, B2, C2, D2]</li><li>Rank3持有[A3, B3, C3, D3]</li></ul></li><li><p><strong>Step 1</strong>: </p><ul><li>发送：R0→D0→R1, R1→D1→R2, R2→D2→R3, R3→D3→R0</li><li>规约：每个节点将接收的D块与本地D块规约</li><li>结果：R0持有D0+D3, R1持有D0+D1, R2持有D1+D2, R3持有D2+D3</li><li>数据流向：环形顺时针流动</li></ul></li><li><p><strong>Step 2</strong>: </p><ul><li>发送：R0→C0→R1, R1→(D0+D1)→R2, R2→(C2+D1)→R3, R3→(D2+D3)→R0</li><li>规约：每个节点将接收的块与本地对应块规约</li><li>关键：R2完成D块的完整规约D* &#x3D; D0+D1+D2+D3</li><li>数据流向：继续顺时针，规约块逐步完成</li></ul></li><li><p><strong>Step 3</strong>: </p><ul><li>发送：R0→B0→R1, R1→(C0+C1)→R2, R2→B2→R3, R3→C*→R0</li><li>规约：R0完成D*的接收，R1和R2完成更多规约</li><li>注：完整算法需继续执行直到所有块规约完成</li></ul></li><li><p><strong>最终状态</strong>（经过p-1步后）:</p><ul><li>Rank0持有A* &#x3D; A0+A1+A2+A3</li><li>Rank1持有B* &#x3D; B0+B1+B2+B3</li><li>Rank2持有C* &#x3D; C0+C1+C2+C3</li><li>Rank3持有D* &#x3D; D0+D1+D2+D3</li></ul></li></ul><p><strong>阶段2: AllGather（p-1&#x3D;3步完成）</strong></p><p>AllGather阶段的目标：将每个节点持有的唯一规约结果块收集到所有节点。</p><ul><li><p><strong>初始状态</strong>:</p><ul><li>Rank0持有[A*, -, -, -]（实际在位置0）</li><li>Rank1持有[-, B*, -, -]（实际在位置1）</li><li>Rank2持有[-, -, C*, -]（实际在位置2）</li><li>Rank3持有[-, -, -, D*]（实际在位置3）</li></ul></li><li><p><strong>Step 1</strong>:</p><ul><li>发送：R0→A*→R1, R1→B*→R2, R2→C*→R3, R3→D*→R0</li><li>接收：每个节点接收一个新的规约块</li><li>结果：R0持有[A*,-,-,D*], R1持有[A*,B*,-,-], R2持有[-,B*,C*,-], R3持有[-,-,C*,D*]</li></ul></li><li><p><strong>Step 2</strong>:</p><ul><li>发送：R0→D*→R1, R1→A*→R2, R2→B*→R3, R3→C*→R0</li><li>接收：每个节点再接收一个规约块</li><li>结果：R0持有[A*,-,C*,D*], R1持有[A*,B*,D*,-], R2持有[A*,B*,C*,-], R3持有[-,B*,C*,D*]</li></ul></li><li><p><strong>Step 3</strong>:</p><ul><li>发送：R0→C*→R1, R1→D*→R2, R2→A*→R3, R3→B*→R0</li><li>接收：每个节点接收最后一个缺失的块</li><li>结果：所有节点持有[A*, B*, C*, D*]</li></ul></li><li><p><strong>通信特点</strong>:</p><ul><li>每步只使用环上的单向链路</li><li>数据沿环顺时针流动</li><li>每步传输的数据量为n&#x2F;p字节</li><li>无需规约操作，纯数据传输</li></ul></li></ul><p><strong>其他原语执行方式：</strong></p><ul><li><strong>Broadcast</strong>: 数据不切分，沿环传输p-1步，每步传输完整数据</li><li><strong>Reduce</strong>: 类似Broadcast，沿环传输并规约，最终根节点得到结果</li><li><strong>AllGather</strong>: 仅执行阶段2，数据切分后沿环收集</li><li><strong>ReduceScatter</strong>: 仅执行阶段1，数据切分后沿环规约</li></ul><h4 id="3-3-3-性能模型"><a href="#3-3-3-性能模型" class="headerlink" title="3.3.3 性能模型"></a>3.3.3 性能模型</h4><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>Scatter</td><td>$(p-1)\alpha + \frac{p-1}{p}n\beta$</td><td>沿环传输p-1步，每步传输1&#x2F;p数据</td></tr><tr><td>Gather</td><td>$(p-1)\alpha + \frac{p-1}{p}n\beta$</td><td>沿环传输p-1步，每步传输1&#x2F;p数据</td></tr><tr><td>Broadcast</td><td>$(p-1)\alpha + (p-1)n\beta$</td><td>沿环传输p-1步，每步传输完整数据</td></tr><tr><td>Reduce</td><td>$(p-1)\alpha + (p-1)n\beta + (p-1)n\gamma$</td><td>沿环传输p-1步，每步传输完整数据并规约</td></tr><tr><td>ReduceScatter</td><td>$(p-1)\alpha + \frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>数据切分为p块，沿环传输p-1步并规约</td></tr><tr><td>AllGather</td><td>$(p-1)\alpha + \frac{p-1}{p}n\beta$</td><td>数据切分为p块，沿环传输p-1步收集</td></tr><tr><td>AllReduce</td><td>$2(p-1)\alpha + 2\frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter(p-1步) + AllGather(p-1步)</td></tr></tbody></table><h3 id="3-4-RHD-Recursive-Halving-Doubling-算法"><a href="#3-4-RHD-Recursive-Halving-Doubling-算法" class="headerlink" title="3.4 RHD (Recursive Halving-Doubling) 算法"></a>3.4 RHD (Recursive Halving-Doubling) 算法</h3><h4 id="3-4-1-算法原理"><a href="#3-4-1-算法原理" class="headerlink" title="3.4.1 算法原理"></a>3.4.1 算法原理</h4><p>递归二分和倍增算法，通过对数级的通信步数实现高效通信。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;非2的幂次：p=5 (2²+1)&quot;</span><br><span class="line">        A1[5个Rank] --&gt; A2[Rank1数据合并到Rank0]</span><br><span class="line">        A2 --&gt; A3[变为4个有效Rank: 2²]</span><br><span class="line">        A3 --&gt; A4[执行标准RHD]</span><br><span class="line">        A4 --&gt; A5[Rank0数据复制到Rank1]</span><br><span class="line">        A5 --&gt; A6[完成]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;2的幂次：p=4&quot;</span><br><span class="line">        B1[4个Rank] --&gt; B2[两两交换并规约: log₂4=2步]</span><br><span class="line">        B2 --&gt; B3[ReduceScatter完成]</span><br><span class="line">        B3 --&gt; B4[两两拼接: 2步]</span><br><span class="line">        B4 --&gt; B5[AllGather完成]</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ul><li><strong>时间复杂度：</strong> $O(\lceil \log_2 N \rceil)$ - 对数复杂度</li><li><strong>适用场景：</strong><ul><li>大规模集群（Server数量多）</li><li>Server数量为2的幂次时性能最优</li><li>中小数据量通信</li></ul></li></ul><h4 id="3-4-2-通信步骤示例（p-4，以AllReduce为例）"><a href="#3-4-2-通信步骤示例（p-4，以AllReduce为例）" class="headerlink" title="3.4.2 通信步骤示例（p&#x3D;4，以AllReduce为例）"></a>3.4.2 通信步骤示例（p&#x3D;4，以AllReduce为例）</h4><p><strong>说明：</strong> RHD算法支持多种集合通信原语（AllReduce、ReduceScatter、AllGather、Broadcast、Reduce等），此处以AllReduce为典型示例展示通信步骤。</p><p><strong>ReduceScatter阶段（Recursive Halving）：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant R0 as Rank0&lt;br/&gt;[A0,B0,C0,D0]</span><br><span class="line">    participant R1 as Rank1&lt;br/&gt;[A1,B1,C1,D1]</span><br><span class="line">    participant R2 as Rank2&lt;br/&gt;[A2,B2,C2,D2]</span><br><span class="line">    participant R3 as Rank3&lt;br/&gt;[A3,B3,C3,D3]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 1: Distance=1, XOR操作(0⊕1=1, 2⊕3=1)</span><br><span class="line">    Note over R0,R3: 每对节点交换后半部分数据并规约</span><br><span class="line">    R0-&gt;&gt;R1: 发送[C0,D0], 接收[C1,D1]</span><br><span class="line">    R1-&gt;&gt;R0: 发送[C1,D1], 接收[C0,D0]</span><br><span class="line">    R2-&gt;&gt;R3: 发送[C2,D2], 接收[C3,D3]</span><br><span class="line">    R3-&gt;&gt;R2: 发送[C3,D3], 接收[C2,D2]</span><br><span class="line">    Note over R0: 持有[A0,B0,C0+C1,D0+D1]</span><br><span class="line">    Note over R1: 持有[A1,B1,C0+C1,D0+D1]</span><br><span class="line">    Note over R2: 持有[A2,B2,C2+C3,D2+D3]</span><br><span class="line">    Note over R3: 持有[A3,B3,C2+C3,D2+D3]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 2: Distance=2, XOR操作(0⊕2=2, 1⊕3=2)</span><br><span class="line">    Note over R0,R3: 交换现有后半部分并规约</span><br><span class="line">    R0-&gt;&gt;R2: 发送[C0+C1,D0+D1], 接收[C2+C3,D2+D3]</span><br><span class="line">    R2-&gt;&gt;R0: 发送[C2+C3,D2+D3], 接收[C0+C1,D0+D1]</span><br><span class="line">    R1-&gt;&gt;R3: 发送[C0+C1,D0+D1], 接收[C2+C3,D2+D3]</span><br><span class="line">    R3-&gt;&gt;R1: 发送[C2+C3,D2+D3], 接收[C0+C1,D0+D1]</span><br><span class="line">    Note over R0: 持有[A0,B0,C*,D*] (C*=C0+C1+C2+C3)</span><br><span class="line">    Note over R1: 持有[A1,B1,C*,D*]</span><br><span class="line">    Note over R2: 持有[A2,B2,C*,D*]</span><br><span class="line">    Note over R3: 持有[A3,B3,C*,D*]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: 继续处理前半部分（此处省略）</span><br><span class="line">    Note over R0,R3: 最终每个Rank持有不同的完整规约块</span><br></pre></td></tr></table></figure><p><strong>AllGather阶段（Recursive Doubling）：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant R0 as Rank0</span><br><span class="line">    participant R1 as Rank1</span><br><span class="line">    participant R2 as Rank2</span><br><span class="line">    participant R3 as Rank3</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: 假设ReduceScatter后:</span><br><span class="line">    Note over R0: [A*,-,-,-]</span><br><span class="line">    Note over R1: [-,B*,-,-]</span><br><span class="line">    Note over R2: [-,-,C*,-]</span><br><span class="line">    Note over R3: [-,-,-,D*]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 1: Distance=2, 交换数据</span><br><span class="line">    R0-&gt;&gt;R2: 发送A*, 接收C*</span><br><span class="line">    R2-&gt;&gt;R0: 发送C*, 接收A*</span><br><span class="line">    R1-&gt;&gt;R3: 发送B*, 接收D*</span><br><span class="line">    R3-&gt;&gt;R1: 发送D*, 接收B*</span><br><span class="line">    Note over R0: [A*,-,C*,-]</span><br><span class="line">    Note over R1: [-,B*,-,D*]</span><br><span class="line">    Note over R2: [A*,-,C*,-]</span><br><span class="line">    Note over R3: [-,B*,-,D*]</span><br><span class="line">    </span><br><span class="line">    Note over R0,R3: Step 2: Distance=1, 交换数据</span><br><span class="line">    R0-&gt;&gt;R1: 发送[A*,C*], 接收[B*,D*]</span><br><span class="line">    R1-&gt;&gt;R0: 发送[B*,D*], 接收[A*,C*]</span><br><span class="line">    R2-&gt;&gt;R3: 发送[A*,C*], 接收[B*,D*]</span><br><span class="line">    R3-&gt;&gt;R2: 发送[B*,D*], 接收[A*,C*]</span><br><span class="line">    Note over R0,R3: 所有节点持有[A*,B*,C*,D*]</span><br></pre></td></tr></table></figure><p><strong>详细执行流程说明：</strong></p><p><strong>ReduceScatter阶段详解（Recursive Halving）：</strong></p><p>该阶段核心思想是通过递归对半分组（Distance Halving）将数据分散规约到各个节点。距离distance采用指数递减模式：p&#x2F;2 → p&#x2F;4 → … → 1。</p><p><strong>初始状态：</strong> 每个节点持有完整数据[A,B,C,D]，需要对4个块分别规约，最终每个节点持有其中一个完整规约块。</p><p><strong>Step 1 (Distance&#x3D;2)：</strong> 节点分为两组：{R0,R1} ↔ {R2,R3}。通过XOR找到通信对（Rank XOR 2），每个节点负责处理一半数据。</p><ul><li><strong>R0 ↔ R2通信</strong>：R0发送后半部分[C,D]并接收同样位置的[C,D]，对接收数据执行规约操作（C←C+C, D←D+D），同时接收R2的前半部分[A,B]并规约到本地前半部分</li><li><strong>R1 ↔ R3通信</strong>：同样的交换和规约操作</li><li><strong>处理后状态</strong>：<ul><li>R0持有[A0+A2, B0+B2, C0+C2, D0+D2]，但仅负责前半部分（后半部分将被丢弃）</li><li>R1持有[A1+A3, B1+B3, C1+C3, D1+D3]，仅负责前半部分</li><li>R2持有[A0+A2, B0+B2, C0+C2, D0+D2]，仅负责后半部分</li><li>R3持有[A1+A3, B1+B3, C1+C3, D1+D3]，仅负责后半部分</li></ul></li><li><strong>数据缩减</strong>：每个节点从4个块缩减为2个有效块</li></ul><p><strong>Step 2 (Distance&#x3D;1)：</strong> 在上一步的基础上继续对半分组。R0↔R1处理前半部分{A,B}，R2↔R3处理后半部分{C,D}。</p><ul><li><strong>R0 ↔ R1通信</strong>：交换和规约B块（R0最终持有完整规约的A块，R1持有B块）</li><li><strong>R2 ↔ R3通信</strong>：交换和规约D块（R2最终持有完整规约的C块，R3持有D块）</li><li><strong>处理后状态</strong>：<ul><li>R0: [A0+A1+A2+A3, -, -, -]（简写为[A*, -, -, -]）</li><li>R1: [-, B0+B1+B2+B3, -, -]（简写为[-, B*, -, -]）</li><li>R2: [-, -, C0+C1+C2+C3, -]（简写为[-, -, C*, -]）</li><li>R3: [-, -, -, D0+D1+D2+D3]（简写为[-, -, -, D*]）</li></ul></li><li><strong>数据缩减</strong>：每个节点从2个块缩减为1个完整规约块</li></ul><p><strong>ReduceScatter特点</strong>：log₂p步递归对半，每步数据量减半，所有节点同时工作，通信和计算高度重叠。</p><p><strong>AllGather阶段详解（Recursive Doubling）：</strong></p><p>该阶段核心思想是通过递归加倍距离（Distance Doubling）收集所有规约结果。距离distance采用指数递增模式：1 → 2 → 4 → …。</p><p><strong>初始状态：</strong> ReduceScatter完成后，每个节点持有一个完整规约块：R0持有[A*,-,-,-]，R1持有[-,B*,-,-]，R2持有[-,-,C*,-]，R3持有[-,-,-,D*]。</p><p><strong>Step 1 (Distance&#x3D;2)：</strong> 首先在距离为2的节点间交换数据。通过XOR找到通信对（Rank XOR 2）。</p><ul><li><strong>R0 ↔ R2通信</strong>：R0发送A<em>并接收C</em>，R2发送C<em>并接收A</em></li><li><strong>R1 ↔ R3通信</strong>：R1发送B<em>并接收D</em>，R3发送D<em>并接收B</em></li><li><strong>通信后状态</strong>：<ul><li>R0: [A*, -, C*, -]（持有第0和第2块的完整规约结果）</li><li>R1: [-, B*, -, D*]（持有第1和第3块）</li><li>R2: [A*, -, C*, -]（持有第0和第2块）</li><li>R3: [-, B*, -, D*]（持有第1和第3块）</li></ul></li><li><strong>数据增长</strong>：每个节点从1个块增长为2个块</li></ul><p><strong>Step 2 (Distance&#x3D;1)：</strong> 在距离为1的相邻节点间交换数据，完成最终收集。</p><ul><li><strong>R0 ↔ R1通信</strong>：R0发送[A*,C*]并接收[B*,D*]，R1发送[B*,D*]并接收[A*,C*]</li><li><strong>R2 ↔ R3通信</strong>：R2发送[A*,C*]并接收[B*,D*]，R3发送[B*,D*]并接收[A*,C*]</li><li><strong>最终状态</strong>：所有节点持有[A*, B*, C*, D*]，即所有数据的完整规约结果</li></ul><p><strong>AllGather特点</strong>：log₂p步递归加倍，每步传输数据量倍增（1块→2块→4块），无需计算操作，纯粹的数据收集。</p><p><strong>RHD算法整体特点</strong>：</p><ol><li><strong>对数级复杂度</strong>：总共2log₂p步通信（ReduceScatter log₂p步 + AllGather log₂p步）</li><li><strong>XOR通信模式</strong>：每步通过Rank XOR Distance确定通信对，保证无冲突并行</li><li><strong>数据量变化</strong>：ReduceScatter递减（n → n&#x2F;2 → n&#x2F;4 → …），AllGather递增（n&#x2F;p → 2n&#x2F;p → 4n&#x2F;p → …）</li><li><strong>最优性</strong>：对于2的幂次节点数，是理论最优算法（最少通信步数）</li><li><strong>限制条件</strong>：仅适用于节点数为2的幂次（非2幂次需要额外步骤处理）</li></ol><p><strong>其他原语执行方式：</strong></p><ul><li><strong>Broadcast</strong>: 采用Distance Halving策略，从根节点开始递归扩散</li><li><strong>ReduceScatter</strong>: 仅执行ReduceScatter阶段（Recursive Halving）</li><li><strong>AllGather</strong>: 仅执行AllGather阶段（Recursive Doubling）</li></ul><h4 id="3-4-3-性能模型"><a href="#3-4-3-性能模型" class="headerlink" title="3.4.3 性能模型"></a>3.4.3 性能模型</h4><p><strong>2的幂次（p &#x3D; 2^k）：</strong></p><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>Broadcast</td><td>$\lceil \log_2 p \rceil\alpha + \lceil \log_2 p \rceil n\beta$</td><td>Distance Halving策略，log₂p步，每步传输完整n字节数据</td></tr><tr><td>ReduceScatter</td><td>$\log_2 p \cdot \alpha + \frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>Vector Doubling + Distance Halving，log₂p步递归对半交换</td></tr><tr><td>AllGather</td><td>$\log_2 p \cdot \alpha + \frac{p-1}{p}n\beta$</td><td>Distance Doubling策略，log₂p步，每步数据量倍增</td></tr><tr><td>AllReduce</td><td>$2\log_2 p \cdot \alpha + 2\frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter(log₂p步) + AllGather(log₂p步)，规约一次</td></tr></tbody></table><p><strong>非2的幂次（需要额外步骤）：</strong></p><p>$$<br>\text{AllReduce} &#x3D; (2\lfloor \log_2 p \rfloor + 2)\alpha + (2\frac{p’-1}{p’} + 2)n\beta + (\frac{p’-1}{p’} + 1)n\gamma<br>$$</p><p>其中 $p’ &#x3D; 2^{\lfloor \log_2 p \rfloor}$</p><h3 id="3-5-PairWise-算法"><a href="#3-5-PairWise-算法" class="headerlink" title="3.5 PairWise 算法"></a>3.5 PairWise 算法</h3><h4 id="3-5-1-算法原理"><a href="#3-5-1-算法原理" class="headerlink" title="3.5.1 算法原理"></a>3.5.1 算法原理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Step 1&quot;</span><br><span class="line">        R0_1[Rank0] --&gt;|发送| R1_1[Rank1]</span><br><span class="line">        R1_1 --&gt;|发送| R0_1</span><br><span class="line">        R2_1[Rank2] --&gt;|发送| R3_1[Rank3]</span><br><span class="line">        R3_1 --&gt;|发送| R2_1</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Step 2&quot;</span><br><span class="line">        R0_2[Rank0] --&gt;|发送| R2_2[Rank2]</span><br><span class="line">        R2_2 --&gt;|发送| R0_2</span><br><span class="line">        R1_2[Rank1] --&gt;|发送| R3_2[Rank3]</span><br><span class="line">        R3_2 --&gt;|发送| R1_2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Step 3&quot;</span><br><span class="line">        R0_3[Rank0] --&gt;|发送| R3_3[Rank3]</span><br><span class="line">        R3_3 --&gt;|发送| R0_3</span><br><span class="line">        R1_3[Rank1] --&gt;|发送| R2_3[Rank2]</span><br><span class="line">        R2_3 --&gt;|发送| R1_3</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><p><strong>算法原理说明：</strong></p><p>PairWise算法是专为<strong>AllToAll&#x2F;AllToAllV算子</strong>设计的高效通信策略，核心目标是<strong>避免网络端口拥塞</strong>。在AllToAll操作中，每个节点需要向其他所有节点发送数据，传统方法可能导致某个节点同时向多个目标发送（”一打多”现象），造成端口带宽竞争和拥塞。</p><p><strong>核心机制：配对交换（Pairwise Exchange）</strong></p><p>PairWise通过将p个节点的通信组织为 <strong>(p-1)轮配对交换</strong>，每轮确保：</p><ol><li><strong>无冲突并行</strong>：每个节点在每轮只与一个对端通信</li><li><strong>端口独占</strong>：避免多流竞争单个网络端口</li><li><strong>循环配对</strong>：通过固定的配对模式覆盖所有节点对</li></ol><p><strong>配对模式（以4节点为例）：</strong></p><ul><li><p><strong>Step 1</strong>：{R0 ↔ R1}, {R2 ↔ R3}</p><ul><li>Rank0与Rank1双向交换数据</li><li>Rank2与Rank3双向交换数据</li><li>两对通信完全并行，无干扰</li></ul></li><li><p><strong>Step 2</strong>：{R0 ↔ R2}, {R1 ↔ R3}</p><ul><li>配对方式改变，Rank0与Rank2交换</li><li>Rank1与Rank3交换</li><li>依然保持两两配对，无冲突</li></ul></li><li><p><strong>Step 3</strong>：{R0 ↔ R3}, {R1 ↔ R2}</p><ul><li>最后一轮配对，Rank0与Rank3交换</li><li>Rank1与Rank2交换</li><li>完成所有节点对的通信</li></ul></li></ul><p><strong>数学规律：</strong></p><ul><li>对于p个节点，需要 <strong>(p-1)轮</strong> 完成所有节点对的交换</li><li>第k轮（k&#x3D;1,2,…,p-1），节点i与节点(i+k) mod p通信</li><li>每轮p&#x2F;2对节点同时通信（p为偶数）</li></ul><p><strong>优势分析：</strong></p><ol><li><strong>避免端口拥塞</strong>：传统AllToAll可能某节点同时向多个目标发送，导致单端口多流竞争；PairWise确保每节点每轮只有一条连接</li><li><strong>RDMA友好</strong>：RDMA环境下，点对点独占通信可获得最佳性能</li><li><strong>大数据量优化</strong>：数据量大时，端口拥塞影响显著，PairWise优势明显</li><li><strong>可预测性能</strong>：通信模式固定，延迟和带宽使用可精确预测</li></ol><p><strong>局限性：</strong></p><ul><li><strong>步数多</strong>：需要p-1步，相比RHD的log₂p步要多（但每步更高效）</li><li><strong>小数据不适用</strong>：小数据量时，启动开销(α)占主导，多步数劣势明显</li><li><strong>专用算子</strong>：主要用于AllToAll，不适用于AllReduce等需要规约的场景</li></ul><p><strong>特点：</strong></p><ul><li><strong>专用场景：</strong> AllToAll、AllToAllV算子</li><li><strong>时间复杂度：</strong> O(p-1) - 线性复杂度</li><li><strong>关键优势：</strong> 避免”一打多”现象（单端口多流竞争）</li><li><strong>适用场景：</strong><ul><li>大数据量通信</li><li>RDMA网络环境</li><li>需避免端口拥塞的场景</li></ul></li></ul><h4 id="3-5-2-性能模型"><a href="#3-5-2-性能模型" class="headerlink" title="3.5.2 性能模型"></a>3.5.2 性能模型</h4><p>定义 $n_{ij}$ 为节点i发送给节点j的数据量：</p><p>$$<br>T &#x3D; (p-1)\alpha + \beta \sum_{k&#x3D;1}^{p-1} \max_{i}(n_{i,i+k})<br>$$</p><h3 id="3-6-Star-算法"><a href="#3-6-Star-算法" class="headerlink" title="3.6 Star 算法"></a>3.6 Star 算法</h3><h4 id="3-6-1-算法原理"><a href="#3-6-1-算法原理" class="headerlink" title="3.6.1 算法原理"></a>3.6.1 算法原理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    Root((Root&lt;br/&gt;根节点))</span><br><span class="line">    </span><br><span class="line">    Root ---|一步完成| R1((Rank1))</span><br><span class="line">    Root ---|一步完成| R2((Rank2))</span><br><span class="line">    Root ---|一步完成| R3((Rank3))</span><br><span class="line">    Root ---|一步完成| R4((Rank4))</span><br><span class="line">    </span><br><span class="line">    style Root fill:#ff6b6b</span><br><span class="line">    style R1 fill:#4ecdc4</span><br><span class="line">    style R2 fill:#4ecdc4</span><br><span class="line">    style R3 fill:#4ecdc4</span><br><span class="line">    style R4 fill:#4ecdc4</span><br></pre></td></tr></table></figure><p><strong>算法原理说明：</strong></p><p>Star算法是最简单直接的集合通信算法，采用<strong>星型拓扑</strong>结构，所有通信都通过<strong>中心根节点（Root）<strong>进行。适用于</strong>Server内高带宽全连接</strong>或<strong>有明确根节点</strong>的通信场景。</p><p><strong>核心特点：单步完成（O(1)复杂度）</strong></p><p>Star算法的核心优势是<strong>一步到位</strong>：根节点直接与所有其他节点通信，无需多跳转发。这在高带宽、低延迟的网络环境下性能最优。</p><p><strong>典型应用场景：</strong></p><ol><li><p><strong>Broadcast（广播）</strong>：</p><ul><li>Root节点持有数据，需要发送给所有其他节点</li><li>执行：Root → {R1, R2, R3, R4}同时发送</li><li>一步完成，耗时 &#x3D; α + nβ（一次延迟 + 传输时间）</li></ul></li><li><p><strong>Reduce（规约到根节点）</strong>：</p><ul><li>所有节点的数据需要规约到Root</li><li>执行：{R1, R2, R3, R4} → Root同时发送并在Root规约</li><li>一步完成，耗时 &#x3D; α + nβ + nγ（延迟 + 传输 + 规约）</li></ul></li><li><p><strong>Gather（收集到根节点）</strong>：</p><ul><li>所有节点的数据收集到Root（无需规约）</li><li>执行：{R1, R2, R3, R4} → Root同时发送</li><li>一步完成，耗时 &#x3D; α + nβ</li></ul></li><li><p><strong>Scatter（分发）</strong>：</p><ul><li>Root将不同数据块分发给不同节点</li><li>执行：Root → {R1, R2, R3, R4}同时发送不同块</li><li>一步完成，耗时 &#x3D; α + (n&#x2F;p)β（每个节点接收n&#x2F;p大小数据）</li></ul></li></ol><p><strong>网络拓扑要求：</strong></p><ul><li><strong>物理全连接</strong>：Root与所有节点直接相连（如Server内NVLink&#x2F;PCIe全连接）</li><li><strong>高带宽链路</strong>：链路带宽足够高，可支持Root同时多流发送而不拥塞</li><li><strong>低延迟</strong>：单跳延迟小，一步通信开销可接受</li></ul><p><strong>优势：</strong></p><ol><li><strong>最优时间复杂度</strong>：O(1)，理论上最快</li><li><strong>逻辑简单</strong>：无需复杂调度和同步</li><li><strong>Server内最优</strong>：Server内NPU通过NVLink全连接，Star是首选</li></ol><p><strong>局限性：</strong></p><ol><li><strong>根节点瓶颈</strong>：Root需要同时与p-1个节点通信，带宽压力大<ul><li>发送带宽：Root需发送(p-1)×n数据（Broadcast场景）</li><li>接收带宽：Root需接收(p-1)×n数据（Reduce场景）</li></ul></li><li><strong>不适合Server间</strong>：跨Server通信带宽有限，Root瓶颈严重</li><li><strong>无负载均衡</strong>：所有流量集中在Root，其他节点链路利用率低</li><li><strong>不支持AllToAll</strong>：AllToAll需要所有节点对通信，Star无法高效实现</li></ol><p><strong>适用原语限制：</strong></p><p>✅ <strong>适用</strong>：Broadcast、Reduce、Gather、Scatter（有明确根节点的单向通信）<br>❌ <strong>不适用</strong>：AllReduce、AllGather、ReduceScatter、AllToAll（需要所有节点间通信）</p><p><strong>特点：</strong></p><ul><li><strong>拓扑：</strong> 星型或全连接</li><li><strong>时间复杂度：</strong> O(1) - 单步完成</li><li><strong>适用算子：</strong> Broadcast、Reduce、Gather、Scatter</li><li><strong>适用场景：</strong> Server内通信，有根节点的操作</li></ul><h4 id="3-6-2-性能模型"><a href="#3-6-2-性能模型" class="headerlink" title="3.6.2 性能模型"></a>3.6.2 性能模型</h4><p>$$<br>T &#x3D; \alpha + n\beta<br>$$</p><p>非常简洁，仅一步通信完成。</p><h3 id="3-7-NHR-Nonuniform-Hierarchical-Ring-算法"><a href="#3-7-NHR-Nonuniform-Hierarchical-Ring-算法" class="headerlink" title="3.7 NHR (Nonuniform Hierarchical Ring) 算法"></a>3.7 NHR (Nonuniform Hierarchical Ring) 算法</h3><h4 id="3-7-1-算法原理"><a href="#3-7-1-算法原理" class="headerlink" title="3.7.1 算法原理"></a>3.7.1 算法原理</h4><p>非均衡的层次环算法，通过构建N棵生成树实现高效通信。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Rank Size = 4 (2的幂次)&quot;</span><br><span class="line">        A1[初始状态] --&gt; A2[Step 1: 交换1/2数据]</span><br><span class="line">        A2 --&gt; A3[Step 2: 交换1/4数据]</span><br><span class="line">        A3 --&gt; A4[完成]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Rank Size = 5 (非2的幂次)&quot;</span><br><span class="line">        B1[初始状态] --&gt; B2[Step 1: 不均匀切片]</span><br><span class="line">        B2 --&gt; B3[Step 2: 大部分连续收发]</span><br><span class="line">        B3 --&gt; B4[Step 3: 少量离散处理]</span><br><span class="line">        B4 --&gt; B5[完成]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Note1[树深度: ⌈log₂N⌉]</span><br><span class="line">    Note2[优化: 聚合发送&lt;br/&gt;减少网络包数]</span><br></pre></td></tr></table></figure><p><strong>算法原理说明：</strong></p><p>NHR（Nonuniform Hierarchical Ring）算法是<strong>非均衡的层次化环算法</strong>，专门解决<strong>节点数不是2的幂次</strong>时的高效通信问题。传统RHD算法要求节点数为2的幂次，否则性能大幅下降；NHR通过<strong>N棵生成树</strong>和<strong>不均匀切片</strong>策略，在任意节点数下都能保持对数级复杂度。</p><p><strong>核心创新：N棵生成树（N Spanning Trees）</strong></p><p>NHR的关键思想是构建<strong>N &#x3D; ⌈log₂p⌉棵生成树</strong>，每棵树负责一部分数据的通信：</p><ul><li><strong>树的深度</strong>：⌈log₂p⌉（与节点数对数相关）</li><li><strong>树的结构</strong>：每棵树根据节点编号和步数动态确定父子关系</li><li><strong>数据分配</strong>：将总数据均匀或非均匀切分到N棵树</li></ul><p><strong>2的幂次 vs 非2的幂次对比：</strong></p><p><strong>场景1：Rank Size &#x3D; 4（2的幂次）</strong></p><ul><li><strong>Step 1</strong>：交换1&#x2F;2数据（距离&#x3D;2）<ul><li>R0 ↔ R2，R1 ↔ R3</li><li>每个节点交换一半数据</li></ul></li><li><strong>Step 2</strong>：交换1&#x2F;4数据（距离&#x3D;1）<ul><li>R0 ↔ R1，R2 ↔ R3</li><li>每个节点再交换剩余的一半</li></ul></li><li><strong>完成</strong>：2步（log₂4&#x3D;2），每步数据量递减</li></ul><p><strong>场景2：Rank Size &#x3D; 5（非2的幂次）</strong></p><ul><li><strong>问题</strong>：不能均匀对半分，需要不均匀切片</li><li><strong>Step 1</strong>：不均匀切片（某些节点交换2&#x2F;5，某些交换3&#x2F;5）<ul><li>根据树结构动态确定交换量</li></ul></li><li><strong>Step 2</strong>：大部分连续收发（利用连续内存，减少小包）<ul><li>聚合发送优化，减少网络包数</li></ul></li><li><strong>Step 3</strong>：少量离散处理（处理不对齐部分）<ul><li>完成剩余数据交换</li></ul></li><li><strong>完成</strong>：3步（⌈log₂5⌉&#x3D;3），虽然不均匀但保持对数复杂度</li></ul><p><strong>关键技术：聚合发送优化（Aggregated Send）</strong></p><p>NHR的另一个优化是针对<strong>小数据包场景</strong>：</p><ul><li><strong>问题</strong>：多棵树可能产生大量小数据包，网络包头开销大</li><li><strong>解决</strong>：在小数据场景下，采用<strong>单棵树策略</strong>，将所有数据通过一棵树传输</li><li><strong>效果</strong>：减少网络包数量，降低协议栈开销</li></ul><p><strong>流量分布优化：</strong></p><p>NHR算法设计时考虑了<strong>物理位置相近性</strong>：</p><ul><li><strong>最大流量</strong>：尽量安排在物理位置相近的节点间（如同Server内）</li><li><strong>减少冲突</strong>：通过树结构避免多流竞争同一链路</li><li><strong>层次化友好</strong>：适配Server内+Server间的层次化网络</li></ul><p><strong>适用场景：</strong></p><ol><li><strong>大规模集群</strong>：Server数量多，节点数往往不是2的幂次</li><li><strong>非对称拓扑</strong>：节点数任意（5、6、7等），不受2的幂次限制</li><li><strong>小数据包通信</strong>：聚合发送优化提升小包性能</li><li><strong>层次化网络</strong>：流量分布优化适配收敛比网络</li></ol><p><strong>与RHD对比：</strong></p><table><thead><tr><th>维度</th><th>RHD</th><th>NHR</th></tr></thead><tbody><tr><td>节点数要求</td><td>必须是2的幂次</td><td>任意节点数</td></tr><tr><td>时间复杂度</td><td>O(log₂p)</td><td>O(⌈log₂p⌉)</td></tr><tr><td>数据切分</td><td>均匀</td><td>可能不均匀</td></tr><tr><td>小数据优化</td><td>无</td><td>单棵树策略</td></tr><tr><td>实现复杂度</td><td>低</td><td>中等</td></tr></tbody></table><p><strong>算法优势：</strong></p><ol><li><strong>通用性强</strong>：任意节点数都能高效运行</li><li><strong>对数复杂度</strong>：保持⌈log₂p⌉步通信，接近理论最优</li><li><strong>灵活优化</strong>：可根据数据量、网络拓扑调整策略</li><li><strong>工程实用</strong>：大规模集群中节点数变化常见，NHR适应性好</li></ol><p><strong>特点：</strong></p><ul><li><strong>时间复杂度：</strong> $O(\lceil \log_2 N \rceil)$</li><li><strong>关键优势：</strong><ul><li>无论节点数是否为2的幂次，均保持对数复杂度</li><li>最大流量集中在物理位置相近节点间</li><li>减少流量冲突</li><li>小数据包场景优化（单棵树策略）</li></ul></li><li><strong>适用场景：</strong> 大规模集群，Server数量多</li></ul><h4 id="3-7-2-性能模型"><a href="#3-7-2-性能模型" class="headerlink" title="3.7.2 性能模型"></a>3.7.2 性能模型</h4><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>ReduceScatter</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>N棵生成树，树深度⌈log₂p⌉，聚合发送减少包数</td></tr><tr><td>AllGather</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta$</td><td>与ReduceScatter对称，⌈log₂p⌉步收集，无规约开销</td></tr><tr><td>AllReduce</td><td>$2\lceil \log_2 p \rceil\alpha + 2\frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter(⌈log₂p⌉步) + AllGather(⌈log₂p⌉步)</td></tr><tr><td>Scatter</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta$</td><td>小数据包场景优化，采用单棵树策略，⌈log₂p⌉步完成</td></tr><tr><td>Broadcast</td><td>$2\lceil \log_2 p \rceil\alpha + 2\frac{p-1}{p}n\beta$</td><td>Scatter(⌈log₂p⌉步) + AllGather(⌈log₂p⌉步)实现</td></tr></tbody></table><h3 id="3-8-NB-Nonuniform-Bruck-算法"><a href="#3-8-NB-Nonuniform-Bruck-算法" class="headerlink" title="3.8 NB (Nonuniform Bruck) 算法"></a>3.8 NB (Nonuniform Bruck) 算法</h3><h4 id="3-8-1-算法原理"><a href="#3-8-1-算法原理" class="headerlink" title="3.8.1 算法原理"></a>3.8.1 算法原理</h4><p>非均匀的数据块通信算法，通过动态调整步长的多重环状结构实现高效通信。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Rank Size = 4&quot;</span><br><span class="line">        A1[初始状态] --&gt; A2[Step 1: 步长=1]</span><br><span class="line">        A2 --&gt; A3[Step 2: 步长=2]</span><br><span class="line">        A3 --&gt; A4[完成 log₂4=2步]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Rank Size = 5&quot;</span><br><span class="line">        B1[初始状态] --&gt; B2[Step 1: 步长=1]</span><br><span class="line">        B2 --&gt; B3[Step 2: 步长=2]</span><br><span class="line">        B3 --&gt; B4[Step 3: 步长=4]</span><br><span class="line">        B4 --&gt; B5[完成 ⌈log₂5⌉=3步]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Note1[关键: 每步发送&lt;br/&gt;⌊N-1+2^k/2^k+1⌋ 份数据]</span><br></pre></td></tr></table></figure><p><strong>算法原理说明：</strong></p><p>NB（Nonuniform Bruck）算法是<strong>非均匀Bruck算法</strong>，是对经典Bruck算法的改进，专门解决<strong>任意节点数</strong>下的高效集合通信。与NHR类似，NB也是为了克服RHD算法对2的幂次节点数的限制，但采用了不同的技术路线：<strong>动态步长多重环</strong>。</p><p><strong>核心机制：动态步长递增（Dynamic Distance Increment）</strong></p><p>NB算法的核心特点是<strong>步长按指数递增</strong>：1 → 2 → 4 → 8 → …</p><ul><li><strong>第k步</strong>：每个节点与距离为2^(k-1)的节点通信</li><li><strong>通信模式</strong>：节点i与节点(i+2^(k-1)) mod p通信</li><li><strong>数据量</strong>：第k步发送 ⌊(p-1+2^k)&#x2F;2^(k+1)⌋ 份数据（非均匀）</li></ul><p><strong>2的幂次 vs 非2的幂次对比：</strong></p><p><strong>场景1：Rank Size &#x3D; 4（2的幂次）</strong></p><ul><li><strong>初始状态</strong>：每个节点持有1份数据，需收集其他3份</li><li><strong>Step 1（步长&#x3D;1）</strong>：<ul><li>R0 ↔ R1，R2 ↔ R3</li><li>每个节点交换1份数据，现在各持有2份</li></ul></li><li><strong>Step 2（步长&#x3D;2）</strong>：<ul><li>R0 ↔ R2，R1 ↔ R3</li><li>每个节点交换2份数据，现在各持有4份（全部数据）</li></ul></li><li><strong>完成</strong>：2步（log₂4&#x3D;2），每步数据量加倍</li></ul><p><strong>场景2：Rank Size &#x3D; 5（非2的幂次）</strong></p><ul><li><strong>初始状态</strong>：每个节点持有1份数据，需收集其他4份</li><li><strong>Step 1（步长&#x3D;1）</strong>：<ul><li>R0 ↔ R1，R1 ↔ R2，R2 ↔ R3，R3 ↔ R4，R4 ↔ R0（环状）</li><li>每个节点交换1份，现在各持有2份</li><li><strong>数据量</strong>：⌊(5-1+2)&#x2F;4⌋ &#x3D; 1份</li></ul></li><li><strong>Step 2（步长&#x3D;2）</strong>：<ul><li>R0 ↔ R2，R1 ↔ R3，R2 ↔ R4，R3 ↔ R0，R4 ↔ R1</li><li>每个节点交换2份，现在各持有3或4份</li><li><strong>数据量</strong>：⌊(5-1+4)&#x2F;8⌋ &#x3D; 1份（但实际交换的是2份，因为有部分重复）</li></ul></li><li><strong>Step 3（步长&#x3D;4）</strong>：<ul><li>R0 ↔ R4，R1 ↔ R0，R2 ↔ R1，R3 ↔ R2，R4 ↔ R3</li><li>补齐剩余数据，所有节点持有全部5份</li><li><strong>数据量</strong>：⌊(5-1+8)&#x2F;16⌋ &#x3D; 0或1份（少量补充）</li></ul></li><li><strong>完成</strong>：3步（⌈log₂5⌉&#x3D;3），通过非均匀数据量分配完成</li></ul><p><strong>关键公式：每步发送数据量</strong></p><p>第k步发送数据量 &#x3D; ⌊(N-1+2^k)&#x2F;2^(k+1)⌋ 份</p><p><strong>公式解释：</strong></p><ul><li><strong>N-1</strong>：总共需要收集的其他节点数据份数</li><li><strong>2^k</strong>：当前步的”覆盖范围”补偿</li><li><strong>2^(k+1)</strong>：归一化因子</li><li><strong>向下取整</strong>：确保数据量为整数</li></ul><p><strong>示例（N&#x3D;5）：</strong></p><ul><li>k&#x3D;1: ⌊(5-1+2)&#x2F;4⌋ &#x3D; ⌊7&#x2F;4⌋ &#x3D; 1</li><li>k&#x3D;2: ⌊(5-1+4)&#x2F;8⌋ &#x3D; ⌊8&#x2F;8⌋ &#x3D; 1</li><li>k&#x3D;3: ⌊(5-1+8)&#x2F;16⌋ &#x3D; ⌊12&#x2F;16⌋ &#x3D; 0（剩余数据很少）</li></ul><p><strong>与RHD和NHR对比：</strong></p><table><thead><tr><th>维度</th><th>RHD</th><th>NHR</th><th>NB</th></tr></thead><tbody><tr><td>节点数要求</td><td>2的幂次</td><td>任意</td><td>任意</td></tr><tr><td>时间复杂度</td><td>O(log₂p)</td><td>O(⌈log₂p⌉)</td><td>O(⌈log₂p⌉)</td></tr><tr><td>通信模式</td><td>XOR</td><td>N棵生成树</td><td>动态步长环</td></tr><tr><td>额外通信量</td><td>无</td><td>少量</td><td>几乎无</td></tr><tr><td>实现复杂度</td><td>低</td><td>中</td><td>中</td></tr></tbody></table><p><strong>NB的关键优势：</strong></p><ol><li><p><strong>避免额外通信量增长</strong>：</p><ul><li>RHD在非2幂次节点时需要额外通信步骤</li><li>NHR可能产生不均匀切片导致部分通信量增加</li><li>NB通过动态调整每步数据量，最小化额外开销</li></ul></li><li><p><strong>环状结构简单</strong>：</p><ul><li>相比NHR的N棵树，NB的环状结构更直观</li><li>实现上更容易理解和调试</li></ul></li><li><p><strong>数学精确性</strong>：</p><ul><li>通过精确公式计算每步数据量</li><li>保证理论最优或接近最优的通信量</li></ul></li></ol><p><strong>适用场景：</strong></p><ol><li><strong>大规模集群</strong>：节点数任意，不受2的幂次限制</li><li><strong>通信量敏感场景</strong>：需要严格控制通信量，避免额外开销</li><li><strong>对数复杂度要求</strong>：要求⌈log₂p⌉步完成，接近理论最优</li></ol><p><strong>算法流程特点：</strong></p><ul><li><strong>逐步聚合</strong>：每一步都在前一步基础上聚合更多数据</li><li><strong>步长倍增</strong>：1→2→4→…，类似二进制展开</li><li><strong>非均匀但最优</strong>：虽然每步数据量可能不同，但总通信量接近理论下界</li></ul><p><strong>特点：</strong></p><ul><li><strong>时间复杂度：</strong> $O(\lceil \log_2 N \rceil)$</li><li><strong>关键优势：</strong><ul><li>不同节点数下均保持对数通信步数</li><li>避免额外通信数据量增长（相比RHD）</li></ul></li><li><strong>适用场景：</strong> 大规模集群，Server数量多</li></ul><h4 id="3-8-2-性能模型"><a href="#3-8-2-性能模型" class="headerlink" title="3.8.2 性能模型"></a>3.8.2 性能模型</h4><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>ReduceScatter</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>动态步长多重环，⌈log₂p⌉步，第k步传输⌊(p-1+2^k)&#x2F;2^(k+1)⌋份数据</td></tr><tr><td>AllGather</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta$</td><td>步长递增(1→2→4→…)，⌈log₂p⌉步，无额外通信量</td></tr><tr><td>AllReduce</td><td>$2\lceil \log_2 p \rceil\alpha + 2\frac{p-1}{p}n\beta + \frac{p-1}{p}n\gamma$</td><td>ReduceScatter(⌈log₂p⌉步) + AllGather(⌈log₂p⌉步)</td></tr><tr><td>Scatter</td><td>$\lceil \log_2 p \rceil\alpha + \frac{p-1}{p}n\beta$</td><td>动态步长散发，⌈log₂p⌉步完成，每步传输量不均匀</td></tr><tr><td>Broadcast</td><td>$2\lceil \log_2 p \rceil\alpha + 2\frac{p-1}{p}n\beta$</td><td>Scatter(⌈log₂p⌉步) + AllGather(⌈log₂p⌉步)实现</td></tr></tbody></table><h3 id="3-9-AHC-Asymmetric-Hierarchical-Concatenate-算法"><a href="#3-9-AHC-Asymmetric-Hierarchical-Concatenate-算法" class="headerlink" title="3.9 AHC (Asymmetric Hierarchical Concatenate) 算法"></a>3.9 AHC (Asymmetric Hierarchical Concatenate) 算法</h3><h4 id="3-9-1-算法原理"><a href="#3-9-1-算法原理" class="headerlink" title="3.9.1 算法原理"></a>3.9.1 算法原理</h4><p>层次化集合通信算法，专门处理非对称层次化网络拓扑。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;Phase 1: 组内ReduceScatter&quot;</span><br><span class="line">        A1[Group1: 2卡] --&gt; A2[并行ReduceScatter]</span><br><span class="line">        A3[Group2: 3卡] --&gt; A2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Phase 2: 构建逻辑同号卡&quot;</span><br><span class="line">        B1[数据切分: LCM×G块] --&gt; B2[Group间对应关系]</span><br><span class="line">        B2 --&gt; B3[逻辑同号卡AllReduce]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Phase 3: 组内AllGather&quot;</span><br><span class="line">        C1[Group1 AllGather] --&gt; C2[完成]</span><br><span class="line">        C3[Group2 AllGather] --&gt; C2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    A2 --&gt; B1</span><br><span class="line">    B3 --&gt; C1</span><br><span class="line">    B3 --&gt; C3</span><br><span class="line">    </span><br><span class="line">    Note1[LCM: 最小公倍数&lt;br/&gt;示例: 2和3 → LCM=6]</span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ul><li><strong>适用场景：</strong><ul><li>层次化网络拓扑</li><li>不同层次间NPU数量不对称</li><li>层次间存在带宽收敛</li></ul></li><li><strong>关键技术：</strong><ul><li>基于拓扑的计算单元重新分组</li><li>逻辑同号卡概念</li><li>非均匀数据块切分</li></ul></li></ul><h4 id="3-9-2-执行流程示例（以AllReduce为例）"><a href="#3-9-2-执行流程示例（以AllReduce为例）" class="headerlink" title="3.9.2 执行流程示例（以AllReduce为例）"></a>3.9.2 执行流程示例（以AllReduce为例）</h4><p><strong>说明：</strong> AHC算法主要应用于AllReduce、ReduceScatter等需要跨层次通信的场景，此处以AllReduce为典型示例。</p><p><strong>场景：</strong> 5个Rank，分为2组（Group1: 2卡，Group2: 3卡）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant G1R0 as Group1-Rank0</span><br><span class="line">    participant G1R1 as Group1-Rank1</span><br><span class="line">    participant G2R0 as Group2-Rank0</span><br><span class="line">    participant G2R1 as Group2-Rank1</span><br><span class="line">    participant G2R2 as Group2-Rank2</span><br><span class="line">    </span><br><span class="line">    Note over G1R0,G2R2: Step 1: 组内ReduceScatter</span><br><span class="line">    G1R0-&gt;&gt;G1R1: ReduceScatter (2卡)</span><br><span class="line">    G2R0-&gt;&gt;G2R1: ReduceScatter (3卡)</span><br><span class="line">    G2R0-&gt;&gt;G2R2: ReduceScatter (3卡)</span><br><span class="line">    </span><br><span class="line">    Note over G1R0,G2R2: Step 2: 逻辑同号卡AllReduce</span><br><span class="line">    Note right of G1R0: 数据切分为LCM(2,3)×2=12块</span><br><span class="line">    G1R0-&gt;&gt;G2R0: 对应数据块AllReduce</span><br><span class="line">    G1R1-&gt;&gt;G2R1: 对应数据块AllReduce</span><br><span class="line">    </span><br><span class="line">    Note over G1R0,G2R2: Step 3: 组内AllGather</span><br><span class="line">    G1R0-&gt;&gt;G1R1: AllGather</span><br><span class="line">    G2R0-&gt;&gt;G2R1: AllGather</span><br><span class="line">    G2R0-&gt;&gt;G2R2: AllGather</span><br></pre></td></tr></table></figure><p><strong>详细执行流程说明：</strong></p><p>AHC算法专门解决<strong>层次化非对称网络拓扑</strong>的集合通信问题。当不同层次间的计算单元数量不对称时（如Group1有2卡，Group2有3卡），传统算法无法直接应用。AHC通过<strong>逻辑同号卡</strong>概念和<strong>LCM数据切分</strong>实现高效通信。</p><p><strong>初始状态：</strong> </p><ul><li>Group1: Rank0和Rank1各持有完整数据[A,B,C,D,E,F]</li><li>Group2: Rank0、Rank1、Rank2各持有完整数据[A,B,C,D,E,F]</li><li>目标：所有5个Rank最终持有所有数据的规约结果</li></ul><p><strong>Phase 1: 组内ReduceScatter阶段</strong></p><p>每个Group内部独立执行ReduceScatter，将数据规约并分散到各个成员节点。</p><ul><li><p><strong>Group1 (2卡) 执行</strong>：</p><ul><li>使用Ring或RHD算法在G1R0和G1R1间执行ReduceScatter</li><li>G1R0持有前半部分规约结果：[A*, B*, C*]</li><li>G1R1持有后半部分规约结果：[D*, E*, F*]</li><li>注：此处的*表示Group1内部的局部规约（仅2个节点的规约）</li></ul></li><li><p><strong>Group2 (3卡) 执行</strong>：</p><ul><li>使用Ring算法在G2R0、G2R1、G2R2间执行ReduceScatter</li><li>G2R0持有第1份：[A*, B*]</li><li>G2R1持有第2份：[C*, D*]</li><li>G2R2持有第3份：[E*, F*]</li><li>注：此处的*表示Group2内部的局部规约（仅3个节点的规约）</li></ul></li><li><p><strong>阶段特点</strong>：各组内部并行执行，无跨组通信，充分利用组内高带宽链路</p></li></ul><p><strong>Phase 2: 跨组逻辑同号卡AllReduce阶段</strong></p><p>这是AHC算法的核心创新，通过<strong>LCM(Least Common Multiple)数据切分</strong>和<strong>逻辑同号卡映射</strong>实现跨组通信。</p><ul><li><p><strong>LCM切分原理</strong>：</p><ul><li>Group1有2卡，Group2有3卡，LCM(2,3) &#x3D; 6</li><li>数据被切分为 LCM × G &#x3D; 6 × 2 &#x3D; 12 个逻辑块</li><li>这样确保每个物理节点都能负责整数个逻辑块</li></ul></li><li><p><strong>逻辑同号卡映射</strong>：</p><ul><li>G1R0（逻辑编号0）对应 G2R0（逻辑编号0）：负责块0-5的规约</li><li>G1R1（逻辑编号1）对应 G2R1（逻辑编号1）：负责块6-11的规约</li><li>G2R2作为额外节点，将其数据分配给对应的逻辑同号卡处理</li></ul></li><li><p><strong>跨组AllReduce执行</strong>：</p><ul><li>G1R0 ↔ G2R0之间对对应数据块执行AllReduce（规约并交换结果）</li><li>G1R1 ↔ G2R1之间对对应数据块执行AllReduce</li><li>G2R2的数据通过环状通信或直接发送方式参与规约</li><li>执行后，逻辑同号卡持有跨组完整规约结果</li></ul></li><li><p><strong>阶段特点</strong>：跨组通信量最小化，仅在逻辑对应节点间进行，避免全连接通信</p></li></ul><p><strong>Phase 3: 组内AllGather阶段</strong></p><p>将跨组规约的结果在各组内部收集，使所有节点都持有完整的最终结果。</p><ul><li><p><strong>Group1 AllGather</strong>：</p><ul><li>G1R0和G1R1执行AllGather</li><li>将各自持有的部分规约结果交换</li><li>最终G1R0和G1R1都持有完整的全局规约结果[A*, B*, C*, D*, E*, F*]</li></ul></li><li><p><strong>Group2 AllGather</strong>：</p><ul><li>G2R0、G2R1、G2R2执行AllGather</li><li>三个节点间交换各自持有的规约结果</li><li>最终所有节点都持有完整的全局规约结果[A*, B*, C*, D*, E*, F*]</li></ul></li><li><p><strong>阶段特点</strong>：再次利用组内高带宽，快速完成数据广播，无额外规约计算</p></li></ul><p><strong>AHC算法整体特点：</strong></p><ol><li><strong>非对称拓扑适应性</strong>：通过LCM切分解决不同组卡数不一致问题</li><li><strong>三阶段分治</strong>：组内→跨组→组内，最小化高延迟跨组通信量</li><li><strong>逻辑同号卡创新</strong>：虚拟化物理节点映射，实现负载均衡</li><li><strong>带宽分层利用</strong>：组内用高带宽链路，跨组用有限带宽链路，充分适应收敛比网络</li><li><strong>复杂度权衡</strong>：虽然增加了数据切分复杂度，但显著减少了跨层通信开销，在层次化网络中性能优异</li></ol><p><strong>典型应用场景：</strong></p><ul><li>多机多卡训练（机器间带宽 &lt;&lt; 机器内带宽）</li><li>Pod内训练（Pod间带宽收敛）</li><li>边缘计算集群（不同边缘节点计算能力不对称）</li></ul><h4 id="3-9-3-性能模型"><a href="#3-9-3-性能模型" class="headerlink" title="3.9.3 性能模型"></a>3.9.3 性能模型</h4><p>采用NB算法作为组内和组间算法时：</p><p>$$<br>T_{ReduceScatter} &#x3D; 2(\lceil \log_2(m+d) \rceil + \lceil \log_2 G \rceil)\alpha + 2(\frac{m+d-1}{m+d} + \frac{(G-1)C}{Gm})n\beta + (\frac{m+d-1}{m+d} + \frac{G-1}{Gm})n\gamma<br>$$</p><p><strong>参数说明：</strong></p><ul><li>m：最小分组卡数</li><li>d：最大分组与最小分组的差值</li><li>G：分组数</li><li>C：组间带宽相对组内带宽的收敛比</li></ul><h3 id="3-10-Pipeline-算法"><a href="#3-10-Pipeline-算法" class="headerlink" title="3.10 Pipeline 算法"></a>3.10 Pipeline 算法</h3><h4 id="3-10-1-算法原理"><a href="#3-10-1-算法原理" class="headerlink" title="3.10.1 算法原理"></a>3.10.1 算法原理</h4><p>流水线并行算法，充分利用Server内和Server间链路的并发能力。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;传统分级算法&quot;</span><br><span class="line">        A1[Server间通信] --&gt; A2[链路利用率]</span><br><span class="line">        A2 --&gt; A3[Server内空闲]</span><br><span class="line">        A3 --&gt; A4[带宽浪费]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;Pipeline算法&quot;</span><br><span class="line">        B1[Server间传输] -.并发.-&gt; B2[Server内传输]</span><br><span class="line">        B2 --&gt; B3[链路充分利用]</span><br><span class="line">        B3 --&gt; B4[带宽利用率提升]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    style A4 fill:#ffcccc</span><br><span class="line">    style B4 fill:#ccffcc</span><br></pre></td></tr></table></figure><p><strong>核心思想：</strong> 挖掘通信算法的数据依赖，通过流水并行解决带宽利用不足问题。</p><h4 id="3-10-2-流水线执行示例（以AllGather为例）"><a href="#3-10-2-流水线执行示例（以AllGather为例）" class="headerlink" title="3.10.2 流水线执行示例（以AllGather为例）"></a>3.10.2 流水线执行示例（以AllGather为例）</h4><p><strong>说明：</strong> Pipeline算法主要应用于AllReduce、AllGather、ReduceScatter等大数据量场景，此处以AllGather为典型示例展示流水线并发机制。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant S0R0 as Server0-Rank0</span><br><span class="line">    participant S0R1 as Server0-Rank1</span><br><span class="line">    participant S1R2 as Server1-Rank2</span><br><span class="line">    participant S1R3 as Server1-Rank3</span><br><span class="line">    </span><br><span class="line">    Note over S0R0,S1R3: Step 1: Server间Ring + Server内传输</span><br><span class="line">    S1R2-&gt;&gt;S0R0: 绿色块 (Server间)</span><br><span class="line">    S0R0-&gt;&gt;S0R1: 绿色块 (Server内并发)</span><br><span class="line">    </span><br><span class="line">    Note over S0R0,S1R3: Step 2: 继续Ring + Server内传输</span><br><span class="line">    S0R0-&gt;&gt;S1R2: 红色块 (Server间)</span><br><span class="line">    S0R0-&gt;&gt;S0R1: 上一步接收的块 (Server内)</span><br><span class="line">    S1R2-&gt;&gt;S1R3: 上一步接收的块 (Server内)</span><br><span class="line">    </span><br><span class="line">    Note over S0R0,S1R3: 每一步都实现Server间和Server内并发</span><br></pre></td></tr></table></figure><p><strong>详细执行流程说明：</strong></p><p>Pipeline算法的核心目标是<strong>解决带宽利用率不足问题</strong>，特别是在层次化网络拓扑中（Server内高带宽 + Server间低带宽），传统分级算法会导致某一时刻只有一层链路工作，另一层链路空闲。Pipeline通过<strong>挖掘数据依赖关系</strong>，实现跨层并发，充分利用所有链路带宽。</p><p><strong>场景设置：</strong></p><ul><li>4个Rank分布在2个Server上：Server0包含Rank0和Rank1，Server1包含Rank2和Rank3</li><li>Server内带宽：高（如NVLink 600GB&#x2F;s）</li><li>Server间带宽：低（如RDMA 100Gb&#x2F;s &#x3D; 12.5GB&#x2F;s，收敛比约48:1）</li><li>初始状态：每个Rank持有不同数据块，需执行AllGather收集所有数据</li></ul><p><strong>传统分级算法的问题：</strong></p><ol><li><strong>阶段1</strong>：Server间通信时，Server内链路完全空闲</li><li><strong>阶段2</strong>：Server内通信时，Server间链路完全空闲</li><li><strong>结果</strong>：链路利用率低，总时间 &#x3D; Server间时间 + Server内时间</li></ol><p><strong>Pipeline算法的创新：</strong></p><p>通过<strong>数据依赖分析</strong>，发现：当Rank0从Rank2接收到一个数据块后，可以<strong>立即</strong>将该数据块转发给Server内的Rank1，而无需等待所有Server间通信完成。这样Server间和Server内通信可以<strong>流水线并发</strong>。</p><p><strong>Step-by-Step执行流程（AllGather示例）：</strong></p><p><strong>初始状态：</strong></p><ul><li>S0R0持有：[红色块]</li><li>S0R1持有：[蓝色块]</li><li>S1R2持有：[绿色块]</li><li>S1R3持有：[黄色块]</li><li>目标：所有Rank持有[红、蓝、绿、黄]全部数据</li></ul><p><strong>Step 1：第一轮并发传输</strong></p><ul><li><p><strong>Server间Ring通信</strong>：</p><ul><li>S1R2 → S0R0：发送绿色块（跨Server，走低带宽链路）</li><li>S0R0 → S1R2：发送红色块（跨Server）</li></ul></li><li><p><strong>Server内并发通信</strong>（与上述同时进行）：</p><ul><li>S0R0 → S0R1：发送红色块（Server内，走高带宽链路）</li><li>S1R2 → S1R3：发送绿色块（Server内）</li></ul></li><li><p><strong>Step 1后状态</strong>：</p><ul><li>S0R0：[红、绿]</li><li>S0R1：[蓝、红]</li><li>S1R2：[绿、红]</li><li>S1R3：[黄、绿]</li></ul></li><li><p><strong>并发效果</strong>：Server间传输绿色块的同时，Server内也在传输红色&#x2F;绿色块，两层链路都在工作</p></li></ul><p><strong>Step 2：第二轮并发传输</strong></p><ul><li><p><strong>Server间Ring通信</strong>：</p><ul><li>S0R0 → S1R2：发送[红、绿]中的绿色块（S0R0刚收到的）</li><li>S1R2 → S0R0：发送[绿、红]中的红色块</li><li>注意：S0R0可以立即转发上一步刚收到的绿色块，无需等待</li></ul></li><li><p><strong>Server内并发通信</strong>：</p><ul><li>S0R0 → S0R1：发送绿色块（S0R0在Step 1收到的）</li><li>S0R1 → S0R0：发送蓝色块</li><li>S1R2 → S1R3：发送红色块（S1R2在Step 1收到的）</li><li>S1R3 → S1R2：发送黄色块</li></ul></li><li><p><strong>Step 2后状态</strong>：</p><ul><li>S0R0：[红、绿、蓝]</li><li>S0R1：[蓝、红、绿]</li><li>S1R2：[绿、红、黄]</li><li>S1R3：[黄、绿、红]</li></ul></li></ul><p><strong>Step 3：第三轮并发传输</strong></p><ul><li><p><strong>Server间Ring通信</strong>：</p><ul><li>S0R0 → S1R2：发送蓝色块</li><li>S1R2 → S0R0：发送黄色块</li></ul></li><li><p><strong>Server内并发通信</strong>：</p><ul><li>S0R0 → S0R1：发送蓝色块或黄色块</li><li>S1R2 → S1R3：发送蓝色块或黄色块</li></ul></li><li><p><strong>最终状态</strong>：所有Rank持有[红、蓝、绿、黄]完整数据</p></li></ul><p><strong>Pipeline核心机制：</strong></p><ol><li><strong>数据流水</strong>：数据块像流水线一样流动，刚到达的数据立即转发，无需等待批次完成</li><li><strong>双层并发</strong>：每个时间片内，Server间和Server内链路同时传输不同数据</li><li><strong>依赖解耦</strong>：通过分析数据依赖，将”Server间完成→Server内开始”的串行依赖解耦为并发</li><li><strong>带宽充分利用</strong>：高带宽链路（Server内）和低带宽链路（Server间）同时工作，总时间约等于 max(Server间时间, Server内时间)，而非二者之和</li></ol><p><strong>性能对比（假设数据量S，Server间带宽β_inter，Server内带宽β_intra）：</strong></p><ul><li><strong>传统分级算法总时间</strong>：$T &#x3D; \frac{S}{\beta_{inter}} + \frac{S}{\beta_{intra}}$</li><li><strong>Pipeline算法总时间</strong>：$T \approx \max(\frac{S}{\beta_{inter}}, \frac{S}{\beta_{intra}})$</li><li><strong>加速比</strong>：当 β_intra &gt;&gt; β_inter 时，加速比接近 $\frac{\beta_{intra} + \beta_{inter}}{\beta_{intra}} \approx 1 + \frac{\beta_{inter}}{\beta_{intra}}$</li></ul><p>例如收敛比48:1的场景，理论加速比可达1.02倍，但实际大数据量场景下加速更明显。</p><p><strong>Pipeline算法特点：</strong></p><ol><li><strong>适用场景广泛</strong>：AllReduce、AllGather、ReduceScatter等多种原语都可应用</li><li><strong>收敛比敏感</strong>：收敛比越大（Server内外带宽差异越大），Pipeline优势越明显</li><li><strong>实现复杂度高</strong>：需要精细管理数据依赖和调度，代码复杂度较高</li><li><strong>内存开销</strong>：需要额外缓冲区存储流水线中的中间数据</li><li><strong>延迟隐藏</strong>：通过并发隐藏跨层通信延迟，特别适合大数据量场景</li></ol><p><strong>关键特性：</strong></p><ul><li><strong>AllReduce</strong>: Server间Ring(ReduceScatter+AllGather) 并发 Server内FullMesh</li><li><strong>ReduceScatter</strong>: Server间Ring 并发 Server内传输</li><li><strong>AllGather</strong>: Server间Ring 并发 Server内传输</li></ul><h4 id="3-10-3-性能模型"><a href="#3-10-3-性能模型" class="headerlink" title="3.10.3 性能模型"></a>3.10.3 性能模型</h4><table><thead><tr><th>操作</th><th>耗时公式</th><th>说明</th></tr></thead><tbody><tr><td>ReduceScatter</td><td>$\max(\frac{s}{p}\beta_{inter} + \alpha_{inter}, \frac{s}{p}\beta_{intra} + \alpha_{intra}) \times (p_{inter}-1) + \frac{s}{p}\beta_{intra} + \alpha_{intra}$</td><td>Server间Ring和Server内Mesh并发，每步取较慢者，最后补一次Server内传输</td></tr><tr><td>AllGather</td><td>$\max(\frac{s}{p}\beta_{inter} + \alpha_{inter}, \frac{s}{p}\beta_{intra} + \alpha_{intra}) \times (p_{inter}-1) + \frac{s}{p}\beta_{intra} + \alpha_{intra}$</td><td>与ReduceScatter对称，流水线式并发收集数据</td></tr><tr><td>AllReduce</td><td>$2 \times (\max(\frac{s}{p}\beta_{inter} + \alpha_{inter}, \frac{s}{p}\beta_{intra} + \alpha_{intra}) \times (p_{inter}-1) + \frac{s}{p}\beta_{intra} + \alpha_{intra})$</td><td>ReduceScatter + AllGather，两阶段流水并发充分利用带宽</td></tr></tbody></table><p><strong>参数说明：</strong></p><ul><li>s：总数据量</li><li>p：总卡数</li><li>$p_{inter}$：Server数量</li><li>$\beta_{inter}$、$\alpha_{inter}$：Server间链路参数</li><li>$\beta_{intra}$、$\alpha_{intra}$：Server内链路参数</li></ul><h3 id="3-11-算法选择策略"><a href="#3-11-算法选择策略" class="headerlink" title="3.11 算法选择策略"></a>3.11 算法选择策略</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    Start[集合通信请求] --&gt; CheckScope&#123;通信范围&#125;</span><br><span class="line">    </span><br><span class="line">    CheckScope --&gt;|Server内| IntraServer[Server内算法选择]</span><br><span class="line">    CheckScope --&gt;|Server间| InterServer[Server间算法选择]</span><br><span class="line">    </span><br><span class="line">    IntraServer --&gt; CheckSize1&#123;数据量&#125;</span><br><span class="line">    CheckSize1 --&gt;|小| Star[Star算法]</span><br><span class="line">    CheckSize1 --&gt;|大| Mesh[Mesh算法]</span><br><span class="line">    </span><br><span class="line">    InterServer --&gt; CheckOp&#123;算子类型&#125;</span><br><span class="line">    </span><br><span class="line">    CheckOp --&gt;|AllToAll系列| PairWise[PairWise算法]</span><br><span class="line">    CheckOp --&gt;|其他| CheckNodes&#123;节点数&#125;</span><br><span class="line">    </span><br><span class="line">    CheckNodes --&gt;|小规模| Ring[Ring算法]</span><br><span class="line">    CheckNodes --&gt;|大规模| CheckPower&#123;是否2的幂&#125;</span><br><span class="line">    </span><br><span class="line">    CheckPower --&gt;|是| RHD[RHD算法]</span><br><span class="line">    CheckPower --&gt;|否| CheckData&#123;数据量&#125;</span><br><span class="line">    </span><br><span class="line">    CheckData --&gt;|小| RHD</span><br><span class="line">    CheckData --&gt;|大| NHR_NB&#123;NHR或NB&#125;</span><br><span class="line">    </span><br><span class="line">    CheckNodes --&gt;|层次化&lt;br/&gt;非对称| AHC[AHC算法]</span><br><span class="line">    </span><br><span class="line">    CheckSize1 --&gt;|大数据量&lt;br/&gt;多机多卡| Pipeline[Pipeline算法]</span><br><span class="line">    </span><br><span class="line">    style Star fill:#ffe6e6</span><br><span class="line">    style Mesh fill:#ffe6e6</span><br><span class="line">    style Ring fill:#e6f3ff</span><br><span class="line">    style RHD fill:#e6f3ff</span><br><span class="line">    style PairWise fill:#fff4e6</span><br><span class="line">    style NHR_NB fill:#e6ffe6</span><br><span class="line">    style AHC fill:#f3e6ff</span><br><span class="line">    style Pipeline fill:#ffffcc</span><br></pre></td></tr></table></figure><h2 id="4-集合通信原语"><a href="#4-集合通信原语" class="headerlink" title="4. 集合通信原语"></a>4. 集合通信原语</h2><h3 id="4-1-核心原语列表"><a href="#4-1-核心原语列表" class="headerlink" title="4.1 核心原语列表"></a>4.1 核心原语列表</h3><table><thead><tr><th>原语</th><th>描述</th><th>主要算法</th></tr></thead><tbody><tr><td><strong>AllReduce</strong></td><td>所有节点规约后广播结果</td><td>Mesh, Ring, RHD, NHR, NB, AHC, Pipeline</td></tr><tr><td><strong>AllGather</strong></td><td>所有节点收集全部数据</td><td>Mesh, Ring, RHD, NHR, NB, Pipeline</td></tr><tr><td><strong>ReduceScatter</strong></td><td>规约后散发到各节点</td><td>Mesh, Ring, RHD, NHR, NB, AHC, Pipeline</td></tr><tr><td><strong>Broadcast</strong></td><td>根节点向所有节点广播</td><td>Mesh, Ring, RHD, NHR, NB, Star</td></tr><tr><td><strong>Reduce</strong></td><td>所有节点向根节点规约</td><td>Mesh, Ring, RHD, Star</td></tr><tr><td><strong>Scatter</strong></td><td>根节点散发数据到各节点</td><td>Mesh, Ring, NHR, NB, Star</td></tr><tr><td><strong>Gather</strong></td><td>所有节点向根节点收集</td><td>Mesh, Ring, Star</td></tr><tr><td><strong>AllToAll</strong></td><td>所有节点间全交换</td><td>PairWise</td></tr><tr><td><strong>AllToAllV</strong></td><td>所有节点间变长全交换</td><td>PairWise</td></tr></tbody></table><h3 id="4-2-原语语义说明"><a href="#4-2-原语语义说明" class="headerlink" title="4.2 原语语义说明"></a>4.2 原语语义说明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph &quot;AllReduce&quot;</span><br><span class="line">        AR1[R0:A0] --&gt; ARS[Sum]</span><br><span class="line">        AR2[R1:A1] --&gt; ARS</span><br><span class="line">        AR3[R2:A2] --&gt; ARS</span><br><span class="line">        ARS --&gt; ARR1[R0:Sum]</span><br><span class="line">        ARS --&gt; ARR2[R1:Sum]</span><br><span class="line">        ARS --&gt; ARR3[R2:Sum]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;AllGather&quot;</span><br><span class="line">        AG1[R0:A0] --&gt; AGR1[R0:A0+A1+A2]</span><br><span class="line">        AG2[R1:A1] --&gt; AGR2[R1:A0+A1+A2]</span><br><span class="line">        AG3[R2:A2] --&gt; AGR3[R2:A0+A1+A2]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;ReduceScatter&quot;</span><br><span class="line">        RS1[R0:A0] --&gt; RSS[Sum切分]</span><br><span class="line">        RS2[R1:A1] --&gt; RSS</span><br><span class="line">        RS3[R2:A2] --&gt; RSS</span><br><span class="line">        RSS --&gt; RSR1[R0:Sum_part0]</span><br><span class="line">        RSS --&gt; RSR2[R1:Sum_part1]</span><br><span class="line">        RSS --&gt; RSR3[R2:Sum_part2]</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><h2 id="5-通信框架设计"><a href="#5-通信框架设计" class="headerlink" title="5. 通信框架设计"></a>5. 通信框架设计</h2><h3 id="5-1-通信域管理"><a href="#5-1-通信域管理" class="headerlink" title="5.1 通信域管理"></a>5.1 通信域管理</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">    class Communicator &#123;</span><br><span class="line">        -int commId</span><br><span class="line">        -int rank</span><br><span class="line">        -int size</span><br><span class="line">        -CommunicatorType type</span><br><span class="line">        +Create()</span><br><span class="line">        +Destroy()</span><br><span class="line">        +GetRank()</span><br><span class="line">        +GetSize()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class CommunicatorGroup &#123;</span><br><span class="line">        -List~Communicator~ comms</span><br><span class="line">        +CreateGroup()</span><br><span class="line">        +SplitGroup()</span><br><span class="line">        +FreeGroup()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class RankInfo &#123;</span><br><span class="line">        -int rankId</span><br><span class="line">        -int deviceId</span><br><span class="line">        -string hostName</span><br><span class="line">        -NetworkInfo netInfo</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    Communicator &quot;1&quot; --&gt; &quot;*&quot; RankInfo</span><br><span class="line">    CommunicatorGroup &quot;1&quot; --&gt; &quot;*&quot; Communicator</span><br></pre></td></tr></table></figure><h3 id="5-2-算子执行流程"><a href="#5-2-算子执行流程" class="headerlink" title="5.2 算子执行流程"></a>5.2 算子执行流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant User as 用户调用</span><br><span class="line">    participant Framework as 通信框架</span><br><span class="line">    participant AlgoSelector as 算法选择器</span><br><span class="line">    participant Algorithm as 通信算法</span><br><span class="line">    participant Platform as 通信平台</span><br><span class="line">    </span><br><span class="line">    User-&gt;&gt;Framework: HcclAllReduce(...)</span><br><span class="line">    Framework-&gt;&gt;Framework: 参数校验</span><br><span class="line">    Framework-&gt;&gt;AlgoSelector: 请求算法选择</span><br><span class="line">    </span><br><span class="line">    AlgoSelector-&gt;&gt;AlgoSelector: 分析通信域信息</span><br><span class="line">    AlgoSelector-&gt;&gt;AlgoSelector: 评估数据量</span><br><span class="line">    AlgoSelector-&gt;&gt;AlgoSelector: 应用选择策略</span><br><span class="line">    AlgoSelector--&gt;&gt;Framework: 返回算法类型</span><br><span class="line">    </span><br><span class="line">    Framework-&gt;&gt;Algorithm: 执行算法</span><br><span class="line">    Algorithm-&gt;&gt;Algorithm: 计算资源需求</span><br><span class="line">    Algorithm-&gt;&gt;Algorithm: 生成任务编排</span><br><span class="line">    </span><br><span class="line">    Algorithm-&gt;&gt;Platform: 申请资源</span><br><span class="line">    Platform--&gt;&gt;Algorithm: 返回资源句柄</span><br><span class="line">    </span><br><span class="line">    Algorithm-&gt;&gt;Platform: 下发任务</span><br><span class="line">    Platform-&gt;&gt;Platform: 执行通信</span><br><span class="line">    Platform--&gt;&gt;Algorithm: 返回执行结果</span><br><span class="line">    </span><br><span class="line">    Algorithm--&gt;&gt;Framework: 算法执行完成</span><br><span class="line">    Framework--&gt;&gt;User: 返回结果</span><br></pre></td></tr></table></figure><h3 id="5-3-算法选择器设计"><a href="#5-3-算法选择器设计" class="headerlink" title="5.3 算法选择器设计"></a>5.3 算法选择器设计</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Input[输入参数] --&gt; Analyzer[参数分析器]</span><br><span class="line">    </span><br><span class="line">    Analyzer --&gt; Topo[拓扑信息分析]</span><br><span class="line">    Analyzer --&gt; Data[数据量分析]</span><br><span class="line">    Analyzer --&gt; Op[算子类型分析]</span><br><span class="line">    </span><br><span class="line">    Topo --&gt; Rules[选择规则库]</span><br><span class="line">    Data --&gt; Rules</span><br><span class="line">    Op --&gt; Rules</span><br><span class="line">    </span><br><span class="line">    Rules --&gt; Model[性能模型评估]</span><br><span class="line">    Model --&gt; Decision[决策引擎]</span><br><span class="line">    </span><br><span class="line">    Decision --&gt; Output[输出算法]</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;规则库&quot;</span><br><span class="line">        R1[拓扑规则]</span><br><span class="line">        R2[数据量规则]</span><br><span class="line">        R3[算子规则]</span><br><span class="line">        R4[历史优化]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Rules -.包含.-&gt; R1</span><br><span class="line">    Rules -.包含.-&gt; R2</span><br><span class="line">    Rules -.包含.-&gt; R3</span><br><span class="line">    Rules -.包含.-&gt; R4</span><br></pre></td></tr></table></figure><h2 id="6-编译与构建系统"><a href="#6-编译与构建系统" class="headerlink" title="6. 编译与构建系统"></a>6. 编译与构建系统</h2><h3 id="6-1-编译流程"><a href="#6-1-编译流程" class="headerlink" title="6.1 编译流程"></a>6.1 编译流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Start[开始编译] --&gt; CheckEnv&#123;检查环境&#125;</span><br><span class="line">    </span><br><span class="line">    CheckEnv --&gt;|CANN已安装| CheckDeps[检查依赖]</span><br><span class="line">    CheckEnv --&gt;|未安装| Error1[报错退出]</span><br><span class="line">    </span><br><span class="line">    CheckDeps --&gt; CMake[CMake配置]</span><br><span class="line">    CMake --&gt; Gen[生成构建文件]</span><br><span class="line">    </span><br><span class="line">    Gen --&gt; CompileKernel&#123;编译模式&#125;</span><br><span class="line">    </span><br><span class="line">    CompileKernel --&gt;|--aicpu| Kernel[编译ccl_kernel.so]</span><br><span class="line">    CompileKernel --&gt;|默认| Full[完整编译]</span><br><span class="line">    </span><br><span class="line">    Kernel --&gt; InstallK[安装Kernel]</span><br><span class="line">    Full --&gt; CompileFramework[编译通信框架]</span><br><span class="line">    CompileFramework --&gt; CompileAlgo[编译通信算法]</span><br><span class="line">    CompileAlgo --&gt; Package[打包.run文件]</span><br><span class="line">    </span><br><span class="line">    Package --&gt; Output[生成输出]</span><br><span class="line">    InstallK --&gt; Output</span><br><span class="line">    </span><br><span class="line">    Output --&gt; End[编译完成]</span><br></pre></td></tr></table></figure><h3 id="6-2-主要编译选项"><a href="#6-2-主要编译选项" class="headerlink" title="6.2 主要编译选项"></a>6.2 主要编译选项</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础编译</span></span><br><span class="line">bash build.sh --nlohmann_path /path/to/nlohmann/include</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅编译AICPU Kernel</span></span><br><span class="line">bash build.sh --nlohmann_path /path --aicpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译并运行测试</span></span><br><span class="line">bash build.sh --nlohmann_path /path --<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使能地址消毒器（用于内存检测）</span></span><br><span class="line">bash build.sh --nlohmann_path /path --asan</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使能代码覆盖率</span></span><br><span class="line">bash build.sh --nlohmann_path /path --cov</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定CANN包路径</span></span><br><span class="line">bash build.sh --nlohmann_path /path -p /usr/local/Ascend/ascend-toolkit/latest</span><br></pre></td></tr></table></figure><h3 id="6-3-依赖关系"><a href="#6-3-依赖关系" class="headerlink" title="6.3 依赖关系"></a>6.3 依赖关系</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    HCCL[HCCL库] --&gt; CANN[CANN开发套件包]</span><br><span class="line">    HCCL --&gt; SDK[CANN SDK包]</span><br><span class="line">    HCCL --&gt; JSON[nlohmann/json]</span><br><span class="line">    </span><br><span class="line">    CANN --&gt; Runtime[CANN Runtime]</span><br><span class="line">    CANN --&gt; Driver[NPU驱动]</span><br><span class="line">    CANN --&gt; Firmware[NPU固件]</span><br><span class="line">    </span><br><span class="line">    SDK --&gt; GTest[Google Test]</span><br><span class="line">    </span><br><span class="line">    Build[构建系统] --&gt; Python[Python ≥ 3.7]</span><br><span class="line">    Build --&gt; GCC[GCC ≥ 7.3]</span><br><span class="line">    Build --&gt; CMake[CMake ≥ 3.16]</span><br><span class="line">    </span><br><span class="line">    style HCCL fill:#ff6b6b</span><br><span class="line">    style CANN fill:#4ecdc4</span><br><span class="line">    style SDK fill:#95e1d3</span><br><span class="line">    style JSON fill:#f38181</span><br></pre></td></tr></table></figure><h2 id="7-测试与验证"><a href="#7-测试与验证" class="headerlink" title="7. 测试与验证"></a>7. 测试与验证</h2><h3 id="7-1-测试分层架构"><a href="#7-1-测试分层架构" class="headerlink" title="7.1 测试分层架构"></a>7.1 测试分层架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;单元测试层&quot;</span><br><span class="line">        UT1[算法单元测试]</span><br><span class="line">        UT2[框架单元测试]</span><br><span class="line">        UT3[工具类单元测试]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;集成测试层&quot;</span><br><span class="line">        IT1[算子集成测试]</span><br><span class="line">        IT2[多算法联合测试]</span><br><span class="line">        IT3[异常场景测试]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;系统测试层&quot;</span><br><span class="line">        ST1[单机多卡测试]</span><br><span class="line">        ST2[多机多卡测试]</span><br><span class="line">        ST3[性能基准测试]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;工具测试层&quot;</span><br><span class="line">        TT1[HCCL Test工具]</span><br><span class="line">        TT2[性能分析工具]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    UT1 --&gt; IT1</span><br><span class="line">    UT2 --&gt; IT2</span><br><span class="line">    UT3 --&gt; IT3</span><br><span class="line">    </span><br><span class="line">    IT1 --&gt; ST1</span><br><span class="line">    IT2 --&gt; ST2</span><br><span class="line">    IT3 --&gt; ST3</span><br><span class="line">    </span><br><span class="line">    ST1 --&gt; TT1</span><br><span class="line">    ST2 --&gt; TT1</span><br><span class="line">    ST3 --&gt; TT2</span><br></pre></td></tr></table></figure><h3 id="7-2-HCCL-Test-工具"><a href="#7-2-HCCL-Test-工具" class="headerlink" title="7.2 HCCL Test 工具"></a>7.2 HCCL Test 工具</h3><p><strong>功能测试示例：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 8卡AllReduce功能测试</span></span><br><span class="line">mpirun -n 8 ./bin/all_reduce_test -b 8K -e 64M -f 2 -d fp32 -o <span class="built_in">sum</span> -p 8</span><br></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><code>-b</code>: 起始数据量（8KB）</li><li><code>-e</code>: 结束数据量（64MB）</li><li><code>-f</code>: 增量系数（每次2倍）</li><li><code>-d</code>: 数据类型（fp32）</li><li><code>-o</code>: 规约操作（sum）</li><li><code>-p</code>: 参与NPU数量（8）</li></ul><p><strong>输出指标：</strong></p><ul><li><code>check_result</code>: 功能正确性（success&#x2F;fail）</li><li><code>aveg_time</code>: 平均执行时间（微秒）</li><li><code>alg_bandwidth</code>: 算法带宽（GB&#x2F;s）</li><li><code>data_size</code>: 单NPU数据量（Bytes）</li></ul><h3 id="7-3-LLT测试命令"><a href="#7-3-LLT测试命令" class="headerlink" title="7.3 LLT测试命令"></a>7.3 LLT测试命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行所有LLT测试</span></span><br><span class="line">sh build.sh --nlohmann_path /path/to/nlohmann/include --<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行特定测试套件</span></span><br><span class="line">sh build.sh --nlohmann_path /path --open_hccl_test</span><br><span class="line">sh build.sh --nlohmann_path /path --executor_hccl_test</span><br><span class="line">sh build.sh --nlohmann_path /path --executor_reduce_hccl_test</span><br><span class="line">sh build.sh --nlohmann_path /path --executor_pipeline_hccl_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使能内存检测</span></span><br><span class="line">sh build.sh --nlohmann_path /path --<span class="built_in">test</span> --asan</span><br></pre></td></tr></table></figure><h2 id="8-性能优化策略"><a href="#8-性能优化策略" class="headerlink" title="8. 性能优化策略"></a>8. 性能优化策略</h2><h3 id="8-1-算法级优化"><a href="#8-1-算法级优化" class="headerlink" title="8.1 算法级优化"></a>8.1 算法级优化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mindmap</span><br><span class="line">  root((性能优化))</span><br><span class="line">    算法选择</span><br><span class="line">      拓扑感知</span><br><span class="line">      数据量自适应</span><br><span class="line">      负载均衡</span><br><span class="line">    内存优化</span><br><span class="line">      零拷贝技术</span><br><span class="line">      内存池管理</span><br><span class="line">      DMA直接访问</span><br><span class="line">    并发优化</span><br><span class="line">      流水线并行</span><br><span class="line">      多流并发</span><br><span class="line">      异步执行</span><br><span class="line">    网络优化</span><br><span class="line">      拥塞控制</span><br><span class="line">      流量调度</span><br><span class="line">      QoS保证</span><br></pre></td></tr></table></figure><h3 id="8-2-关键性能指标"><a href="#8-2-关键性能指标" class="headerlink" title="8.2 关键性能指标"></a>8.2 关键性能指标</h3><table><thead><tr><th>指标</th><th>说明</th><th>目标</th></tr></thead><tbody><tr><td><strong>带宽利用率</strong></td><td>实际带宽&#x2F;理论带宽</td><td>&gt; 90%</td></tr><tr><td><strong>通信延迟</strong></td><td>端到端通信时间</td><td>最小化</td></tr><tr><td><strong>可扩展性</strong></td><td>节点数增加时的性能保持</td><td>接近线性</td></tr><tr><td><strong>负载均衡</strong></td><td>各节点负载方差</td><td>&lt; 10%</td></tr><tr><td><strong>内存开销</strong></td><td>额外内存消耗</td><td>&lt; 20%</td></tr></tbody></table><h3 id="8-3-性能调优参数"><a href="#8-3-性能调优参数" class="headerlink" title="8.3 性能调优参数"></a>8.3 性能调优参数</h3><p><strong>环境变量：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 算法选择策略</span></span><br><span class="line"><span class="built_in">export</span> HCCL_ALGO=&lt;algo_name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 流水线深度</span></span><br><span class="line"><span class="built_in">export</span> HCCL_PIPELINE_DEPTH=&lt;depth&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并发流数量</span></span><br><span class="line"><span class="built_in">export</span> HCCL_STREAM_NUM=&lt;num&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志级别</span></span><br><span class="line"><span class="built_in">export</span> ASCEND_SLOG_PRINT_TO_STDOUT=1</span><br><span class="line"><span class="built_in">export</span> ASCEND_GLOBAL_LOG_LEVEL=&lt;level&gt;</span><br></pre></td></tr></table></figure><h2 id="9-安全与可靠性"><a href="#9-安全与可靠性" class="headerlink" title="9. 安全与可靠性"></a>9. 安全与可靠性</h2><h3 id="9-1-安全措施"><a href="#9-1-安全措施" class="headerlink" title="9.1 安全措施"></a>9.1 安全措施</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph &quot;编译安全&quot;</span><br><span class="line">        S1[栈保护 -fstack-protector]</span><br><span class="line">        S2[位置无关代码 -fPIC]</span><br><span class="line">        S3[RELRO保护]</span><br><span class="line">        S4[NX保护]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;运行安全&quot;</span><br><span class="line">        R1[输入参数校验]</span><br><span class="line">        R2[内存边界检查]</span><br><span class="line">        R3[资源泄漏检测]</span><br><span class="line">        R4[异常捕获与处理]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;通信安全&quot;</span><br><span class="line">        C1[端口认证]</span><br><span class="line">        C2[数据完整性校验]</span><br><span class="line">        C3[访问控制]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    S1 --&gt; Build[构建产物]</span><br><span class="line">    S2 --&gt; Build</span><br><span class="line">    S3 --&gt; Build</span><br><span class="line">    S4 --&gt; Build</span><br><span class="line">    </span><br><span class="line">    Build --&gt; Deploy[部署]</span><br><span class="line">    </span><br><span class="line">    R1 --&gt; Runtime[运行时]</span><br><span class="line">    R2 --&gt; Runtime</span><br><span class="line">    R3 --&gt; Runtime</span><br><span class="line">    R4 --&gt; Runtime</span><br><span class="line">    </span><br><span class="line">    Deploy --&gt; Runtime</span><br><span class="line">    </span><br><span class="line">    C1 --&gt; Comm[通信层]</span><br><span class="line">    C2 --&gt; Comm</span><br><span class="line">    C3 --&gt; Comm</span><br><span class="line">    </span><br><span class="line">    Runtime --&gt; Comm</span><br></pre></td></tr></table></figure><h3 id="9-2-错误处理机制"><a href="#9-2-错误处理机制" class="headerlink" title="9.2 错误处理机制"></a>9.2 错误处理机制</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">stateDiagram-v2</span><br><span class="line">    [*] --&gt; Normal: 初始化</span><br><span class="line">    Normal --&gt; DetectError: 检测到错误</span><br><span class="line">    DetectError --&gt; Classify: 分类错误</span><br><span class="line">    </span><br><span class="line">    Classify --&gt; Recoverable: 可恢复错误</span><br><span class="line">    Classify --&gt; Fatal: 致命错误</span><br><span class="line">    </span><br><span class="line">    Recoverable --&gt; Retry: 重试机制</span><br><span class="line">    Retry --&gt; Success: 重试成功</span><br><span class="line">    Retry --&gt; Fallback: 重试失败</span><br><span class="line">    Fallback --&gt; Alternative: 切换备用方案</span><br><span class="line">    Alternative --&gt; Normal</span><br><span class="line">    </span><br><span class="line">    Success --&gt; Normal</span><br><span class="line">    </span><br><span class="line">    Fatal --&gt; Log: 记录日志</span><br><span class="line">    Log --&gt; Cleanup: 资源清理</span><br><span class="line">    Cleanup --&gt; [*]: 退出</span><br></pre></td></tr></table></figure><h3 id="9-3-日志与调试"><a href="#9-3-日志与调试" class="headerlink" title="9.3 日志与调试"></a>9.3 日志与调试</h3><p><strong>日志级别：</strong></p><ul><li>ERROR: 错误信息</li><li>WARNING: 警告信息</li><li>INFO: 一般信息</li><li>DEBUG: 调试信息</li></ul><p><strong>关键日志点：</strong></p><ol><li>通信域创建&#x2F;销毁</li><li>算法选择决策</li><li>资源申请&#x2F;释放</li><li>任务执行状态</li><li>性能统计信息</li></ol><h2 id="10-版本管理与兼容性"><a href="#10-版本管理与兼容性" class="headerlink" title="10. 版本管理与兼容性"></a>10. 版本管理与兼容性</h2><h3 id="10-1-版本策略"><a href="#10-1-版本策略" class="headerlink" title="10.1 版本策略"></a>10.1 版本策略</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">timeline</span><br><span class="line">    title HCCL版本演进</span><br><span class="line">    section CANN 8.x</span><br><span class="line">        标签v8.0.0 : 基础算法 : Mesh/Ring/RHD</span><br><span class="line">        标签v8.0.1 : 新增PairWise/Star</span><br><span class="line">    section CANN 9.x</span><br><span class="line">        标签v9.0.0 : 新增NHR/NB</span><br><span class="line">        标签v9.0.1 : 新增AHC</span><br><span class="line">    section CANN 10.x</span><br><span class="line">        标签v10.0.0 : Pipeline优化</span><br><span class="line">        标签v10.0.1 : 性能优化</span><br></pre></td></tr></table></figure><h3 id="10-2-兼容性矩阵"><a href="#10-2-兼容性矩阵" class="headerlink" title="10.2 兼容性矩阵"></a>10.2 兼容性矩阵</h3><table><thead><tr><th>HCCL版本</th><th>CANN版本</th><th>固件版本</th><th>支持硬件</th></tr></thead><tbody><tr><td>v8.0.x</td><td>8.0.x</td><td>对应CANN</td><td>Atlas 800&#x2F;900</td></tr><tr><td>v9.0.x</td><td>9.0.x</td><td>对应CANN</td><td>Atlas 800&#x2F;900</td></tr><tr><td>v10.0.x</td><td>10.0.x</td><td>对应CANN</td><td>Atlas 800&#x2F;900&#x2F;新硬件</td></tr></tbody></table><h3 id="10-3-升级与回滚"><a href="#10-3-升级与回滚" class="headerlink" title="10.3 升级与回滚"></a>10.3 升级与回滚</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装自定义HCCL包</span></span><br><span class="line">./CANN-hccl_alg-&lt;version&gt;-linux.&lt;<span class="built_in">arch</span>&gt;.run</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回滚到上一个版本</span></span><br><span class="line">./CANN-hccl_alg-&lt;version&gt;-linux.&lt;<span class="built_in">arch</span>&gt;.run --rollback</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 回滚仅支持回退到上一次安装的版本状态。</p><h2 id="11-扩展与定制开发"><a href="#11-扩展与定制开发" class="headerlink" title="11. 扩展与定制开发"></a>11. 扩展与定制开发</h2><h3 id="11-1-新算法集成流程"><a href="#11-1-新算法集成流程" class="headerlink" title="11.1 新算法集成流程"></a>11.1 新算法集成流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    Start[开始] --&gt; Design[算法设计]</span><br><span class="line">    Design --&gt; Implement[实现算法类]</span><br><span class="line">    </span><br><span class="line">    Implement --&gt; Interface[实现算法接口]</span><br><span class="line">    Interface --&gt; Resource[资源计算逻辑]</span><br><span class="line">    Resource --&gt; Schedule[任务编排逻辑]</span><br><span class="line">    </span><br><span class="line">    Schedule --&gt; Register[注册到框架]</span><br><span class="line">    Register --&gt; Selector[更新选择器规则]</span><br><span class="line">    </span><br><span class="line">    Selector --&gt; UnitTest[单元测试]</span><br><span class="line">    UnitTest --&gt; IntegTest[集成测试]</span><br><span class="line">    IntegTest --&gt; PerfTest[性能测试]</span><br><span class="line">    </span><br><span class="line">    PerfTest --&gt; Doc[文档编写]</span><br><span class="line">    Doc --&gt; End[完成]</span><br></pre></td></tr></table></figure><h3 id="11-2-算法接口规范"><a href="#11-2-算法接口规范" class="headerlink" title="11.2 算法接口规范"></a>11.2 算法接口规范</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 伪代码示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CollectiveAlgorithm</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 初始化算法</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Status <span class="title">Init</span><span class="params">(<span class="type">const</span> AlgorithmConfig&amp; config)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 计算资源需求</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Status <span class="title">CalculateResource</span><span class="params">(ResourceInfo&amp; resource)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 生成任务编排</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Status <span class="title">GenerateTaskSchedule</span><span class="params">(TaskSchedule&amp; schedule)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 执行算法</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Status <span class="title">Execute</span><span class="params">(<span class="type">const</span> ExecuteContext&amp; context)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 清理资源</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> Status <span class="title">Cleanup</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="11-3-贡献指南"><a href="#11-3-贡献指南" class="headerlink" title="11.3 贡献指南"></a>11.3 贡献指南</h3><ol><li><strong>Issue讨论：</strong> 新特性需先通过Issue讨论方案</li><li><strong>CLA签署：</strong> 首次贡献需签署CLA协议</li><li><strong>代码规范：</strong> 遵循项目代码规范</li><li><strong>测试覆盖：</strong> 提供完整的单元测试和集成测试</li><li><strong>文档更新：</strong> 同步更新相关文档</li><li><strong>PR模板：</strong> 按模板填写PR信息</li></ol><h2 id="12-参考资料"><a href="#12-参考资料" class="headerlink" title="12. 参考资料"></a>12. 参考资料</h2><h3 id="12-1-官方文档"><a href="#12-1-官方文档" class="headerlink" title="12.1 官方文档"></a>12.1 官方文档</h3><ul><li><a href="https://hiascend.com/document/redirect/CannCommunityHcclUg">集合通信用户指南</a></li><li><a href="https://cann-doc.obs.cn-north-4.myhuaweicloud.com/hccl/">集合通信源码定制开发指南</a></li><li><a href="https://hiascend.com/document/redirect/CannCommunityEnvRef">环境变量参考</a></li><li><a href="https://hiascend.com/document/redirect/CannCommunityToolHcclTest">HCCL性能测试工具用户指南</a></li></ul><h3 id="12-2-技术文章"><a href="#12-2-技术文章" class="headerlink" title="12.2 技术文章"></a>12.2 技术文章</h3><ul><li><a href="https://www.hiascend.com/zh/developer/techArticles/20240809-1">HCCL—昇腾高性能集合通信库简介</a></li><li><a href="https://www.hiascend.com/zh/developer/techArticles/20240903-1">HCCL集合通信算法开发Hello World示例</a></li><li><a href="https://www.hiascend.com/zh/developer/techArticles/20240930-1">HCCL集合通信常见问题定位思路</a></li><li><a href="https://www.hiascend.com/zh/developer/techArticles/20241111-1">深度学习的分布式训练与集合通信（一）</a></li><li><a href="https://www.hiascend.com/zh/developer/techArticles/20241122-1">深度学习的分布式训练与集合通信（二）</a></li></ul><h3 id="12-3-培训视频"><a href="#12-3-培训视频" class="headerlink" title="12.3 培训视频"></a>12.3 培训视频</h3><ul><li><a href="https://www.bilibili.com/video/BV1MMmWYkEfS/">昇腾集合通信系列教程——什么是HCCL</a></li><li><a href="https://www.bilibili.com/video/BV1YtUWYQEq2/">昇腾集合通信系列教程——常见集合通信原语</a></li><li><a href="https://www.bilibili.com/video/BV1XEz5YeE3A/">昇腾集合通信系列教程——集合通信典型算法</a></li><li><a href="https://www.bilibili.com/video/BV1w4sVeFEsB/">HCCL设计原理和实现系列</a></li></ul><h3 id="12-4-性能基准"><a href="#12-4-性能基准" class="headerlink" title="12.4 性能基准"></a>12.4 性能基准</h3><p><strong>典型场景性能（以AllReduce为例）：</strong></p><table><thead><tr><th>节点数</th><th>数据量</th><th>算法</th><th>带宽利用率</th><th>延迟</th></tr></thead><tbody><tr><td>8 (单机)</td><td>1GB</td><td>Mesh</td><td>&gt;95%</td><td>&lt;1ms</td></tr><tr><td>16 (2机)</td><td>1GB</td><td>Pipeline</td><td>&gt;90%</td><td>&lt;2ms</td></tr><tr><td>64 (8机)</td><td>1GB</td><td>NHR</td><td>&gt;85%</td><td>&lt;5ms</td></tr><tr><td>128 (16机)</td><td>1GB</td><td>NB</td><td>&gt;80%</td><td>&lt;10ms</td></tr></tbody></table><h2 id="13-总结"><a href="#13-总结" class="headerlink" title="13. 总结"></a>13. 总结</h2><h3 id="13-1-核心优势"><a href="#13-1-核心优势" class="headerlink" title="13.1 核心优势"></a>13.1 核心优势</h3><ol><li><strong>丰富的算法库</strong>：9种算法覆盖各种场景</li><li><strong>智能算法选择</strong>：基于α-β模型的性能评估</li><li><strong>层次化设计</strong>：清晰的三层架构</li><li><strong>高性能实现</strong>：充分利用硬件特性</li><li><strong>开放可扩展</strong>：支持自定义算法开发</li></ol><h3 id="13-2-应用场景"><a href="#13-2-应用场景" class="headerlink" title="13.2 应用场景"></a>13.2 应用场景</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mindmap</span><br><span class="line">  root((HCCL应用))</span><br><span class="line">    深度学习</span><br><span class="line">      分布式训练</span><br><span class="line">      数据并行</span><br><span class="line">      模型并行</span><br><span class="line">      流水线并行</span><br><span class="line">    高性能计算</span><br><span class="line">      科学计算</span><br><span class="line">      大规模仿真</span><br><span class="line">      图计算</span><br><span class="line">    大数据</span><br><span class="line">      分布式处理</span><br><span class="line">      MapReduce</span><br><span class="line">      Spark集成</span><br></pre></td></tr></table></figure><h3 id="13-3-未来展望"><a href="#13-3-未来展望" class="headerlink" title="13.3 未来展望"></a>13.3 未来展望</h3><ul><li><strong>算法优化</strong>：持续优化现有算法性能</li><li><strong>新算法引入</strong>：引入更多先进算法</li><li><strong>智能调度</strong>：基于AI的算法选择</li><li><strong>异构支持</strong>：支持更多硬件平台</li><li><strong>生态建设</strong>：与更多框架深度集成</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 系统架构分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HCCL </tag>
            
            <tag> 集合通信 </tag>
            
            <tag> 华为 </tag>
            
            <tag> 昇腾 </tag>
            
            <tag> AllReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepEP 架构分析</title>
      <link href="/2025/11/17/DeepEP_DeepDive/"/>
      <url>/2025/11/17/DeepEP_DeepDive/</url>
      
        <content type="html"><![CDATA[<h1 id="DeepEP-架构分析文档"><a href="#DeepEP-架构分析文档" class="headerlink" title="DeepEP 架构分析文档"></a>DeepEP 架构分析文档</h1><h2 id="1-项目概述"><a href="#1-项目概述" class="headerlink" title="1. 项目概述"></a>1. 项目概述</h2><p><strong>DeepEP</strong> 是一个专为混合专家模型(Mixture-of-Experts, MoE)和专家并行(Expert Parallelism, EP)设计的高性能通信库。它提供了高吞吐量和低延迟的All-to-All GPU内核，专门优化了MoE模型中的dispatch和combine操作。</p><p>该项目由DeepSeek团队开发，是支撑DeepSeek-V3大规模MoE训练和推理的核心基础设施。</p><h3 id="核心特性"><a href="#核心特性" class="headerlink" title="核心特性"></a>核心特性</h3><ul><li><strong>高吞吐量内核</strong>: 支持NVLink域到RDMA域的非对称带宽转发</li><li><strong>低延迟内核</strong>: 主要使用RDMA通信，适用于推理解码任务（可配置是否使用NVLink加速）</li><li><strong>多精度支持</strong>: 支持FP8、BF16等低精度操作</li><li><strong>通信计算重叠</strong>: 基于hook的方法，不占用SM资源</li><li><strong>可扩展性</strong>: 支持节点内(NVLink)和节点间(RDMA)通信</li></ul><h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><p><strong>节点内通信</strong> (H800, ~160 GB&#x2F;s NVLink):</p><ul><li>8个EP ranks: ~153-158 GB&#x2F;s</li></ul><p><strong>节点间通信</strong> (H800 + CX7 IB 400Gb&#x2F;s, ~50 GB&#x2F;s RDMA):</p><ul><li>16-64个EP ranks: ~43-58 GB&#x2F;s</li></ul><p><strong>低延迟模式</strong>:</p><ul><li>8 EP ranks: 77-114 us</li><li>256 EP ranks: 194-360 us</li></ul><h3 id="依赖的关键技术"><a href="#依赖的关键技术" class="headerlink" title="依赖的关键技术"></a>依赖的关键技术</h3><p>DeepEP 构建在多个先进的GPU通信技术之上：</p><h4 id="1-NVSHMEM-NVIDIA-Symmetric-Memory"><a href="#1-NVSHMEM-NVIDIA-Symmetric-Memory" class="headerlink" title="1. NVSHMEM (NVIDIA Symmetric Memory)"></a>1. <strong>NVSHMEM (NVIDIA Symmetric Memory)</strong></h4><p>NVSHMEM 是 NVIDIA 提供的分布式GPU内存访问库，实现了 OpenSHMEM 标准的 GPU 扩展。</p><p><strong>核心能力</strong>:</p><ul><li><strong>对称内存模型</strong>: 所有GPU可以直接访问彼此的显存，无需CPU参与</li><li><strong>单边通信</strong>: 支持 PUT&#x2F;GET 操作，发起方可以直接读写远端GPU内存</li><li><strong>集合通信</strong>: 提供 barrier、broadcast、reduction 等原语</li><li><strong>多传输支持</strong>: 同时支持 NVLink (节点内) 和 InfiniBand RDMA (节点间)</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>节点间数据传输的底层实现</li><li>低延迟模式的核心通信机制</li><li>RDMA buffer 的对称内存管理</li></ul><h4 id="2-IBGDA-InfiniBand-GPU-Direct-Async"><a href="#2-IBGDA-InfiniBand-GPU-Direct-Async" class="headerlink" title="2. IBGDA (InfiniBand GPU Direct Async)"></a>2. <strong>IBGDA (InfiniBand GPU Direct Async)</strong></h4><p>IBGDA 是 NVIDIA 与 Mellanox 合作开发的技术，允许 GPU 直接发起 RDMA 操作。</p><p><strong>核心能力</strong>:</p><ul><li><strong>零CPU开销</strong>: GPU 可以直接操作 InfiniBand HCA (Host Channel Adapter)</li><li><strong>多QP并行</strong>: 支持每个GPU使用多个Queue Pair同时传输</li><li><strong>低延迟</strong>: 绕过CPU，减少PCIe往返延迟</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>节点间dispatch&#x2F;combine的高性能数据传输</li><li>低延迟模式的快速RDMA操作</li><li>多QP并行以提升带宽利用率</li></ul><h4 id="3-NVLink"><a href="#3-NVLink" class="headerlink" title="3. NVLink"></a>3. <strong>NVLink</strong></h4><p>NVIDIA 的高速GPU互联技术，提供节点内GPU之间的直接连接。</p><p><strong>核心能力</strong>:</p><ul><li><strong>高带宽</strong>: H800单向带宽 ~160 GB&#x2F;s per GPU</li><li><strong>低延迟</strong>: 比PCIe延迟低一个数量级</li><li><strong>Peer-to-Peer</strong>: GPU之间可以直接访问彼此的显存</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>节点内 dispatch&#x2F;combine 的主要传输路径</li><li>节点间通信的本地聚合&#x2F;分发</li><li>低延迟模式的可选加速路径</li></ul><h4 id="4-CUDA-IPC-Inter-Process-Communication"><a href="#4-CUDA-IPC-Inter-Process-Communication" class="headerlink" title="4. CUDA IPC (Inter-Process Communication)"></a>4. <strong>CUDA IPC (Inter-Process Communication)</strong></h4><p>CUDA提供的进程间共享GPU内存机制。</p><p><strong>核心能力</strong>:</p><ul><li><strong>内存共享</strong>: 不同进程可以访问同一块GPU内存</li><li><strong>零拷贝</strong>: 通过句柄 (handle) 映射，避免数据复制</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>节点内多进程的buffer共享</li><li>Barrier信号的共享内存实现</li></ul><h4 id="5-CUDA-Fabric-API-可选"><a href="#5-CUDA-Fabric-API-可选" class="headerlink" title="5. CUDA Fabric API (可选)"></a>5. <strong>CUDA Fabric API (可选)</strong></h4><p>新一代GPU内存管理API，支持更灵活的内存访问模式。</p><p><strong>核心能力</strong>:</p><ul><li><strong>统一寻址</strong>: 提供跨GPU的统一虚拟地址空间</li><li><strong>细粒度控制</strong>: 更好的内存访问权限管理</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>作为CUDA IPC的替代方案</li><li>支持更大规模的GPU集群</li></ul><h4 id="6-TMA-Tensor-Memory-Accelerator-SM90"><a href="#6-TMA-Tensor-Memory-Accelerator-SM90" class="headerlink" title="6. TMA (Tensor Memory Accelerator, SM90+)"></a>6. <strong>TMA (Tensor Memory Accelerator, SM90+)</strong></h4><p>Hopper架构引入的硬件加速内存拷贝单元。</p><p><strong>核心能力</strong>:</p><ul><li><strong>硬件加速</strong>: 专用硬件单元处理张量拷贝</li><li><strong>高带宽</strong>: 更高效地利用显存带宽</li><li><strong>异步执行</strong>: 不占用SM计算资源</li></ul><p><strong>在DeepEP中的应用</strong>:</p><ul><li>H100&#x2F;H800上的向量化数据传输</li><li>高吞吐dispatch&#x2F;combine优化</li></ul><hr><h2 id="2-系统架构"><a href="#2-系统架构" class="headerlink" title="2. 系统架构"></a>2. 系统架构</h2><h3 id="2-1-整体架构图"><a href="#2-1-整体架构图" class="headerlink" title="2.1 整体架构图"></a>2.1 整体架构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph PythonAPI[&quot; Python API 层 &quot;]</span><br><span class="line">        Buffer[&quot;Buffer&quot;]</span><br><span class="line">        EventOverlap[&quot;EventOverlap&quot;]</span><br><span class="line">        Config[&quot;Config&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph CppRuntime[&quot; C++ Runtime 层 &quot;]</span><br><span class="line">        BufferMgmt[&quot;Buffer管理&quot;]</span><br><span class="line">        MemAlloc[&quot;内存分配&quot;]</span><br><span class="line">        ProcSync[&quot;进程同步&quot;]</span><br><span class="line">        EventMgmt[&quot;事件管理&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph CUDAKernel[&quot; CUDA Kernel 层 &quot;]</span><br><span class="line">        direction LR</span><br><span class="line">        Intranode[&quot;Intranode&lt;br/&gt;节点内通信&quot;]</span><br><span class="line">        Internode[&quot;Internode&lt;br/&gt;节点间通信&quot;]</span><br><span class="line">        LowLatency[&quot;Internode LL&lt;br/&gt;低延迟模式&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Hardware[&quot; 硬件通信层 &quot;]</span><br><span class="line">        direction LR</span><br><span class="line">        NVLink[&quot;NVLink&quot;]</span><br><span class="line">        NVSHMEM[&quot;NVSHMEM&quot;]</span><br><span class="line">        IBGDA[&quot;IBGDA&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Buffer -.&quot;PyBind11&quot;.-&gt; BufferMgmt</span><br><span class="line">    EventOverlap -.&quot;PyBind11&quot;.-&gt; EventMgmt</span><br><span class="line">    Config -.&quot;PyBind11&quot;.-&gt; BufferMgmt</span><br><span class="line">    </span><br><span class="line">    BufferMgmt --&gt; Intranode</span><br><span class="line">    MemAlloc --&gt; Internode</span><br><span class="line">    ProcSync --&gt; LowLatency</span><br><span class="line">    EventMgmt --&gt; Intranode</span><br><span class="line">    </span><br><span class="line">    Intranode --&gt; NVLink</span><br><span class="line">    Internode --&gt; NVSHMEM</span><br><span class="line">    LowLatency --&gt; IBGDA</span><br><span class="line">    </span><br><span class="line">    style PythonAPI fill:#4fc3f7</span><br><span class="line">    style CppRuntime fill:#ffb74d</span><br><span class="line">    style CUDAKernel fill:#ba68c8</span><br><span class="line">    style Hardware fill:#81c784</span><br></pre></td></tr></table></figure><h3 id="2-2-数据流架构"><a href="#2-2-数据流架构" class="headerlink" title="2.2 数据流架构"></a>2.2 数据流架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    Start[&quot;输入张量 X&lt;br/&gt;[num_tokens, hidden]&quot;]</span><br><span class="line">    TopK[&quot;topk_idx&lt;br/&gt;[num_tokens, num_topk]&quot;]</span><br><span class="line">    </span><br><span class="line">    subgraph Layout[&quot; 1. Layout 计算 &quot;]</span><br><span class="line">        L1[&quot;计算 token→rank 映射&quot;]</span><br><span class="line">        L2[&quot;生成 prefix sum&quot;]</span><br><span class="line">        L3[&quot;统计 token 数量&quot;]</span><br><span class="line">        L1 --&gt; L2 --&gt; L3</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Dispatch[&quot; 2. Dispatch 阶段 &quot;]</span><br><span class="line">        D0[&quot;All-to-All Scatter&quot;]</span><br><span class="line">        D1[&quot;传输 hidden data&quot;]</span><br><span class="line">        D2[&quot;传输 scales/metadata&quot;]</span><br><span class="line">        D3[&quot;传输 topk info&quot;]</span><br><span class="line">        D0 --&gt; D1 --&gt; D2 --&gt; D3</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Expert[&quot; 3. Expert 计算 &quot;]</span><br><span class="line">        E1[&quot;各 rank 处理&lt;br/&gt;分配的 experts&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Combine[&quot; 4. Combine 阶段 &quot;]</span><br><span class="line">        C1[&quot;All-to-All Gather&quot;]</span><br><span class="line">        C2[&quot;按 metadata 路由&quot;]</span><br><span class="line">        C3[&quot;应用 topk_weights&quot;]</span><br><span class="line">        C4[&quot;可选 bias 加法&quot;]</span><br><span class="line">        C1 --&gt; C2 --&gt; C3 --&gt; C4</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    End[&quot;输出张量 Y&lt;br/&gt;[num_tokens, hidden]&quot;]</span><br><span class="line">    </span><br><span class="line">    Start --&gt; Layout</span><br><span class="line">    TopK --&gt; Layout</span><br><span class="line">    Layout --&gt; Dispatch</span><br><span class="line">    Dispatch --&gt; Expert</span><br><span class="line">    Expert --&gt; Combine</span><br><span class="line">    Combine --&gt; End</span><br><span class="line">    </span><br><span class="line">    style Start fill:#4fc3f7</span><br><span class="line">    style TopK fill:#4fc3f7</span><br><span class="line">    style Layout fill:#ffb74d</span><br><span class="line">    style Dispatch fill:#ba68c8</span><br><span class="line">    style Expert fill:#81c784</span><br><span class="line">    style Combine fill:#f06292</span><br><span class="line">    style End fill:#4fc3f7</span><br></pre></td></tr></table></figure><hr><h2 id="3-核心模块详解"><a href="#3-核心模块详解" class="headerlink" title="3. 核心模块详解"></a>3. 核心模块详解</h2><h3 id="3-1-Buffer-管理模块"><a href="#3-1-Buffer-管理模块" class="headerlink" title="3.1 Buffer 管理模块"></a>3.1 Buffer 管理模块</h3><p><strong>位置</strong>: <code>deep_ep/buffer.py</code> + <code>csrc/deep_ep.hpp/cpp</code></p><h4 id="核心职责"><a href="#核心职责" class="headerlink" title="核心职责"></a>核心职责</h4><ol><li><p><strong>内存管理</strong></p><ul><li>NVLink Buffer: 节点内通信缓冲区</li><li>RDMA Buffer: 节点间通信缓冲区 (通过NVSHMEM)</li><li>支持 Fabric API (GPU Direct Storage)</li></ul></li><li><p><strong>进程同步</strong></p><ul><li>IPC Handle 同步 (CUDA IPC &#x2F; Fabric)</li><li>NVSHMEM 初始化和 Unique ID 交换</li><li>Barrier 机制</li></ul></li><li><p><strong>通信流管理</strong></p><ul><li>独立的 communication stream</li><li>事件同步机制</li><li>计算通信重叠支持</li></ul></li></ol><h4 id="关键数据结构"><a href="#关键数据结构" class="headerlink" title="关键数据结构"></a>关键数据结构</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Buffer</span> &#123;</span><br><span class="line">    <span class="comment">// 缓冲区指针</span></span><br><span class="line">    <span class="type">void</span>* buffer_ptrs[NUM_MAX_NVL_PEERS];     <span class="comment">// NVLink buffers</span></span><br><span class="line">    <span class="type">void</span>* rdma_buffer_ptr;                     <span class="comment">// NVSHMEM buffer</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 同步信号</span></span><br><span class="line">    <span class="type">int</span>* barrier_signal_ptrs[NUM_MAX_NVL_PEERS];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 接收计数器 (CPU-GPU 通信)</span></span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span>* moe_recv_counter;</span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span>* moe_recv_expert_counter;</span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span>* moe_recv_rdma_counter;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 拓扑信息</span></span><br><span class="line">    <span class="type">int</span> rank, rdma_rank, nvl_rank;</span><br><span class="line">    <span class="type">int</span> num_ranks, num_rdma_ranks, num_nvl_ranks;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 低延迟模式特定</span></span><br><span class="line">    <span class="type">bool</span> low_latency_mode;</span><br><span class="line">    <span class="type">int</span> low_latency_buffer_idx;  <span class="comment">// 双缓冲</span></span><br><span class="line">    <span class="type">int</span>* mask_buffer_ptr;         <span class="comment">// 动态 rank 屏蔽</span></span><br><span class="line">    <span class="type">int</span>* sync_buffer_ptr;         <span class="comment">// 自定义 barrier</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="初始化流程"><a href="#初始化流程" class="headerlink" title="初始化流程"></a>初始化流程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 创建 Buffer 对象</span></span><br><span class="line">buffer = deep_ep.Buffer(</span><br><span class="line">    group=dist_group,</span><br><span class="line">    num_nvl_bytes=nvl_size,</span><br><span class="line">    num_rdma_bytes=rdma_size,</span><br><span class="line">    low_latency_mode=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 自动执行同步</span></span><br><span class="line"><span class="comment"># - 交换 device IDs</span></span><br><span class="line"><span class="comment"># - 交换 IPC handles</span></span><br><span class="line"><span class="comment"># - 初始化 NVSHMEM (如果需要)</span></span><br><span class="line"><span class="comment"># - 设置 IBGDA QP 数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Buffer 可用</span></span><br><span class="line"><span class="keyword">assert</span> buffer.runtime.is_available()</span><br></pre></td></tr></table></figure><hr><h3 id="3-2-Intranode-Kernels-节点内通信"><a href="#3-2-Intranode-Kernels-节点内通信" class="headerlink" title="3.2 Intranode Kernels (节点内通信)"></a>3.2 Intranode Kernels (节点内通信)</h3><p><strong>位置</strong>: <code>csrc/kernels/intranode.cu</code></p><h4 id="核心原理"><a href="#核心原理" class="headerlink" title="核心原理"></a>核心原理</h4><p><strong>通信模式</strong>: NVLink peer-to-peer 直接内存访问</p><p><strong>Dispatch 流程</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph Phase1[&quot; 阶段1: notify_dispatch &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        SM0[&quot;SM 0: 同步和元数据&quot;]</span><br><span class="line">        SM0_1[&quot;执行 barrier&quot;]</span><br><span class="line">        SM0_2[&quot;统计 tokens&quot;]</span><br><span class="line">        SM0_3[&quot;计算 prefix sum&quot;]</span><br><span class="line">        </span><br><span class="line">        SMN[&quot;SM 1-N: channel 分布&quot;]</span><br><span class="line">        SMN_1[&quot;处理目标 rank&quot;]</span><br><span class="line">        SMN_2[&quot;计算 prefix matrix&quot;]</span><br><span class="line">        </span><br><span class="line">        SM0 --&gt; SM0_1 --&gt; SM0_2 --&gt; SM0_3</span><br><span class="line">        SMN --&gt; SMN_1 --&gt; SMN_2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Phase2[&quot; 阶段2: dispatch 数据传输 &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        D1[&quot;每个 SM 负责一个 rank&quot;]</span><br><span class="line">        D2[&quot;channel 划分负载&quot;]</span><br><span class="line">        D3[&quot;NVLink 写入对端&quot;]</span><br><span class="line">        D4[&quot;在线类型转换&quot;]</span><br><span class="line">        </span><br><span class="line">        D1 --&gt; D2 --&gt; D3 --&gt; D4</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Phase1 ==&gt; Phase2</span><br><span class="line">    </span><br><span class="line">    style Phase1 fill:#4fc3f7</span><br><span class="line">    style Phase2 fill:#ba68c8</span><br></pre></td></tr></table></figure><h4 id="关键优化技术"><a href="#关键优化技术" class="headerlink" title="关键优化技术"></a>关键优化技术</h4><ol><li><p><strong>Channel 并行</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 将 tokens 分成多个 channel，每个 channel 独立处理</span><br><span class="line">int token_start_idx, token_end_idx;</span><br><span class="line">get_channel_task_range(num_tokens, num_channels, </span><br><span class="line">                       channel_id, token_start_idx, token_end_idx);</span><br></pre></td></tr></table></figure></li><li><p><strong>Barrier 优化</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">template &lt;int kNumRanks, bool init&gt;</span><br><span class="line">__device__ void barrier_block(int** barrier_signal_ptrs, int rank) &#123;</span><br><span class="line">    // 使用 GPU 内存的原子操作实现快速 barrier</span><br><span class="line">    // 避免 CPU 参与</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>对齐和向量化</strong></p><ul><li>数据对齐到 128 bytes</li><li>使用 int4 向量加载&#x2F;存储</li><li>TMA (Tensor Memory Accelerator) 支持 (SM90+)</li></ul></li></ol><h4 id="Combine-流程"><a href="#Combine-流程" class="headerlink" title="Combine 流程"></a>Combine 流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 从多个源 rank 收集数据</span><br><span class="line">2. 按照 src_idx 排序重组</span><br><span class="line">3. 应用 topk_weights 加权</span><br><span class="line">4. 累加到输出张量 (使用 atomicAdd)</span><br><span class="line">5. 可选添加 bias</span><br></pre></td></tr></table></figure><hr><h3 id="3-3-Internode-Kernels-节点间通信"><a href="#3-3-Internode-Kernels-节点间通信" class="headerlink" title="3.3 Internode Kernels (节点间通信)"></a>3.3 Internode Kernels (节点间通信)</h3><p><strong>位置</strong>: <code>csrc/kernels/internode.cu</code></p><h4 id="核心原理-1"><a href="#核心原理-1" class="headerlink" title="核心原理"></a>核心原理</h4><p><strong>通信模式</strong>: NVSHMEM + NVLink 混合</p><ul><li><strong>RDMA 通信</strong>: 跨节点数据传输</li><li><strong>NVLink 通信</strong>: 节点内聚合&#x2F;分发</li></ul><h4 id="拓扑结构"><a href="#拓扑结构" class="headerlink" title="拓扑结构"></a>拓扑结构</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    subgraph RDMA0[&quot; RDMA Rank 0 (节点0) &quot;]</span><br><span class="line">        direction LR</span><br><span class="line">        N00[&quot;GPU0&quot;]</span><br><span class="line">        N01[&quot;GPU1&quot;]</span><br><span class="line">        N02[&quot;GPU2&quot;]</span><br><span class="line">        N0X[&quot;...&quot;]  </span><br><span class="line">        N07[&quot;GPU7&quot;]</span><br><span class="line">        </span><br><span class="line">        N00 -.&quot;NVLink&quot;.-&gt; N01 -.&quot;NVLink&quot;.-&gt; N02 -.&quot;NVLink&quot;.-&gt; N0X -.&quot;NVLink&quot;.-&gt; N07</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph RDMA1[&quot; RDMA Rank 1 (节点1) &quot;]</span><br><span class="line">        direction LR</span><br><span class="line">        N10[&quot;GPU0&quot;]</span><br><span class="line">        N11[&quot;GPU1&quot;]</span><br><span class="line">        N12[&quot;GPU2&quot;]</span><br><span class="line">        N1X[&quot;...&quot;]</span><br><span class="line">        N17[&quot;GPU7&quot;]</span><br><span class="line">        </span><br><span class="line">        N10 -.&quot;NVLink&quot;.-&gt; N11 -.&quot;NVLink&quot;.-&gt; N12 -.&quot;NVLink&quot;.-&gt; N1X -.&quot;NVLink&quot;.-&gt; N17</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    RDMA0 &lt;==&quot; RDMA IB &quot;===&gt; RDMA1</span><br><span class="line">    </span><br><span class="line">    style RDMA0 fill:#4fc3f7</span><br><span class="line">    style RDMA1 fill:#ffb74d</span><br></pre></td></tr></table></figure><h4 id="Dispatch-流程"><a href="#Dispatch-流程" class="headerlink" title="Dispatch 流程"></a>Dispatch 流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph Stage1[&quot; 阶段1: RDMA 元数据交换 &quot;]</span><br><span class="line">        S1_1[&quot;收集本地 GPU 统计&quot;]</span><br><span class="line">        S1_2[&quot;NVSHMEM PUT 广播&quot;]</span><br><span class="line">        S1_3[&quot;同步等待&quot;]</span><br><span class="line">        S1_1 --&gt; S1_2 --&gt; S1_3</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Stage2[&quot; 阶段2: RDMA 数据传输 &quot;]</span><br><span class="line">        S2_1[&quot;读取 NVLink buffer&quot;]</span><br><span class="line">        S2_2[&quot;FP8 量化(可选)&quot;]</span><br><span class="line">        S2_3[&quot;NVSHMEM PUT 远端&quot;]</span><br><span class="line">        S2_4[&quot;IBGDA 多 QP 加速&quot;]</span><br><span class="line">        S2_1 --&gt; S2_2 --&gt; S2_3 --&gt; S2_4</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Stage3[&quot; 阶段3: NVLink 本地分发 &quot;]</span><br><span class="line">        S3_1[&quot;读取 RDMA buffer&quot;]</span><br><span class="line">        S3_2[&quot;分发到本地 ranks&quot;]</span><br><span class="line">        S3_3[&quot;写入接收 buffer&quot;]</span><br><span class="line">        S3_1 --&gt; S3_2 --&gt; S3_3</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Stage1 ==&gt; Stage2</span><br><span class="line">    Stage2 ==&gt; Stage3</span><br><span class="line">    </span><br><span class="line">    style Stage1 fill:#81c784</span><br><span class="line">    style Stage2 fill:#ffb74d</span><br><span class="line">    style Stage3 fill:#ba68c8</span><br></pre></td></tr></table></figure><h4 id="SourceMeta-编码"><a href="#SourceMeta-编码" class="headerlink" title="SourceMeta 编码"></a>SourceMeta 编码</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">SourceMeta</span> &#123;</span><br><span class="line">    <span class="type">int</span> src_rdma_rank;                  <span class="comment">// 源 RDMA rank</span></span><br><span class="line">    <span class="type">int</span> is_token_in_nvl_rank_bits;      <span class="comment">// 8-bit mask for 8 NVL ranks</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 编码示例: token 来自 RDMA rank 2 的 GPU 0, 3, 5</span></span><br><span class="line">    <span class="comment">// src_rdma_rank = 2</span></span><br><span class="line">    <span class="comment">// is_token_in_nvl_rank_bits = 0b00101001</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这个元数据在 combine 阶段用于路由数据返回正确的 GPU。</p><h4 id="关键优化"><a href="#关键优化" class="headerlink" title="关键优化"></a>关键优化</h4><ol><li><p><strong>IBGDA (InfiniBand GPU Direct Async)</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvshmemi_ibgda_put_nbi_warp&lt;true&gt;(</span><br><span class="line">    dst_addr, src_addr, size, dst_rank, qp_id, lane_id, 0</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li>直接从 GPU 发起 RDMA 操作</li><li>多 QP 并行传输</li><li>Warp 级别的协作</li></ul></li><li><p><strong>双层 Buffer</strong></p><ul><li>RDMA buffer: 节点间数据缓冲</li><li>NVLink buffer: 节点内数据分发</li></ul></li><li><p><strong>动态 Channel 分配</strong></p><ul><li>根据负载动态分配 channel</li><li>平衡 NVLink 和 RDMA 带宽</li></ul></li></ol><hr><h3 id="3-4-Low-Latency-Kernels-低延迟内核"><a href="#3-4-Low-Latency-Kernels-低延迟内核" class="headerlink" title="3.4 Low-Latency Kernels (低延迟内核)"></a>3.4 Low-Latency Kernels (低延迟内核)</h3><p><strong>位置</strong>: <code>csrc/kernels/internode_ll.cu</code></p><h4 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h4><p>专为<strong>推理解码</strong>场景优化:</p><ul><li>小 batch size (通常 128-512 tokens)</li><li>极低延迟要求 (&lt; 200 us)</li><li>主要使用 RDMA 通信，在允许的配置下也会利用 NVLink 加速</li><li>支持通过环境变量 <code>NVSHMEM_DISABLE_P2P</code> 控制是否使用 NVLink</li><li>主要使用 RDMA 通信，在允许的配置下也会利用 NVLink 加速</li><li>支持通过环境变量 <code>NVSHMEM_DISABLE_P2P</code> 控制是否使用 NVLink</li></ul><h4 id="核心特性-1"><a href="#核心特性-1" class="headerlink" title="核心特性"></a>核心特性</h4><ol><li><p><strong>双缓冲机制</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> low_latency_buffer_idx;  <span class="comment">// 0 or 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// dispatch 使用 buffer 0，combine 使用 buffer 1</span></span><br><span class="line"><span class="comment">// 下一次迭代切换</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Hook-based 通信计算重叠</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dispatch 返回 recv_hook</span></span><br><span class="line">recv_x, handle, event, recv_hook = buffer.low_latency_dispatch(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 expert 计算前调用 hook</span></span><br><span class="line"><span class="keyword">if</span> recv_hook:</span><br><span class="line">    recv_hook()  <span class="comment"># 轮询 RDMA 接收完成</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># expert 计算</span></span><br><span class="line">output = expert_forward(recv_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># combine (同样有 hook)</span></span><br><span class="line">result, event, recv_hook = buffer.low_latency_combine(...)</span><br></pre></td></tr></table></figure></li><li><p><strong>动态 Rank 屏蔽</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 用于容错: 如果某个 rank 超时，动态屏蔽它</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">low_latency_update_mask_buffer</span><span class="params">(<span class="type">int</span> rank_to_mask, <span class="type">bool</span> mask)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier 实现会跳过被屏蔽的 rank</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">is_rank_masked</span>(mask_buffer_ptr, dst_rank)) &#123;</span><br><span class="line">    <span class="keyword">continue</span>;  <span class="comment">// 跳过这个 rank</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Dispatch-流程-1"><a href="#Dispatch-流程-1" class="headerlink" title="Dispatch 流程"></a>Dispatch 流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph SendPhase[&quot; 发送阶段 LOW_LATENCY_SEND &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        Send1[&quot;读取 topk_idx&quot;]</span><br><span class="line">        Send2[&quot;按 expert 分组&quot;]</span><br><span class="line">        Send3[&quot;FP8 量化(可选)&quot;]</span><br><span class="line">        Send4[&quot;RDMA PUT 固定 buffer&quot;]</span><br><span class="line">        Send5[&quot;更新 atomic counter&quot;]</span><br><span class="line">        </span><br><span class="line">        Send1 --&gt; Send2 --&gt; Send3 --&gt; Send4 --&gt; Send5</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph RecvPhase[&quot; 接收阶段 LOW_LATENCY_RECV &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        Recv1[&quot;轮询 atomic counter&quot;]</span><br><span class="line">        Recv2[&quot;等待预期值&quot;]</span><br><span class="line">        Recv3[&quot;收集统计(可选)&quot;]</span><br><span class="line">        </span><br><span class="line">        Recv1 --&gt; Recv2 --&gt; Recv3</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    SendPhase ==&gt; RecvPhase</span><br><span class="line">    </span><br><span class="line">    style SendPhase fill:#4fc3f7</span><br><span class="line">    style RecvPhase fill:#81c784</span><br></pre></td></tr></table></figure><h4 id="Combine-流程-1"><a href="#Combine-流程-1" class="headerlink" title="Combine 流程"></a>Combine 流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">类似 dispatch，但数据流反向:</span><br><span class="line">1. Expert 输出写入 RDMA buffer</span><br><span class="line">2. 根据 src_info 路由回源 token</span><br><span class="line">3. 应用 topk_weights</span><br><span class="line">4. 可选零拷贝模式 (直接写入输出张量)</span><br></pre></td></tr></table></figure><h4 id="关键优化-1"><a href="#关键优化-1" class="headerlink" title="关键优化"></a>关键优化</h4><ol><li><p><strong>预分配固定大小 Buffer</strong></p><ul><li>每个 rank 为每个 expert 预分配固定空间</li><li>避免动态内存分配和复杂的地址计算</li></ul></li><li><p><strong>Warp-Group 并行</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 每个 expert 由一个 warp-group (多个 warp) 处理</span><br><span class="line">const auto warp_group_id = warp_id / num_warps_per_group;</span><br><span class="line">const auto responsible_expert_idx = sm_id * num_warp_groups + warp_group_id;</span><br></pre></td></tr></table></figure></li><li><p><strong>轮询 vs 中断</strong></p><ul><li>使用主动轮询 (polling) 而非中断</li><li>更低的延迟，代价是持续占用 CPU&#x2F;GPU</li></ul></li><li><p><strong>超时和容错</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">auto start_time = clock64();</span><br><span class="line">uint64_t wait_recv_cost = 0;</span><br><span class="line">while (condition &amp;&amp; </span><br><span class="line">       (wait_recv_cost = clock64() - start_time) &lt;= NUM_TIMEOUT_CYCLES) &#123;</span><br><span class="line">    // polling</span><br><span class="line">&#125;</span><br><span class="line">if (wait_recv_cost &gt; NUM_TIMEOUT_CYCLES) &#123;</span><br><span class="line">    // 超时处理: 屏蔽该 rank</span><br><span class="line">    atomicExch(mask_buffer_ptr + dst_rank, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="3-5-Layout-计算模块"><a href="#3-5-Layout-计算模块" class="headerlink" title="3.5 Layout 计算模块"></a>3.5 Layout 计算模块</h3><p><strong>位置</strong>: <code>csrc/kernels/layout.cu</code></p><h4 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h4><p>在 dispatch 之前计算路由信息:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_tokens_per_rank,      <span class="comment"># [num_ranks] 每个 rank 接收的 token 数</span></span><br><span class="line">num_tokens_per_rdma_rank, <span class="comment"># [num_rdma_ranks] 每个 RDMA rank 接收数</span></span><br><span class="line">num_tokens_per_expert,    <span class="comment"># [num_experts] 每个 expert 接收数</span></span><br><span class="line">is_token_in_rank,         <span class="comment"># [num_tokens, num_ranks] bool 矩阵</span></span><br><span class="line">layout_event              <span class="comment"># CUDA event</span></span><br><span class="line">= buffer.get_dispatch_layout(topk_idx, num_experts)</span><br></pre></td></tr></table></figure><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    Input[&quot;输入: topk_idx&lt;br/&gt;[num_tokens, num_topk]&quot;]</span><br><span class="line">    </span><br><span class="line">    subgraph Step1[&quot; 步骤1: 扫描 top-k experts &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        S1_1[&quot;遍历所有 tokens&quot;]</span><br><span class="line">        S1_2[&quot;获取 expert_id&quot;]</span><br><span class="line">        S1_3[&quot;计算目标 rank&quot;]</span><br><span class="line">        S1_4[&quot;标记 is_token_in_rank&quot;]</span><br><span class="line">        S1_1 --&gt; S1_2 --&gt; S1_3 --&gt; S1_4</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Step2[&quot; 步骤2: 统计 token 数量 &quot;]</span><br><span class="line">        direction TB</span><br><span class="line">        S2_1[&quot;sum per rank&quot;]</span><br><span class="line">        S2_2[&quot;count per expert&quot;]</span><br><span class="line">        S2_1 --&gt; S2_2</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph Step3[&quot; 步骤3: 对齐处理 &quot;]</span><br><span class="line">        S3_1[&quot;align_up to&lt;br/&gt;expert_alignment&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    Input --&gt; Step1</span><br><span class="line">    Step1 --&gt; Step2</span><br><span class="line">    Step2 --&gt; Step3</span><br><span class="line">    </span><br><span class="line">    style Input fill:#4fc3f7</span><br><span class="line">    style Step1 fill:#ffb74d</span><br><span class="line">    style Step2 fill:#ba68c8</span><br><span class="line">    style Step3 fill:#81c784</span><br></pre></td></tr></table></figure><h4 id="优化技术"><a href="#优化技术" class="headerlink" title="优化技术"></a>优化技术</h4><ul><li>Warp-level reduction</li><li>Shared memory 聚合</li><li>Coalesced memory access</li></ul><hr><h3 id="3-6-事件和同步机制"><a href="#3-6-事件和同步机制" class="headerlink" title="3.6 事件和同步机制"></a>3.6 事件和同步机制</h3><p><strong>位置</strong>: <code>csrc/event.hpp</code> + <code>deep_ep/utils.py</code></p><h4 id="EventOverlap-类"><a href="#EventOverlap-类" class="headerlink" title="EventOverlap 类"></a>EventOverlap 类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EventOverlap</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通信计算重叠的便利封装&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, event: EventHandle</span>):</span><br><span class="line">        <span class="variable language_">self</span>.event = event</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">current_stream_wait</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;当前流等待事件完成&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.event.current_stream_wait()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 支持 with 语法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, ...</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.event <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.event.current_stream_wait()</span><br></pre></td></tr></table></figure><h4 id="使用模式"><a href="#使用模式" class="headerlink" title="使用模式"></a>使用模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模式 1: 手动同步</span></span><br><span class="line">event = buffer.dispatch(...)</span><br><span class="line"><span class="comment"># ... 其他计算 ...</span></span><br><span class="line">event.current_stream_wait()  <span class="comment"># 等待 dispatch 完成</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模式 2: with 语法</span></span><br><span class="line">event = buffer.dispatch(...)</span><br><span class="line"><span class="keyword">with</span> event:</span><br><span class="line">    <span class="comment"># 这里的计算与 dispatch 重叠</span></span><br><span class="line">    expert_computation()</span><br><span class="line"><span class="comment"># 退出 with 时自动等待</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模式 3: async 模式</span></span><br><span class="line">event = buffer.dispatch(..., async_mode=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># dispatch 在独立 stream 上执行</span></span><br><span class="line"><span class="comment"># 主流继续运行</span></span><br></pre></td></tr></table></figure><hr><h2 id="4-关键技术细节"><a href="#4-关键技术细节" class="headerlink" title="4. 关键技术细节"></a>4. 关键技术细节</h2><h3 id="4-1-FP8-量化"><a href="#4-1-FP8-量化" class="headerlink" title="4.1 FP8 量化"></a>4.1 FP8 量化</h3><p>DeepEP 支持两种 FP8 格式:</p><ul><li><strong>E4M3</strong>: 常规 FP8 (4-bit exponent, 3-bit mantissa)</li><li><strong>UE8M0</strong>: 特殊格式 (无符号 8-bit 整数作为 scale)</li></ul><h4 id="Per-Token-量化"><a href="#Per-Token-量化" class="headerlink" title="Per-Token 量化"></a>Per-Token 量化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// dispatch 时量化</span><br><span class="line">scale = max(abs(token)) / 448.0  // E4M3 max</span><br><span class="line">quantized_token = token / scale</span><br><span class="line"></span><br><span class="line">// combine 时反量化</span><br><span class="line">dequantized_token = quantized_token * scale</span><br></pre></td></tr></table></figure><h4 id="Scale-存储优化"><a href="#Scale-存储优化" class="headerlink" title="Scale 存储优化"></a>Scale 存储优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 每 128 个元素一个 scale (channel-wise)</span><br><span class="line">constexpr int kNumPerChannels = 128;</span><br><span class="line">int num_scales = hidden / kNumPerChannels;</span><br><span class="line"></span><br><span class="line">// UE8M0: 将 float scale 编码为 uint8</span><br><span class="line">uint8_t encode_ue8m0(float scale);</span><br><span class="line">float decode_ue8m0(uint8_t encoded_scale);</span><br></pre></td></tr></table></figure><h3 id="4-2-IBGDA-InfiniBand-GPU-Direct-Async"><a href="#4-2-IBGDA-InfiniBand-GPU-Direct-Async" class="headerlink" title="4.2 IBGDA (InfiniBand GPU Direct Async)"></a>4.2 IBGDA (InfiniBand GPU Direct Async)</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>IBGDA 允许 GPU 直接发起 RDMA 操作，无需 CPU 参与:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">传统 NVSHMEM:</span><br><span class="line">GPU → PCIe → CPU → IB HCA → Network</span><br><span class="line">                ↓</span><br><span class="line">           RDMA 操作排队</span><br><span class="line"></span><br><span class="line">IBGDA:</span><br><span class="line">GPU → Direct Access to IB HCA → Network</span><br><span class="line">      (通过 BAR 映射)</span><br></pre></td></tr></table></figure><h4 id="多-QP-并行"><a href="#多-QP-并行" class="headerlink" title="多 QP 并行"></a>多 QP 并行</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 配置多个 Queue Pair (QP) 用于并行传输</span><br><span class="line">int qps_per_rank = num_rc_per_pe * num_devices_initialized;</span><br><span class="line"></span><br><span class="line">// 并行发起多个 QP 上的传输</span><br><span class="line">for (int qp_id = 0; qp_id &lt; qps_per_rank; qp_id++) &#123;</span><br><span class="line">    nvshmemi_ibgda_put_nbi_warp(</span><br><span class="line">        dst_addr, src_addr, size, dst_rank, qp_id, ...</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Quiet: 确保所有操作完成</span><br><span class="line">nvshmemi_ibgda_quiet(dst_rank, qp_id);</span><br></pre></td></tr></table></figure><h3 id="4-3-Barrier-实现"><a href="#4-3-Barrier-实现" class="headerlink" title="4.3 Barrier 实现"></a>4.3 Barrier 实现</h3><h4 id="GPU-only-Barrier"><a href="#GPU-only-Barrier" class="headerlink" title="GPU-only Barrier"></a>GPU-only Barrier</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">template &lt;int kNumRanks, bool init = false&gt;</span><br><span class="line">__device__ void barrier_block(int** barrier_signal_ptrs, int rank) &#123;</span><br><span class="line">    __shared__ int barrier_signals[kNumRanks];</span><br><span class="line">    </span><br><span class="line">    if (init) &#123;</span><br><span class="line">        // 初始化: 每个 rank 重置自己的信号</span><br><span class="line">        if (threadIdx.x == 0) &#123;</span><br><span class="line">            for (int i = 0; i &lt; kNumRanks; i++) &#123;</span><br><span class="line">                barrier_signals[i] = 0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    // Phase 1: 通知其他 ranks</span><br><span class="line">    if (threadIdx.x &lt; kNumRanks &amp;&amp; threadIdx.x != rank) &#123;</span><br><span class="line">        atomicAdd(barrier_signal_ptrs[threadIdx.x] + rank, 1);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    // Phase 2: 等待其他 ranks 通知</span><br><span class="line">    if (threadIdx.x == 0) &#123;</span><br><span class="line">        for (int i = 0; i &lt; kNumRanks; i++) &#123;</span><br><span class="line">            if (i != rank) &#123;</span><br><span class="line">                while (barrier_signal_ptrs[rank][i] &lt; expected_count) &#123;</span><br><span class="line">                    // spin wait</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-4-内存访问优化"><a href="#4-4-内存访问优化" class="headerlink" title="4.4 内存访问优化"></a>4.4 内存访问优化</h3><h4 id="Load-Store-指令选择"><a href="#Load-Store-指令选择" class="headerlink" title="Load&#x2F;Store 指令选择"></a>Load&#x2F;Store 指令选择</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// Global memory with cache control</span><br><span class="line">template &lt;typename T&gt;</span><br><span class="line">__device__ T ld_volatile_global(const T* addr) &#123;</span><br><span class="line">    return *((volatile T*)addr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">template &lt;typename T&gt;</span><br><span class="line">__device__ void st_na_global(T* addr, T val) &#123;</span><br><span class="line">    // Non-allocating store (不污染 L2 cache)</span><br><span class="line">    #ifndef DISABLE_AGGRESSIVE_PTX_INSTRS</span><br><span class="line">    asm volatile(&quot;st.global.cs.L2::no_allocate %0, %1;&quot; </span><br><span class="line">                 : : &quot;l&quot;(addr), &quot;r&quot;(val));</span><br><span class="line">    #else</span><br><span class="line">    *addr = val;</span><br><span class="line">    #endif</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// System-level atomics (跨 CPU-GPU)</span><br><span class="line">template &lt;typename T&gt;</span><br><span class="line">__device__ T ld_acquire_sys_global(const T* addr) &#123;</span><br><span class="line">    T val;</span><br><span class="line">    asm volatile(&quot;ld.acquire.sys.global.b32 %0, [%1];&quot; </span><br><span class="line">                 : &quot;=r&quot;(val) : &quot;l&quot;(addr));</span><br><span class="line">    return val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="TMA-Tensor-Memory-Accelerator"><a href="#TMA-Tensor-Memory-Accelerator" class="headerlink" title="TMA (Tensor Memory Accelerator)"></a>TMA (Tensor Memory Accelerator)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// SM90+ 特性: 硬件加速的张量内存拷贝</span><br><span class="line">#ifndef DISABLE_SM90_FEATURES</span><br><span class="line">template &lt;int kNumBytes&gt;</span><br><span class="line">__device__ void tma_load(void* dst, const void* src) &#123;</span><br><span class="line">    // 使用 TMA 硬件单元</span><br><span class="line">    // 更高带宽，更低延迟</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><h3 id="4-5-Warp-level-原语"><a href="#4-5-Warp-level-原语" class="headerlink" title="4.5 Warp-level 原语"></a>4.5 Warp-level 原语</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Warp reduce sum</span><br><span class="line">__device__ int warp_reduce_sum(int val) &#123;</span><br><span class="line">    #pragma unroll</span><br><span class="line">    for (int offset = 16; offset &gt; 0; offset /= 2) &#123;</span><br><span class="line">        val += __shfl_down_sync(0xffffffff, val, offset);</span><br><span class="line">    &#125;</span><br><span class="line">    return val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Elect one thread in warp (通常是 lane 0)</span><br><span class="line">__device__ bool elect_one_sync() &#123;</span><br><span class="line">    return __match_any_sync(0xffffffff, 1) == 0xffffffff &amp;&amp; </span><br><span class="line">           get_lane_id() == 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Warp-level copy</span><br><span class="line">#define UNROLLED_WARP_COPY(num, lane_id, count, dst, src, ld_fn, st_fn) \</span><br><span class="line">    _Pragma(&quot;unroll&quot;) \</span><br><span class="line">    for (int i = lane_id * num; i &lt; count; i += 32 * num) &#123; \</span><br><span class="line">        auto tmp = ld_fn(src + i); \</span><br><span class="line">        st_fn(dst + i, tmp); \</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5-性能优化策略"><a href="#5-性能优化策略" class="headerlink" title="5. 性能优化策略"></a>5. 性能优化策略</h2><h3 id="5-1-通信计算重叠"><a href="#5-1-通信计算重叠" class="headerlink" title="5.1 通信计算重叠"></a>5.1 通信计算重叠</h3><p><strong>策略 1: 独立 Stream</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dispatch 和 expert 计算在不同 stream</span></span><br><span class="line">comm_stream = buffer.get_comm_stream()</span><br><span class="line">event = buffer.dispatch(..., async_mode=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># comm_stream 上执行 dispatch</span></span><br><span class="line"><span class="comment"># 主流继续执行其他计算</span></span><br><span class="line"><span class="keyword">with</span> event:</span><br><span class="line">    expert_forward()  <span class="comment"># 等待 dispatch 完成</span></span><br></pre></td></tr></table></figure><p><strong>策略 2: Hook-based (低延迟模式)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">recv_x, handle, event, recv_hook = buffer.low_latency_dispatch(...)</span><br><span class="line"><span class="comment"># dispatch 立即返回</span></span><br><span class="line"><span class="comment"># 在计算前调用 hook 确保数据到达</span></span><br><span class="line"><span class="keyword">if</span> recv_hook:</span><br><span class="line">    recv_hook()  <span class="comment"># 轮询接收</span></span><br><span class="line">expert_output = expert_forward(recv_x)</span><br></pre></td></tr></table></figure><h3 id="5-2-负载均衡"><a href="#5-2-负载均衡" class="headerlink" title="5.2 负载均衡"></a>5.2 负载均衡</h3><p><strong>Channel 机制</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 将 tokens 分成 num_channels 个 channel</span><br><span class="line">// 每个 SM/warp 处理一个 channel</span><br><span class="line">// 动态平衡各 channel 负载</span><br><span class="line">get_channel_task_range(num_tokens, num_channels, </span><br><span class="line">                       channel_id, start_idx, end_idx);</span><br></pre></td></tr></table></figure><p><strong>SM 分配</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可配置 SM 数量</span></span><br><span class="line">Buffer.set_num_sms(<span class="number">20</span>)  <span class="comment"># 使用 20 个 SM</span></span><br><span class="line">config = Config(num_sms=<span class="number">20</span>, ...)</span><br></pre></td></tr></table></figure><h3 id="5-3-内存带宽优化"><a href="#5-3-内存带宽优化" class="headerlink" title="5.3 内存带宽优化"></a>5.3 内存带宽优化</h3><ol><li><strong>对齐</strong>: 所有数据对齐到 128 bytes</li><li><strong>Coalescing</strong>: 连续线程访问连续内存</li><li><strong>向量化</strong>: 使用 int4&#x2F;int2 向量 load&#x2F;store</li><li><strong>Cache 控制</strong>: 使用 PTX 指令控制 L2 cache</li></ol><h3 id="5-4-延迟优化-低延迟模式"><a href="#5-4-延迟优化-低延迟模式" class="headerlink" title="5.4 延迟优化 (低延迟模式)"></a>5.4 延迟优化 (低延迟模式)</h3><ol><li><strong>预分配固定 buffer</strong>: 避免动态地址计算</li><li><strong>轮询接收</strong>: 主动轮询而非被动等待</li><li><strong>主要使用 RDMA</strong>: 减少 NVLink hop，但在允许的配置下也会利用 NVLink 加速</li><li><strong>超时机制</strong>: 快速检测和跳过慢 rank</li></ol><hr><h2 id="6-使用示例"><a href="#6-使用示例" class="headerlink" title="6. 使用示例"></a>6. 使用示例</h2><h3 id="6-1-基本用法-节点内"><a href="#6-1-基本用法-节点内" class="headerlink" title="6.1 基本用法 (节点内)"></a>6.1 基本用法 (节点内)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> deep_ep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化分布式环境</span></span><br><span class="line">dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)</span><br><span class="line">group = dist.new_group()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 buffer</span></span><br><span class="line">nvl_buffer_size = <span class="number">256</span> * <span class="number">1024</span> * <span class="number">1024</span>  <span class="comment"># 256 MB</span></span><br><span class="line">buffer = deep_ep.Buffer(</span><br><span class="line">    group=group,</span><br><span class="line">    num_nvl_bytes=nvl_buffer_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MoE forward</span></span><br><span class="line">x = torch.randn(<span class="number">4096</span>, <span class="number">7168</span>, dtype=torch.bfloat16, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">topk_idx = routing(x)  <span class="comment"># [4096, 8]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Layout 计算</span></span><br><span class="line">num_tokens_per_rank, _, num_tokens_per_expert, is_token_in_rank, _ = \</span><br><span class="line">    buffer.get_dispatch_layout(topk_idx, num_experts=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dispatch</span></span><br><span class="line"><span class="comment"># Config参数: (num_sms, nvl_send_chunk, nvl_recv_chunk, rdma_send_chunk, rdma_recv_chunk)</span></span><br><span class="line">config = deep_ep.Config(<span class="number">20</span>, <span class="number">6</span>, <span class="number">256</span>, <span class="number">6</span>, <span class="number">128</span>)  <span class="comment"># 8 ranks示例</span></span><br><span class="line">recv_x, recv_x_scales, handle, event = buffer.dispatch(</span><br><span class="line">    x=x,</span><br><span class="line">    topk_idx=topk_idx,</span><br><span class="line">    topk_weights=topk_weights,</span><br><span class="line">    num_tokens_per_rank=num_tokens_per_rank,</span><br><span class="line">    is_token_in_rank=is_token_in_rank,</span><br><span class="line">    num_tokens_per_expert=num_tokens_per_expert,</span><br><span class="line">    config=config</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expert 计算</span></span><br><span class="line"><span class="keyword">with</span> event:  <span class="comment"># 等待 dispatch 完成</span></span><br><span class="line">    expert_output = expert_forward(recv_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine</span></span><br><span class="line">output, combine_event = buffer.combine(</span><br><span class="line">    x=expert_output,</span><br><span class="line">    handle=handle,</span><br><span class="line">    topk_weights=topk_weights,</span><br><span class="line">    config=config</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="6-2-节点间通信"><a href="#6-2-节点间通信" class="headerlink" title="6.2 节点间通信"></a>6.2 节点间通信</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 buffer (包含 RDMA)</span></span><br><span class="line">rdma_buffer_size = <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span>  <span class="comment"># 1 GB</span></span><br><span class="line">buffer = deep_ep.Buffer(</span><br><span class="line">    group=group,</span><br><span class="line">    num_nvl_bytes=nvl_buffer_size,</span><br><span class="line">    num_rdma_bytes=rdma_buffer_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 RDMA layout</span></span><br><span class="line">num_tokens_per_rdma_rank = ...  <span class="comment"># 额外的 RDMA level 统计</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dispatch (internode)</span></span><br><span class="line">recv_x, recv_x_scales, handle, event = buffer.internode_dispatch(</span><br><span class="line">    x=x,</span><br><span class="line">    topk_idx=topk_idx,</span><br><span class="line">    topk_weights=topk_weights,</span><br><span class="line">    num_tokens_per_rank=num_tokens_per_rank,</span><br><span class="line">    num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,</span><br><span class="line">    is_token_in_rank=is_token_in_rank,</span><br><span class="line">    num_tokens_per_expert=num_tokens_per_expert,</span><br><span class="line">    config=config</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine (internode)</span></span><br><span class="line">output, combine_event = buffer.internode_combine(</span><br><span class="line">    x=expert_output,</span><br><span class="line">    handle=handle,</span><br><span class="line">    topk_weights=topk_weights,</span><br><span class="line">    config=config</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="6-3-低延迟模式-推理"><a href="#6-3-低延迟模式-推理" class="headerlink" title="6.3 低延迟模式 (推理)"></a>6.3 低延迟模式 (推理)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建低延迟 buffer</span></span><br><span class="line">buffer = deep_ep.Buffer(</span><br><span class="line">    group=group,</span><br><span class="line">    num_rdma_bytes=rdma_buffer_size,</span><br><span class="line">    low_latency_mode=<span class="literal">True</span>,</span><br><span class="line">    num_qps_per_rank=num_experts  <span class="comment"># 每个 expert 一个 QP</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理 buffer (首次或 batch 改变时)</span></span><br><span class="line">buffer.clean_low_latency_buffer(</span><br><span class="line">    num_max_dispatch_tokens_per_rank=<span class="number">512</span>,</span><br><span class="line">    hidden=<span class="number">7168</span>,</span><br><span class="line">    num_experts=<span class="number">64</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dispatch with hook</span></span><br><span class="line">recv_x, recv_x_scales, handle, event, recv_hook = \</span><br><span class="line">    buffer.low_latency_dispatch(</span><br><span class="line">        x=x,</span><br><span class="line">        topk_idx=topk_idx,</span><br><span class="line">        num_max_dispatch_tokens_per_rank=<span class="number">512</span>,</span><br><span class="line">        num_experts=<span class="number">64</span>,</span><br><span class="line">        use_fp8=<span class="literal">True</span>,</span><br><span class="line">        return_recv_hook=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 hook 确保数据到达</span></span><br><span class="line"><span class="keyword">if</span> recv_hook:</span><br><span class="line">    recv_hook()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expert 计算</span></span><br><span class="line">expert_output = expert_forward(recv_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine with hook</span></span><br><span class="line">output, combine_event, recv_hook = buffer.low_latency_combine(</span><br><span class="line">    x=expert_output,</span><br><span class="line">    topk_idx=topk_idx,</span><br><span class="line">    topk_weights=topk_weights,</span><br><span class="line">    handle=handle,</span><br><span class="line">    num_max_dispatch_tokens_per_rank=<span class="number">512</span>,</span><br><span class="line">    num_experts=<span class="number">64</span>,</span><br><span class="line">    return_recv_hook=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> recv_hook:</span><br><span class="line">    recv_hook()</span><br></pre></td></tr></table></figure><hr><h2 id="7-配置和调优"><a href="#7-配置和调优" class="headerlink" title="7. 配置和调优"></a>7. 配置和调优</h2><h3 id="7-1-关键配置参数"><a href="#7-1-关键配置参数" class="headerlink" title="7.1 关键配置参数"></a>7.1 关键配置参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>:</span><br><span class="line">    num_sms: <span class="built_in">int</span>                           <span class="comment"># 使用的 SM 数量 (通常 20)</span></span><br><span class="line">    num_max_nvl_chunked_send_tokens: <span class="built_in">int</span>   <span class="comment"># NVLink 发送端 chunk 大小</span></span><br><span class="line">    num_max_nvl_chunked_recv_tokens: <span class="built_in">int</span>   <span class="comment"># NVLink 接收端 chunk 大小</span></span><br><span class="line">    num_max_rdma_chunked_send_tokens: <span class="built_in">int</span>  <span class="comment"># RDMA 发送端 chunk 大小</span></span><br><span class="line">    num_max_rdma_chunked_recv_tokens: <span class="built_in">int</span>  <span class="comment"># RDMA 接收端 chunk 大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：</span></span><br><span class="line"><span class="comment"># Intranode (8 ranks): Config(20, 6, 256, 6, 128)</span></span><br><span class="line"><span class="comment"># Internode (16 ranks): Config(20, 36, 288, 20, 128)</span></span><br><span class="line"><span class="comment"># Internode (64 ranks): Config(20, 32, 288, 8, 128)</span></span><br></pre></td></tr></table></figure><h3 id="7-2-环境变量"><a href="#7-2-环境变量" class="headerlink" title="7.2 环境变量"></a>7.2 环境变量</h3><p><strong>NVSHMEM 配置</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IBGDA 相关</span></span><br><span class="line"><span class="built_in">export</span> NVSHMEM_IB_ENABLE_IBGDA=1</span><br><span class="line"><span class="built_in">export</span> NVSHMEM_IBGDA_NUM_RC_PER_PE=24  <span class="comment"># 每个 PE 的 QP 数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># QP 深度 (必须 &gt; 在途 WR 数)</span></span><br><span class="line"><span class="built_in">export</span> NVSHMEM_QP_DEPTH=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用 P2P (低延迟模式可能需要)</span></span><br><span class="line"><span class="built_in">export</span> NVSHMEM_DISABLE_P2P=0/1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内存相关</span></span><br><span class="line"><span class="built_in">export</span> NVSHMEM_CUMEM_GRANULARITY=536870912  <span class="comment"># 512 MB</span></span><br><span class="line"><span class="built_in">export</span> NVSHMEM_MAX_TEAMS=7</span><br></pre></td></tr></table></figure><p><strong>编译选项</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SM 架构</span></span><br><span class="line"><span class="built_in">export</span> TORCH_CUDA_ARCH_LIST=<span class="string">&quot;9.0&quot;</span>  <span class="comment"># H100/H800</span></span><br><span class="line"><span class="built_in">export</span> TORCH_CUDA_ARCH_LIST=<span class="string">&quot;8.0&quot;</span>  <span class="comment"># A100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用 SM90 特性 (Ampere)</span></span><br><span class="line"><span class="built_in">export</span> DISABLE_SM90_FEATURES=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用激进 PTX 指令</span></span><br><span class="line"><span class="built_in">export</span> DISABLE_AGGRESSIVE_PTX_INSTRS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># topk_idx 位数</span></span><br><span class="line"><span class="built_in">export</span> TOPK_IDX_BITS=64  <span class="comment"># or 32</span></span><br></pre></td></tr></table></figure><h3 id="7-3-性能调优建议"><a href="#7-3-性能调优建议" class="headerlink" title="7.3 性能调优建议"></a>7.3 性能调优建议</h3><p><strong>Training (高吞吐)</strong>:</p><ul><li>使用 internode kernels (跨节点) 或 intranode kernels (节点内)</li><li>num_sms &#x3D; 20-40</li><li>chunk sizes 根据 rank 数量调整 (参考 get_dispatch_config&#x2F;get_combine_config)</li><li>buffer 大小通过 Config.get_nvl_buffer_size_hint() 和 get_rdma_buffer_size_hint() 计算</li></ul><p><strong>Inference Prefill</strong>:</p><ul><li>同 training 配置</li><li>启用 FP8 量化可减少带宽需求</li></ul><p><strong>Inference Decode (低延迟)</strong>:</p><ul><li>使用 low_latency kernels</li><li>num_qps_per_rank &#x3D; num_local_experts (每个 expert 一个 QP)</li><li>num_max_dispatch_tokens_per_rank 根据最大 batch 设置</li><li>NVSHMEM_QP_DEPTH &gt;&#x3D; (num_max_dispatch_tokens_per_rank + 1) * 2</li><li>考虑启用 hook-based 重叠以提高吞吐</li></ul><hr><h2 id="8-与-DeepSeek-V3-的关系"><a href="#8-与-DeepSeek-V3-的关系" class="headerlink" title="8. 与 DeepSeek-V3 的关系"></a>8. 与 DeepSeek-V3 的关系</h2><p>DeepEP 是为 DeepSeek-V3 架构设计的通信库，针对其特定需求优化:</p><h3 id="8-1-DeepSeek-V3-MoE-配置"><a href="#8-1-DeepSeek-V3-MoE-配置" class="headerlink" title="8.1 DeepSeek-V3 MoE 配置"></a>8.1 DeepSeek-V3 MoE 配置</h3><ul><li><strong>总 experts</strong>: 256</li><li><strong>Active experts</strong>: top-8</li><li><strong>Group-limited gating</strong>: top-4 groups</li><li><strong>Hidden dimension</strong>: 7168</li><li><strong>EP parallelism</strong>: 通常 64-256 ranks</li></ul><h3 id="8-2-关键优化对应"><a href="#8-2-关键优化对应" class="headerlink" title="8.2 关键优化对应"></a>8.2 关键优化对应</h3><ol><li><p><strong>Group-limited gating</strong></p><ul><li>限制每个 token 只能选择特定 groups 的 experts</li><li>DeepEP 的 asymmetric bandwidth forwarding 优化这个模式</li></ul></li><li><p><strong>高维度 (7168)</strong></p><ul><li>带宽密集型</li><li>DeepEP 的向量化和 TMA 加速</li></ul></li><li><p><strong>大规模并行 (256 ranks)</strong></p><ul><li>需要高效的 RDMA 和多级拓扑</li><li>DeepEP 的 RDMA + NVLink 混合架构</li></ul></li></ol><hr><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>DeepEP 是一个高度优化的 MoE 通信库，具有以下特点:</p><p><strong>三种通信模式</strong>: intranode (NVLink), internode (RDMA+NVLink), low-latency (纯 RDMA)</p><p><strong>高性能</strong>: </p><ul><li>Intranode: ~155 GB&#x2F;s (接近硬件峰值)</li><li>Internode: ~43-58 GB&#x2F;s (RDMA 带宽限制)</li><li>Low-latency: &lt;200 us (256 ranks)</li></ul><p><strong>灵活性</strong>:</p><ul><li>支持 FP8&#x2F;BF16</li><li>可配置 SM 和 channel 数量</li><li>支持通信计算重叠</li></ul><p><strong>鲁棒性</strong>:</p><ul><li>超时检测和容错</li><li>动态 rank 屏蔽</li><li>完善的错误检查</li></ul><p><strong>可扩展性</strong>:</p><ul><li>支持数百个 ranks</li><li>多级拓扑 (NVLink + RDMA)</li><li>高效的元数据交换</li></ul><p>DeepEP 是大规模 MoE 训练和推理的关键基础设施组件，充分利用现代 GPU 和网络硬件的能力。</p>]]></content>
      
      
      <categories>
          
          <category> 系统架构分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepEP </tag>
            
            <tag> MoE </tag>
            
            <tag> Expert Parallelism </tag>
            
            <tag> 通信库 </tag>
            
            <tag> All-to-All </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/07/24/Hello-World/"/>
      <url>/2025/07/24/Hello-World/</url>
      
        <content type="html"><![CDATA[<h1 id="你好，世界！"><a href="#你好，世界！" class="headerlink" title="你好，世界！"></a>你好，世界！</h1><p>欢迎来到我的个人博客！🎉</p><p>这是我使用 <a href="https://hexo.io/">Hexo</a> 静态博客生成器和美丽的 <a href="https://xaoxuu.com/wiki/stellar/">Stellar</a> 主题搭建的全新博客。</p><h2 id="关于这个博客"><a href="#关于这个博客" class="headerlink" title="关于这个博客"></a>关于这个博客</h2><p>在这个博客中，我将分享：</p><ul><li>📚 技术学习笔记</li><li>💻 编程经验总结  </li><li>🌱 生活感悟</li><li>🎯 项目记录</li></ul><h2 id="关于-Stellar-主题"><a href="#关于-Stellar-主题" class="headerlink" title="关于 Stellar 主题"></a>关于 Stellar 主题</h2><p>Stellar 是一个现代化的 Hexo 主题，具有以下特点：</p><ul><li>🎨 简洁美观的设计</li><li>📱 完美的移动端适配</li><li>⚡ 快速的加载速度</li><li>🔧 丰富的自定义选项</li><li>🎯 优秀的 SEO 支持</li></ul><h2 id="开始探索"><a href="#开始探索" class="headerlink" title="开始探索"></a>开始探索</h2><p>感谢您访问我的博客！希望您能在这里找到有用的内容。如果您有任何建议或想法，欢迎随时联系我。</p><p>祝您阅读愉快！ 😊</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hello </tag>
            
            <tag> world </tag>
            
            <tag> stellar </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
